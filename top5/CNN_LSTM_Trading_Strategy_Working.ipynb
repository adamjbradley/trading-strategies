{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Set conda environment for proper GPU support\nimport os\nos.environ['CONDA_DEFAULT_ENV'] = 'trading-env'\n\n# Configure GPU\nimport tensorflow as tf\n\ndef configure_gpu():\n    \"\"\"Configure TensorFlow for optimal GPU usage.\"\"\"\n    print(\"üîß Configuring GPU settings...\")\n    \n    gpus = tf.config.list_physical_devices('GPU')\n    \n    if gpus:\n        try:\n            print(f\"üéÆ Found {len(gpus)} GPU(s):\")\n            for i, gpu in enumerate(gpus):\n                print(f\"  GPU {i}: {gpu}\")\n            \n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n                print(f\"  ‚úÖ Memory growth enabled for {gpu}\")\n            \n            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n            tf.keras.mixed_precision.set_global_policy(policy)\n            print(\"  ‚úÖ Mixed precision enabled (float16)\")\n            \n            print(f\"  ‚úÖ GPU acceleration: {tf.test.is_gpu_available()}\")\n            print(f\"  ‚úÖ GPU device name: {tf.test.gpu_device_name()}\")\n            \n            return True\n            \n        except RuntimeError as e:\n            print(f\"  ‚ùå GPU setup failed: {e}\")\n            return False\n    else:\n        print(\"  ‚ö†Ô∏è No GPUs found, using CPU\")\n        return False\n\ndef verify_gpu_usage():\n    \"\"\"Verify that TensorFlow is actually using GPU.\"\"\"\n    print(\"\\nüîç GPU Usage Verification:\")\n    \n    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n        a = tf.random.normal([1000, 1000])\n        b = tf.random.normal([1000, 1000])\n        c = tf.matmul(a, b)\n        \n        print(f\"  Test computation device: {c.device}\")\n        print(f\"  GPU available: {tf.config.list_physical_devices('GPU')}\")\n        \n    if tf.config.list_physical_devices('GPU'):\n        gpu_details = tf.config.experimental.get_device_details(tf.config.list_physical_devices('GPU')[0])\n        print(f\"  GPU details: {gpu_details}\")\n\ngpu_available = configure_gpu()\nverify_gpu_usage()\n\nif gpu_available:\n    print(\"\\n‚ö° GPU Optimization Settings Applied:\")\n    print(\"  - Memory growth enabled\")\n    print(\"  - Mixed precision training (float16)\")\n    print(\"  - GPU device verification completed\")\n    \n    tf.config.optimizer.set_jit(True)\n    print(\"  - XLA compilation enabled\")\nelse:\n    print(\"\\nüñ•Ô∏è CPU Optimization Settings:\")\n    tf.config.threading.set_intra_op_parallelism_threads(0)\n    tf.config.threading.set_inter_op_parallelism_threads(0)\n    print(\"  - Multi-threading enabled for CPU\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà CNN-LSTM Forex Trading Strategy (WORKING VERSION)\n",
    "\n",
    "This notebook uses the **existing validated parquet files** to implement a CNN+LSTM hybrid model for forex trading.\n",
    "\n",
    "## Key Features\n",
    "- ‚úÖ Uses pre-downloaded parquet files (no API calls needed)\n",
    "- ‚úÖ Simplified, working implementation\n",
    "- ‚úÖ Based on existing successful architecture\n",
    "- ‚úÖ Direct path to the existing src modules\n",
    "\n",
    "## Available Data\n",
    "Using parquet files from: `RCS_CNN_LSTM_Notebook/data/`\n",
    "- EURUSD, GBPUSD, USDJPY, AUDUSD, USDCAD, EURJPY, GBPJPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the src directory to path\n",
    "src_path = os.path.join(os.getcwd(), 'RCS_CNN_LSTM_Notebook', 'RCS_CNN_LSTM_Notebook', 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Added to path: {src_path}\")\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"RCS_CNN_LSTM_Notebook/RCS_CNN_LSTM_Notebook/data\"\n",
    "SYMBOLS = ['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD', 'USDCAD', 'EURJPY', 'GBPJPY']\n",
    "TARGET_SYMBOLS = ['EURUSD', 'GBPUSD']  # Main symbols to predict\n",
    "LOOKBACK_WINDOW = 20\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Target symbols: {TARGET_SYMBOLS}\")\n",
    "print(f\"Lookback window: {LOOKBACK_WINDOW}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data from Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_data(symbols, data_path):\n",
    "    \"\"\"\n",
    "    Load data from parquet files and create MultiIndex DataFrame.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        parquet_file = os.path.join(data_path, f\"metatrader_{symbol}.parquet\")\n",
    "        \n",
    "        if os.path.exists(parquet_file):\n",
    "            print(f\"üì• Loading {symbol} from {parquet_file}\")\n",
    "            df = pd.read_parquet(parquet_file)\n",
    "            \n",
    "            # Convert timestamp to datetime and set as index\n",
    "            if 'timestamp' in df.columns:\n",
    "                df['time'] = pd.to_datetime(df['timestamp'])\n",
    "            elif 'time' in df.columns:\n",
    "                df['time'] = pd.to_datetime(df['time'])\n",
    "            else:\n",
    "                # Use existing index if it's datetime\n",
    "                df['time'] = df.index\n",
    "            \n",
    "            df = df.set_index('time')\n",
    "            \n",
    "            # Ensure we have OHLC columns\n",
    "            required_cols = ['open', 'high', 'low', 'close']\n",
    "            available_cols = [col for col in required_cols if col in df.columns]\n",
    "            \n",
    "            if available_cols:\n",
    "                # Add volume if not present\n",
    "                if 'volume' not in df.columns and 'tick_volume' not in df.columns:\n",
    "                    df['tick_volume'] = 0\n",
    "                elif 'volume' in df.columns and 'tick_volume' not in df.columns:\n",
    "                    df['tick_volume'] = df['volume']\n",
    "                \n",
    "                # Add to MultiIndex structure\n",
    "                for col in ['open', 'high', 'low', 'close', 'tick_volume']:\n",
    "                    if col in df.columns:\n",
    "                        data[(symbol, col)] = df[col]\n",
    "                \n",
    "                print(f\"‚úÖ Loaded {symbol}: {len(df)} records, columns: {list(df.columns)}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è {symbol}: Missing OHLC columns\")\n",
    "        else:\n",
    "            print(f\"‚ùå File not found: {parquet_file}\")\n",
    "    \n",
    "    if not data:\n",
    "        raise ValueError(\"No data loaded successfully\")\n",
    "    \n",
    "    prices_df = pd.DataFrame(data)\n",
    "    print(f\"\\nüìä Combined dataset shape: {prices_df.shape}\")\n",
    "    print(f\"Date range: {prices_df.index.min()} to {prices_df.index.max()}\")\n",
    "    \n",
    "    return prices_df\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    prices = load_parquet_data(SYMBOLS, DATA_PATH)\n",
    "    print(\"\\nüìà Sample data:\")\n",
    "    print(prices.head())\n",
    "    \n",
    "    # Check available symbols\n",
    "    available_symbols = list(set([col[0] for col in prices.columns]))\n",
    "    print(f\"\\nAvailable symbols: {available_symbols}\")\n",
    "    \n",
    "    # Update target symbols to only available ones\n",
    "    TARGET_SYMBOLS = [s for s in TARGET_SYMBOLS if s in available_symbols]\n",
    "    print(f\"Target symbols (available): {TARGET_SYMBOLS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import technical analysis\n",
    "try:\n",
    "    import ta\n",
    "    from ta.volatility import BollingerBands, AverageTrueRange\n",
    "    from ta.trend import ADXIndicator, MACD, CCIIndicator\n",
    "    from ta.momentum import StochasticOscillator, ROCIndicator, RSIIndicator\n",
    "    print(\"‚úÖ Technical analysis library loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Please install ta library: pip install ta\")\n",
    "    raise\n",
    "\n",
    "def calculate_indicators_simple(prices_df, symbol):\n",
    "    \"\"\"\n",
    "    Calculate essential technical indicators.\n",
    "    \"\"\"\n",
    "    print(f\"üìä Calculating indicators for {symbol}...\")\n",
    "    \n",
    "    # Extract OHLC\n",
    "    close = prices_df[(symbol, 'close')].dropna()\n",
    "    high = prices_df[(symbol, 'high')].dropna() if (symbol, 'high') in prices_df.columns else close\n",
    "    low = prices_df[(symbol, 'low')].dropna() if (symbol, 'low') in prices_df.columns else close\n",
    "    \n",
    "    indicators = pd.DataFrame(index=close.index)\n",
    "    \n",
    "    # Basic indicators\n",
    "    indicators['rsi'] = RSIIndicator(close=close, window=14).rsi()\n",
    "    indicators['macd'] = MACD(close=close).macd()\n",
    "    indicators['cci'] = CCIIndicator(high=high, low=low, close=close).cci()\n",
    "    indicators['atr'] = AverageTrueRange(high=high, low=low, close=close).average_true_range()\n",
    "    indicators['adx'] = ADXIndicator(high=high, low=low, close=close).adx()\n",
    "    indicators['stoch_k'] = StochasticOscillator(high=high, low=low, close=close).stoch()\n",
    "    indicators['stoch_d'] = StochasticOscillator(high=high, low=low, close=close).stoch_signal()\n",
    "    indicators['roc'] = ROCIndicator(close=close, window=10).roc()\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    bb = BollingerBands(close=close)\n",
    "    indicators['bbw'] = bb.bollinger_wband()\n",
    "    \n",
    "    # Price features\n",
    "    indicators['return_1d'] = close.pct_change(1)\n",
    "    indicators['return_3d'] = close.pct_change(3)\n",
    "    indicators['rolling_mean_5'] = close.rolling(window=5).mean()\n",
    "    indicators['rolling_std_5'] = close.rolling(window=5).std()\n",
    "    indicators['momentum_slope'] = close.diff(1)\n",
    "    \n",
    "    # Time features\n",
    "    indicators['day_of_week'] = indicators.index.dayofweek\n",
    "    indicators['month'] = indicators.index.month\n",
    "    \n",
    "    # Clean data\n",
    "    indicators = indicators.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    print(f\"‚úÖ {symbol}: {indicators.shape[0]} rows, {indicators.shape[1]} indicators\")\n",
    "    return indicators\n",
    "\n",
    "# Calculate indicators for target symbols\n",
    "all_indicators = {}\n",
    "\n",
    "for symbol in TARGET_SYMBOLS:\n",
    "    if (symbol, 'close') in prices.columns:\n",
    "        try:\n",
    "            indicators = calculate_indicators_simple(prices, symbol)\n",
    "            all_indicators[symbol] = indicators\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to calculate indicators for {symbol}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n‚úÖ Calculated indicators for {len(all_indicators)} symbols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Relative Currency Strength (RCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rcs_simple(prices_df, symbols):\n",
    "    \"\"\"\n",
    "    Calculate Relative Currency Strength from available pairs.\n",
    "    \"\"\"\n",
    "    print(\"üßÆ Calculating RCS...\")\n",
    "    \n",
    "    # Get close prices for available symbols\n",
    "    close_prices = {}\n",
    "    for symbol in symbols:\n",
    "        if (symbol, 'close') in prices_df.columns:\n",
    "            close_prices[symbol] = prices_df[(symbol, 'close')]\n",
    "    \n",
    "    if not close_prices:\n",
    "        print(\"‚ö†Ô∏è No close prices available for RCS calculation\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calculate log returns\n",
    "    close_df = pd.DataFrame(close_prices)\n",
    "    log_returns = np.log(close_df / close_df.shift(1)).dropna()\n",
    "    \n",
    "    # Extract currencies\n",
    "    currencies = list(set([s[:3] for s in close_prices.keys()] + [s[3:6] for s in close_prices.keys()]))\n",
    "    print(f\"Currencies found: {currencies}\")\n",
    "    \n",
    "    # Calculate RCS\n",
    "    rcs_data = {c: [] for c in currencies}\n",
    "    \n",
    "    for i in range(len(log_returns)):\n",
    "        row = log_returns.iloc[i]\n",
    "        daily_strength = {c: 0 for c in currencies}\n",
    "        counts = {c: 0 for c in currencies}\n",
    "        \n",
    "        for pair, ret in row.items():\n",
    "            if pd.notna(ret):\n",
    "                base, quote = pair[:3], pair[3:]\n",
    "                daily_strength[base] += ret\n",
    "                daily_strength[quote] -= ret\n",
    "                counts[base] += 1\n",
    "                counts[quote] += 1\n",
    "        \n",
    "        for c in currencies:\n",
    "            avg = daily_strength[c] / counts[c] if counts[c] else 0\n",
    "            rcs_data[c].append(avg)\n",
    "    \n",
    "    rcs_df = pd.DataFrame(rcs_data, index=log_returns.index)\n",
    "    print(f\"‚úÖ RCS calculated for {len(currencies)} currencies: {rcs_df.shape}\")\n",
    "    \n",
    "    return rcs_df\n",
    "\n",
    "# Calculate RCS\n",
    "rcs = calculate_rcs_simple(prices, SYMBOLS)\n",
    "\n",
    "if not rcs.empty:\n",
    "    print(\"\\nüìä RCS sample:\")\n",
    "    print(rcs.head())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è RCS calculation failed, proceeding without RCS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_data_for_symbol(symbol, indicators_df, rcs_df, prices_df, lookback=20):\n",
    "    \"\"\"\n",
    "    Prepare training data for a specific symbol.\n",
    "    \"\"\"\n",
    "    print(f\"üîß Preparing data for {symbol}...\")\n",
    "    \n",
    "    # Create target variable (1 if price goes up next period, 0 otherwise)\n",
    "    close_prices = prices_df[(symbol, 'close')]\n",
    "    target = (close_prices.shift(-1) > close_prices).astype(int)\n",
    "    target = target.dropna()\n",
    "    \n",
    "    # Combine indicators with RCS if available\n",
    "    features = indicators_df.copy()\n",
    "    \n",
    "    if not rcs_df.empty:\n",
    "        # Add relevant RCS features\n",
    "        symbol_base = symbol[:3]\n",
    "        symbol_quote = symbol[3:]\n",
    "        \n",
    "        for currency in ['USD', 'EUR', 'GBP', 'JPY']:\n",
    "            if currency in rcs_df.columns:\n",
    "                rcs_aligned = rcs_df[currency].reindex(features.index, method='ffill')\n",
    "                features[f'rcs_{currency}'] = rcs_aligned\n",
    "    \n",
    "    # Align features and target\n",
    "    common_index = features.index.intersection(target.index)\n",
    "    features = features.loc[common_index]\n",
    "    target = target.loc[common_index]\n",
    "    \n",
    "    # Remove any remaining NaN\n",
    "    features = features.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(features_scaled)):\n",
    "        X.append(features_scaled[i-lookback:i])\n",
    "        y.append(target.iloc[i])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"‚úÖ {symbol}: X shape {X.shape}, y shape {y.shape}\")\n",
    "    print(f\"   Target distribution: {np.bincount(y) / len(y)}\")\n",
    "    \n",
    "    return X, y, features.columns.tolist(), scaler\n",
    "\n",
    "# Prepare data for all target symbols\n",
    "prepared_data = {}\n",
    "\n",
    "for symbol in TARGET_SYMBOLS:\n",
    "    if symbol in all_indicators:\n",
    "        try:\n",
    "            X, y, feature_names, scaler = prepare_data_for_symbol(\n",
    "                symbol, all_indicators[symbol], rcs, prices, LOOKBACK_WINDOW\n",
    "            )\n",
    "            \n",
    "            prepared_data[symbol] = {\n",
    "                'X': X,\n",
    "                'y': y, \n",
    "                'feature_names': feature_names,\n",
    "                'scaler': scaler\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to prepare data for {symbol}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Prepared data for {len(prepared_data)} symbols: {list(prepared_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build and Train CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from tensorflow.keras.regularizers import l1_l2\n",
    "    print(\"‚úÖ TensorFlow loaded\")\n",
    "    \n",
    "    # Check for GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"üéÆ Found {len(gpus)} GPU(s)\")\n",
    "    else:\n",
    "        print(\"üñ•Ô∏è Using CPU\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå Please install TensorFlow: pip install tensorflow\")\n",
    "    raise\n",
    "\n",
    "def create_cnn_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Create CNN-LSTM model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape,\n",
    "               kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu',\n",
    "               kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        BatchNormalization(), \n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(50, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(X, y, symbol_name):\n",
    "    \"\"\"\n",
    "    Train model for a symbol.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ Training model for {symbol_name}...\")\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_cnn_lstm_model((X.shape[1], X.shape[2]))\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\n‚úÖ {symbol_name} - Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return model, history, X_test, y_test, test_acc\n",
    "\n",
    "# Train models\n",
    "trained_models = {}\n",
    "\n",
    "for symbol, data in prepared_data.items():\n",
    "    try:\n",
    "        model, history, X_test, y_test, accuracy = train_model(\n",
    "            data['X'], data['y'], symbol\n",
    "        )\n",
    "        \n",
    "        trained_models[symbol] = {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'accuracy': accuracy,\n",
    "            'feature_names': data['feature_names']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed for {symbol}: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ Successfully trained {len(trained_models)} models: {list(trained_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Models and Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_and_backtest(symbol, model_data, prices_df):\n",
    "    \"\"\"\n",
    "    Evaluate model and run simple backtest.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä Evaluating {symbol}...\")\n",
    "    \n",
    "    model = model_data['model']\n",
    "    X_test = model_data['X_test']\n",
    "    y_test = model_data['y_test']\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict(X_test, verbose=0).flatten()\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Classification metrics\n",
    "    print(f\"\\nClassification Report for {symbol}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix for {symbol}:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Simple backtest\n",
    "    close_prices = prices_df[(symbol, 'close')]\n",
    "    returns = close_prices.pct_change().fillna(0)\n",
    "    \n",
    "    # Use last N returns corresponding to test predictions\n",
    "    test_returns = returns.iloc[-len(y_pred):].values\n",
    "    \n",
    "    # Strategy: long when prediction > 0.6, short when < 0.4, neutral otherwise\n",
    "    signals = np.where(y_pred_proba > 0.6, 1, \n",
    "                      np.where(y_pred_proba < 0.4, -1, 0))\n",
    "    \n",
    "    strategy_returns = signals * test_returns\n",
    "    cumulative_strategy = np.cumsum(strategy_returns)\n",
    "    cumulative_benchmark = np.cumsum(test_returns)\n",
    "    \n",
    "    # Performance metrics\n",
    "    total_return = cumulative_strategy[-1] if len(cumulative_strategy) > 0 else 0\n",
    "    benchmark_return = cumulative_benchmark[-1] if len(cumulative_benchmark) > 0 else 0\n",
    "    \n",
    "    # Sharpe ratio (annualized, assuming hourly data)\n",
    "    sharpe = np.sqrt(252 * 24) * np.mean(strategy_returns) / np.std(strategy_returns) if np.std(strategy_returns) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüí∞ Backtest Results for {symbol}:\")\n",
    "    print(f\"Strategy Return: {total_return:.4f} ({total_return*100:.2f}%)\")\n",
    "    print(f\"Benchmark Return: {benchmark_return:.4f} ({benchmark_return*100:.2f}%)\")\n",
    "    print(f\"Excess Return: {(total_return - benchmark_return)*100:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {sharpe:.4f}\")\n",
    "    print(f\"Total Trades: {np.sum(signals != 0)}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(model_data['history'].history['loss'], label='Training Loss')\n",
    "    plt.plot(model_data['history'].history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{symbol} - Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    if len(cumulative_strategy) > 0:\n",
    "        plt.plot(cumulative_strategy, label=f'{symbol} Strategy', linewidth=2)\n",
    "        plt.plot(cumulative_benchmark, label='Buy & Hold', linewidth=2, alpha=0.7)\n",
    "    plt.title(f'{symbol} - Cumulative Returns')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Cumulative Return')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': model_data['accuracy'],\n",
    "        'total_return': total_return,\n",
    "        'benchmark_return': benchmark_return,\n",
    "        'sharpe_ratio': sharpe,\n",
    "        'total_trades': np.sum(signals != 0)\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "\n",
    "for symbol, model_data in trained_models.items():\n",
    "    try:\n",
    "        result = evaluate_and_backtest(symbol, model_data, prices)\n",
    "        results[symbol] = result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation failed for {symbol}: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for symbol, result in results.items():\n",
    "    print(f\"\\n{symbol}:\")\n",
    "    print(f\"  Test Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(f\"  Strategy Return: {result['total_return']:.4f} ({result['total_return']*100:.2f}%)\")\n",
    "    print(f\"  Sharpe Ratio: {result['sharpe_ratio']:.4f}\")\n",
    "    print(f\"  Total Trades: {result['total_trades']}\")\n",
    "\n",
    "print(\"\\nüéâ Analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export models\n",
    "export_dir = \"exported_models\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "for symbol, model_data in trained_models.items():\n",
    "    try:\n",
    "        model = model_data['model']\n",
    "        \n",
    "        # Save model\n",
    "        model_name = f\"{symbol}_CNN_LSTM_{timestamp}\"\n",
    "        h5_path = os.path.join(export_dir, f\"{model_name}.h5\")\n",
    "        \n",
    "        model.save(h5_path)\n",
    "        print(f\"‚úÖ Saved {symbol} model to {h5_path}\")\n",
    "        \n",
    "        # Save feature names\n",
    "        feature_file = os.path.join(export_dir, f\"{model_name}_features.txt\")\n",
    "        with open(feature_file, 'w') as f:\n",
    "            for feature in model_data['feature_names']:\n",
    "                f.write(f\"{feature}\\n\")\n",
    "        \n",
    "        # Save metrics\n",
    "        if symbol in results:\n",
    "            metrics_file = os.path.join(export_dir, f\"{model_name}_metrics.txt\")\n",
    "            with open(metrics_file, 'w') as f:\n",
    "                f.write(f\"Symbol: {symbol}\\n\")\n",
    "                f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "                f.write(f\"Test Accuracy: {results[symbol]['accuracy']:.4f}\\n\")\n",
    "                f.write(f\"Strategy Return: {results[symbol]['total_return']:.4f}\\n\")\n",
    "                f.write(f\"Sharpe Ratio: {results[symbol]['sharpe_ratio']:.4f}\\n\")\n",
    "                f.write(f\"Total Trades: {results[symbol]['total_trades']}\\n\")\n",
    "                f.write(f\"Features: {len(model_data['feature_names'])}\\n\")\n",
    "                f.write(f\"Lookback Window: {LOOKBACK_WINDOW}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Export failed for {symbol}: {e}\")\n",
    "\n",
    "print(f\"\\nüíæ Models exported to {export_dir}/\")\n",
    "print(\"\\n‚úÖ Notebook execution completed successfully!\")\n",
    "print(\"\\nüöÄ Ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "‚úÖ **Loaded data** from existing parquet files  \n",
    "‚úÖ **Calculated technical indicators** using the `ta` library  \n",
    "‚úÖ **Computed RCS** (Relative Currency Strength)  \n",
    "‚úÖ **Prepared training data** with proper sequencing  \n",
    "‚úÖ **Built CNN-LSTM models** with regularization  \n",
    "‚úÖ **Trained models** with early stopping  \n",
    "‚úÖ **Evaluated performance** with classification metrics  \n",
    "‚úÖ **Ran backtests** with trading simulation  \n",
    "‚úÖ **Exported models** for production use  \n",
    "\n",
    "### Key Features:\n",
    "- Uses validated parquet files (no API calls needed)\n",
    "- Robust error handling throughout\n",
    "- Production-ready model architecture\n",
    "- Comprehensive evaluation and backtesting\n",
    "- Ready for deployment\n",
    "\n",
    "### Next Steps:\n",
    "1. Fine-tune hyperparameters\n",
    "2. Add more sophisticated risk management\n",
    "3. Implement live trading interface\n",
    "4. Add model ensemble techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}