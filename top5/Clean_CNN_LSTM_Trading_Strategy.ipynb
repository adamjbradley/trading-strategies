{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Set conda environment for proper GPU support\nimport os\nos.environ['CONDA_DEFAULT_ENV'] = 'trading-env'\n\n# Configure GPU\nimport tensorflow as tf\n\ndef configure_gpu():\n    \"\"\"Configure TensorFlow for optimal GPU usage.\"\"\"\n    print(\"üîß Configuring GPU settings...\")\n    \n    gpus = tf.config.list_physical_devices('GPU')\n    \n    if gpus:\n        try:\n            print(f\"üéÆ Found {len(gpus)} GPU(s):\")\n            for i, gpu in enumerate(gpus):\n                print(f\"  GPU {i}: {gpu}\")\n            \n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n                print(f\"  ‚úÖ Memory growth enabled for {gpu}\")\n            \n            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n            tf.keras.mixed_precision.set_global_policy(policy)\n            print(\"  ‚úÖ Mixed precision enabled (float16)\")\n            \n            print(f\"  ‚úÖ GPU acceleration: {tf.test.is_gpu_available()}\")\n            print(f\"  ‚úÖ GPU device name: {tf.test.gpu_device_name()}\")\n            \n            return True\n            \n        except RuntimeError as e:\n            print(f\"  ‚ùå GPU setup failed: {e}\")\n            return False\n    else:\n        print(\"  ‚ö†Ô∏è No GPUs found, using CPU\")\n        return False\n\ndef verify_gpu_usage():\n    \"\"\"Verify that TensorFlow is actually using GPU.\"\"\"\n    print(\"\\nüîç GPU Usage Verification:\")\n    \n    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n        a = tf.random.normal([1000, 1000])\n        b = tf.random.normal([1000, 1000])\n        c = tf.matmul(a, b)\n        \n        print(f\"  Test computation device: {c.device}\")\n        print(f\"  GPU available: {tf.config.list_physical_devices('GPU')}\")\n        \n    if tf.config.list_physical_devices('GPU'):\n        gpu_details = tf.config.experimental.get_device_details(tf.config.list_physical_devices('GPU')[0])\n        print(f\"  GPU details: {gpu_details}\")\n\ngpu_available = configure_gpu()\nverify_gpu_usage()\n\nif gpu_available:\n    print(\"\\n‚ö° GPU Optimization Settings Applied:\")\n    print(\"  - Memory growth enabled\")\n    print(\"  - Mixed precision training (float16)\")\n    print(\"  - GPU device verification completed\")\n    \n    tf.config.optimizer.set_jit(True)\n    print(\"  - XLA compilation enabled\")\nelse:\n    print(\"\\nüñ•Ô∏è CPU Optimization Settings:\")\n    tf.config.threading.set_intra_op_parallelism_threads(0)\n    tf.config.threading.set_inter_op_parallelism_threads(0)\n    print(\"  - Multi-threading enabled for CPU\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Clean CNN-LSTM Forex Trading Strategy\n",
    "\n",
    "This notebook implements a clean, production-ready CNN+LSTM hybrid model for forex price direction prediction using technical indicators and relative currency strength (RCS).\n",
    "\n",
    "## Overview\n",
    "- **Architecture**: CNN layers for feature extraction + LSTM for temporal patterns\n",
    "- **Features**: Technical indicators (RSI, MACD, ATR, etc.) + Relative Currency Strength\n",
    "- **Target**: Binary classification (price direction prediction)\n",
    "- **Data Source**: MetaTrader 5 or Yahoo Finance\n",
    "- **Export**: Trained models in H5 and ONNX formats\n",
    "\n",
    "## Requirements\n",
    "- Python 3.8+\n",
    "- TensorFlow 2.x\n",
    "- scikit-learn\n",
    "- pandas, numpy\n",
    "- ta (technical analysis library)\n",
    "- matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration set\n",
      "Target symbols: ['EURUSD', 'GBPUSD']\n",
      "Lookback window: 20 periods\n",
      "Data provider: metatrader\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "SYMBOLS = ['EURUSD', 'GBPUSD']  # Main trading pairs to predict\n",
    "ALL_SYMBOLS = [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"AUDUSD\", \"USDCAD\", \"EURJPY\", \"GBPJPY\"]  # For RCS calculation\n",
    "LOOKBACK_WINDOW = 20  # Number of time steps for sequence input\n",
    "TEST_SIZE = 0.2  # Proportion of data for testing\n",
    "VALIDATION_SIZE = 0.15  # Proportion of training data for validation\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# Data source configuration\n",
    "PROVIDER = \"metatrader\"  # or \"yahoo\"\n",
    "BROKER = \"amp_global\"\n",
    "INTERVAL = \"H1\"\n",
    "\n",
    "print(\"‚úÖ Configuration set\")\n",
    "print(f\"Target symbols: {SYMBOLS}\")\n",
    "print(f\"Lookback window: {LOOKBACK_WINDOW} periods\")\n",
    "print(f\"Data provider: {PROVIDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Enable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU configuration utilities loaded\n",
      "‚ö†Ô∏è No GPU devices found\n",
      "üñ•Ô∏è Using CPU fallback\n",
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, LSTM, Dense, Dropout, \n",
    "    BatchNormalization, concatenate\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# Technical analysis\n",
    "import ta\n",
    "from ta.volatility import BollingerBands, AverageTrueRange\n",
    "from ta.trend import ADXIndicator, MACD, CCIIndicator\n",
    "from ta.momentum import StochasticOscillator, ROCIndicator, RSIIndicator\n",
    "\n",
    "# Local modules (ensure these exist in your src/ directory)\n",
    "from src.utils.gpu_config import configure_tensorflow_gpu\n",
    "from src.data.loader import load_or_fetch, load_metatrader_data\n",
    "\n",
    "# Configure GPU if available\n",
    "gpu_configured = configure_tensorflow_gpu()\n",
    "if gpu_configured:\n",
    "    print(\"üéÆ GPU acceleration enabled!\")\n",
    "else:\n",
    "    print(\"üñ•Ô∏è Using CPU fallback\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "\n",
    "Load OHLC data for all currency pairs to calculate relative currency strength and technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading forex data...\n",
      "üì• Loading data for EURUSD...\n",
      "üîÑ Fetching fresh data from metatrader API\n",
      "üì¶ Saved to data/metatrader_EURUSD.parquet and data/metatrader_EURUSD.h5\n",
      "‚ö†Ô∏è Missing required columns for EURUSD\n",
      "üì• Loading data for GBPUSD...\n",
      "üîÑ Fetching fresh data from metatrader API\n",
      "üì¶ Saved to data/metatrader_GBPUSD.parquet and data/metatrader_GBPUSD.h5\n",
      "‚ö†Ô∏è Missing required columns for GBPUSD\n",
      "üì• Loading data for USDJPY...\n",
      "üîÑ Fetching fresh data from metatrader API\n",
      "üì¶ Saved to data/metatrader_USDJPY.parquet and data/metatrader_USDJPY.h5\n",
      "‚ö†Ô∏è Missing required columns for USDJPY\n",
      "üì• Loading data for AUDUSD...\n",
      "üîÑ Fetching fresh data from metatrader API\n",
      "üì¶ Saved to data/metatrader_AUDUSD.parquet and data/metatrader_AUDUSD.h5\n",
      "‚ö†Ô∏è Missing required columns for AUDUSD\n",
      "üì• Loading data for USDCAD...\n",
      "üîÑ Fetching fresh data from metatrader API\n",
      "üì¶ Saved to data/metatrader_USDCAD.parquet and data/metatrader_USDCAD.h5\n",
      "‚ö†Ô∏è Missing required columns for USDCAD\n",
      "üì• Loading data for EURJPY...\n",
      "üîÑ Fetching fresh data from metatrader API\n",
      "üì¶ Saved to data/metatrader_EURJPY.parquet and data/metatrader_EURJPY.h5\n",
      "‚ö†Ô∏è Missing required columns for EURJPY\n",
      "üì• Loading data for GBPJPY...\n",
      "üîÑ Fetching fresh data from metatrader API\n",
      "üì¶ Saved to data/metatrader_GBPJPY.parquet and data/metatrader_GBPJPY.h5\n",
      "‚ö†Ô∏è Missing required columns for GBPJPY\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No data loaded successfully",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Load data for all symbols\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîÑ Loading forex data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m prices = \u001b[43mload_forex_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALL_SYMBOLS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPROVIDER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbroker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBROKER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINTERVAL\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìà Sample data:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(prices.head())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mload_forex_data\u001b[39m\u001b[34m(symbols, provider, broker, interval)\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo data loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m prices_df = pd.DataFrame(data)\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Final dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprices_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: No data loaded successfully"
     ]
    }
   ],
   "source": [
    "def load_forex_data(symbols, provider=\"metatrader\", broker=\"amp_global\", interval=\"H1\"):\n",
    "    \"\"\"\n",
    "    Load forex data for multiple symbols and create a MultiIndex DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        symbols: List of symbol names (e.g., ['EURUSD', 'GBPUSD'])\n",
    "        provider: Data provider ('metatrader' or 'yahoo')\n",
    "        broker: Broker name for MetaTrader data\n",
    "        interval: Time interval (e.g., 'H1', 'M15')\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: MultiIndex DataFrame with (symbol, field) columns\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        print(f\"üì• Loading data for {symbol}...\")\n",
    "        \n",
    "        try:\n",
    "            if provider == \"metatrader\":\n",
    "                df = load_or_fetch(\n",
    "                    symbol=symbol,\n",
    "                    provider=provider,\n",
    "                    loader_func=load_metatrader_data,\n",
    "                    api_key=\"\",\n",
    "                    interval=interval,\n",
    "                    broker=broker,\n",
    "                    force_refresh=False\n",
    "                )\n",
    "            else:\n",
    "                # Yahoo Finance fallback\n",
    "                import yfinance as yf\n",
    "                ticker = f\"{symbol}=X\" if len(symbol) == 6 else symbol\n",
    "                df = yf.download(ticker, period=\"2y\", interval=\"1h\")\n",
    "                df = df.reset_index()\n",
    "                df.columns = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "                df['tick_volume'] = df['volume']\n",
    "            \n",
    "            # Standardize column names and data structure\n",
    "            required_cols = ['time', 'open', 'high', 'low', 'close', 'tick_volume']\n",
    "            if all(col in df.columns for col in required_cols):\n",
    "                df = df[required_cols].dropna()\n",
    "                df['time'] = pd.to_datetime(df['time'])\n",
    "                df = df.set_index('time')\n",
    "                \n",
    "                # Add to MultiIndex structure\n",
    "                for col in ['open', 'high', 'low', 'close', 'tick_volume']:\n",
    "                    data[(symbol, col)] = df[col]\n",
    "                    \n",
    "                print(f\"‚úÖ Loaded {len(df)} records for {symbol}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Missing required columns for {symbol}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {symbol}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not data:\n",
    "        raise ValueError(\"No data loaded successfully\")\n",
    "    \n",
    "    prices_df = pd.DataFrame(data)\n",
    "    print(f\"\\nüìä Final dataset shape: {prices_df.shape}\")\n",
    "    print(f\"Date range: {prices_df.index.min()} to {prices_df.index.max()}\")\n",
    "    \n",
    "    return prices_df\n",
    "\n",
    "# Load data for all symbols\n",
    "print(\"üîÑ Loading forex data...\")\n",
    "prices = load_forex_data(\n",
    "    symbols=ALL_SYMBOLS,\n",
    "    provider=PROVIDER,\n",
    "    broker=BROKER,\n",
    "    interval=INTERVAL\n",
    ")\n",
    "\n",
    "print(\"\\nüìà Sample data:\")\n",
    "print(prices.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Calculate technical indicators and relative currency strength for enhanced predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 136\u001b[39m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m indicators\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Calculate RCS\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m rcs = calculate_relative_currency_strength(\u001b[43mprices\u001b[49m)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Calculate technical indicators for target symbols\u001b[39;00m\n\u001b[32m    139\u001b[39m all_features = {}\n",
      "\u001b[31mNameError\u001b[39m: name 'prices' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_relative_currency_strength(prices_df):\n",
    "    \"\"\"\n",
    "    Calculate Relative Currency Strength (RCS) from forex pair log returns.\n",
    "    \n",
    "    Args:\n",
    "        prices_df: MultiIndex DataFrame with (symbol, 'close') columns\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: RCS values for each currency\n",
    "    \"\"\"\n",
    "    print(\"üßÆ Calculating Relative Currency Strength...\")\n",
    "    \n",
    "    # Extract close prices and calculate log returns\n",
    "    close_prices = {}\n",
    "    for symbol in ALL_SYMBOLS:\n",
    "        if (symbol, 'close') in prices_df.columns:\n",
    "            close_prices[symbol] = prices_df[(symbol, 'close')]\n",
    "    \n",
    "    close_df = pd.DataFrame(close_prices)\n",
    "    log_returns = np.log(close_df / close_df.shift(1)).dropna()\n",
    "    \n",
    "    # Extract unique currencies\n",
    "    currencies = list(set([s[:3] for s in log_returns.columns] + [s[3:6] for s in log_returns.columns]))\n",
    "    \n",
    "    # Calculate RCS\n",
    "    rcs_data = {c: [] for c in currencies}\n",
    "    \n",
    "    for i in range(len(log_returns)):\n",
    "        row = log_returns.iloc[i]\n",
    "        daily_strength = {c: 0 for c in currencies}\n",
    "        counts = {c: 0 for c in currencies}\n",
    "        \n",
    "        for pair, ret in row.items():\n",
    "            if pd.notna(ret):\n",
    "                base, quote = pair[:3], pair[3:]\n",
    "                daily_strength[base] += ret\n",
    "                daily_strength[quote] -= ret\n",
    "                counts[base] += 1\n",
    "                counts[quote] += 1\n",
    "        \n",
    "        for c in currencies:\n",
    "            avg_strength = daily_strength[c] / counts[c] if counts[c] > 0 else 0\n",
    "            rcs_data[c].append(avg_strength)\n",
    "    \n",
    "    rcs_df = pd.DataFrame(rcs_data, index=log_returns.index)\n",
    "    print(f\"‚úÖ RCS calculated for {len(currencies)} currencies\")\n",
    "    \n",
    "    return rcs_df\n",
    "\n",
    "def calculate_technical_indicators(prices_df, symbol):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive technical indicators for a given symbol.\n",
    "    \n",
    "    Args:\n",
    "        prices_df: MultiIndex DataFrame with OHLC data\n",
    "        symbol: Symbol name (e.g., 'EURUSD')\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Technical indicators\n",
    "    \"\"\"\n",
    "    print(f\"üìä Calculating technical indicators for {symbol}...\")\n",
    "    \n",
    "    # Extract OHLC data\n",
    "    ohlc_data = {}\n",
    "    for field in ['open', 'high', 'low', 'close']:\n",
    "        if (symbol, field) in prices_df.columns:\n",
    "            ohlc_data[field] = prices_df[(symbol, field)]\n",
    "        else:\n",
    "            # Fallback to close price if field missing\n",
    "            ohlc_data[field] = prices_df[(symbol, 'close')]\n",
    "    \n",
    "    close = ohlc_data['close']\n",
    "    high = ohlc_data['high']\n",
    "    low = ohlc_data['low']\n",
    "    \n",
    "    indicators = pd.DataFrame(index=close.index)\n",
    "    \n",
    "    # Momentum indicators\n",
    "    indicators['rsi'] = RSIIndicator(close=close, window=14).rsi()\n",
    "    indicators['roc'] = ROCIndicator(close=close, window=10).roc()\n",
    "    indicators['momentum'] = close.pct_change(periods=10)\n",
    "    \n",
    "    # Trend indicators\n",
    "    macd = MACD(close=close)\n",
    "    indicators['macd'] = macd.macd()\n",
    "    indicators['macd_signal'] = macd.macd_signal()\n",
    "    indicators['macd_histogram'] = macd.macd_diff()\n",
    "    \n",
    "    indicators['cci'] = CCIIndicator(high=high, low=low, close=close).cci()\n",
    "    indicators['adx'] = ADXIndicator(high=high, low=low, close=close).adx()\n",
    "    \n",
    "    # Volatility indicators\n",
    "    indicators['atr'] = AverageTrueRange(high=high, low=low, close=close).average_true_range()\n",
    "    \n",
    "    bb = BollingerBands(close=close)\n",
    "    indicators['bb_upper'] = bb.bollinger_hband()\n",
    "    indicators['bb_lower'] = bb.bollinger_lband()\n",
    "    indicators['bb_width'] = (bb.bollinger_hband() - bb.bollinger_lband()) / bb.bollinger_mavg()\n",
    "    indicators['bb_position'] = (close - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())\n",
    "    \n",
    "    # Stochastic oscillator\n",
    "    stoch = StochasticOscillator(high=high, low=low, close=close)\n",
    "    indicators['stoch_k'] = stoch.stoch()\n",
    "    indicators['stoch_d'] = stoch.stoch_signal()\n",
    "    \n",
    "    # Price-based features\n",
    "    indicators['return_1h'] = close.pct_change(1)\n",
    "    indicators['return_4h'] = close.pct_change(4)\n",
    "    indicators['return_24h'] = close.pct_change(24)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    indicators['sma_5'] = close.rolling(window=5).mean()\n",
    "    indicators['sma_20'] = close.rolling(window=20).mean()\n",
    "    indicators['ema_12'] = close.ewm(span=12).mean()\n",
    "    indicators['ema_26'] = close.ewm(span=26).mean()\n",
    "    \n",
    "    indicators['volatility_5'] = close.rolling(window=5).std()\n",
    "    indicators['volatility_20'] = close.rolling(window=20).std()\n",
    "    \n",
    "    # Price position indicators\n",
    "    indicators['price_position_5'] = (close - close.rolling(5).min()) / (close.rolling(5).max() - close.rolling(5).min())\n",
    "    indicators['price_position_20'] = (close - close.rolling(20).min()) / (close.rolling(20).max() - close.rolling(20).min())\n",
    "    \n",
    "    # Time-based features\n",
    "    indicators['hour'] = indicators.index.hour\n",
    "    indicators['day_of_week'] = indicators.index.dayofweek\n",
    "    indicators['month'] = indicators.index.month\n",
    "    \n",
    "    # Forward fill and backward fill NaN values\n",
    "    indicators = indicators.ffill().bfill()\n",
    "    \n",
    "    print(f\"‚úÖ Calculated {len(indicators.columns)} technical indicators\")\n",
    "    return indicators\n",
    "\n",
    "# Calculate RCS\n",
    "rcs = calculate_relative_currency_strength(prices)\n",
    "\n",
    "# Calculate technical indicators for target symbols\n",
    "all_features = {}\n",
    "for symbol in SYMBOLS:\n",
    "    if (symbol, 'close') in prices.columns:\n",
    "        indicators = calculate_technical_indicators(prices, symbol)\n",
    "        all_features[symbol] = indicators\n",
    "        print(f\"üìä {symbol}: {indicators.shape[0]} rows, {indicators.shape[1]} features\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature engineering completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Variable Creation and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_variable(prices_df, symbol, prediction_horizon=1):\n",
    "    \"\"\"\n",
    "    Create binary target variable for price direction prediction.\n",
    "    \n",
    "    Args:\n",
    "        prices_df: MultiIndex DataFrame with price data\n",
    "        symbol: Symbol name\n",
    "        prediction_horizon: Number of periods ahead to predict\n",
    "    \n",
    "    Returns:\n",
    "        pandas.Series: Binary target (1 = price up, 0 = price down)\n",
    "    \"\"\"\n",
    "    close_prices = prices_df[(symbol, 'close')]\n",
    "    future_prices = close_prices.shift(-prediction_horizon)\n",
    "    target = (future_prices > close_prices).astype(int)\n",
    "    return target\n",
    "\n",
    "def prepare_features_and_target(indicators_df, rcs_df, target_series, lookback_window):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix and target for model training.\n",
    "    \n",
    "    Args:\n",
    "        indicators_df: Technical indicators DataFrame\n",
    "        rcs_df: Relative Currency Strength DataFrame\n",
    "        target_series: Target variable Series\n",
    "        lookback_window: Number of time steps for sequences\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y, feature_names)\n",
    "    \"\"\"\n",
    "    print(\"üîß Preparing features and target...\")\n",
    "    \n",
    "    # Combine technical indicators with RCS\n",
    "    # Align indices\n",
    "    common_index = indicators_df.index.intersection(rcs_df.index).intersection(target_series.index)\n",
    "    \n",
    "    # Select relevant RCS currencies (e.g., base and quote currencies)\n",
    "    symbol_base = target_series.name[:3] if hasattr(target_series, 'name') else 'EUR'\n",
    "    symbol_quote = target_series.name[3:6] if hasattr(target_series, 'name') else 'USD'\n",
    "    \n",
    "    rcs_features = []\n",
    "    for currency in [symbol_base, symbol_quote, 'USD', 'EUR', 'GBP', 'JPY']:\n",
    "        if currency in rcs_df.columns:\n",
    "            rcs_features.append(f'rcs_{currency}')\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = indicators_df.loc[common_index].copy()\n",
    "    \n",
    "    for i, currency in enumerate(['USD', 'EUR', 'GBP', 'JPY', 'AUD', 'CAD']):\n",
    "        if currency in rcs_df.columns:\n",
    "            combined_features[f'rcs_{currency}'] = rcs_df[currency].loc[common_index]\n",
    "    \n",
    "    # Remove any remaining NaN values\n",
    "    combined_features = combined_features.ffill().bfill().dropna()\n",
    "    \n",
    "    # Align target with features\n",
    "    final_index = combined_features.index.intersection(target_series.index)\n",
    "    X_df = combined_features.loc[final_index]\n",
    "    y_series = target_series.loc[final_index]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_df)\n",
    "    \n",
    "    # Create sequences for LSTM\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(lookback_window, len(X_scaled)):\n",
    "        X_sequences.append(X_scaled[i-lookback_window:i])\n",
    "        y_sequences.append(y_series.iloc[i])\n",
    "    \n",
    "    X = np.array(X_sequences)\n",
    "    y = np.array(y_sequences)\n",
    "    \n",
    "    feature_names = X_df.columns.tolist()\n",
    "    \n",
    "    print(f\"‚úÖ Prepared sequences: X shape {X.shape}, y shape {y.shape}\")\n",
    "    print(f\"‚úÖ Features: {len(feature_names)} total\")\n",
    "    \n",
    "    return X, y, feature_names, scaler\n",
    "\n",
    "# Prepare data for each target symbol\n",
    "prepared_data = {}\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    if symbol in all_features and (symbol, 'close') in prices.columns:\n",
    "        print(f\"\\nüéØ Preparing data for {symbol}...\")\n",
    "        \n",
    "        # Create target\n",
    "        target = create_target_variable(prices, symbol)\n",
    "        target.name = symbol\n",
    "        \n",
    "        # Prepare features\n",
    "        X, y, feature_names, scaler = prepare_features_and_target(\n",
    "            all_features[symbol], rcs, target, LOOKBACK_WINDOW\n",
    "        )\n",
    "        \n",
    "        prepared_data[symbol] = {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'feature_names': feature_names,\n",
    "            'scaler': scaler,\n",
    "            'target_distribution': pd.Series(y).value_counts(normalize=True)\n",
    "        }\n",
    "        \n",
    "        print(f\"üìä Target distribution for {symbol}:\")\n",
    "        print(prepared_data[symbol]['target_distribution'])\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation completed for all symbols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(input_shape, num_classes=1):\n",
    "    \"\"\"\n",
    "    Create CNN+LSTM hybrid model for forex price direction prediction.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input sequences (timesteps, features)\n",
    "        num_classes: Number of output classes (1 for binary classification)\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    print(f\"üèóÔ∏è Building CNN-LSTM model for input shape: {input_shape}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # CNN layers for feature extraction\n",
    "        Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu',\n",
    "            input_shape=input_shape,\n",
    "            kernel_regularizer=l1_l2(l1=0.01, l2=0.01)\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        \n",
    "        Conv1D(\n",
    "            filters=32, \n",
    "            kernel_size=3, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=l1_l2(l1=0.01, l2=0.01)\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        \n",
    "        # LSTM layer for temporal patterns\n",
    "        LSTM(\n",
    "            units=50, \n",
    "            return_sequences=False,\n",
    "            kernel_regularizer=l1_l2(l1=0.01, l2=0.01),\n",
    "            recurrent_regularizer=l1_l2(l1=0.01, l2=0.01)\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(\n",
    "            units=num_classes, \n",
    "            activation='sigmoid',\n",
    "            kernel_regularizer=l1_l2(l1=0.01, l2=0.01)\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Model compiled successfully\")\n",
    "    return model\n",
    "\n",
    "def create_callbacks():\n",
    "    \"\"\"\n",
    "    Create training callbacks for model optimization.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of Keras callbacks\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=EARLY_STOPPING_PATIENCE,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "print(\"‚úÖ Model architecture functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X, y, symbol_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate CNN-LSTM model for a specific symbol.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature sequences\n",
    "        y: Target values\n",
    "        symbol_name: Name of the trading symbol\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training results including model, history, and metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ Training model for {symbol_name}...\")\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(len(X) * (1 - TEST_SIZE))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Further split training data for validation\n",
    "    val_split_idx = int(len(X_train) * (1 - VALIDATION_SIZE))\n",
    "    X_train_final = X_train[:val_split_idx]\n",
    "    X_val = X_train[val_split_idx:]\n",
    "    y_train_final = y_train[:val_split_idx]\n",
    "    y_val = y_train[val_split_idx:]\n",
    "    \n",
    "    print(f\"üìä Data splits:\")\n",
    "    print(f\"  Training: {X_train_final.shape[0]} samples\")\n",
    "    print(f\"  Validation: {X_val.shape[0]} samples\")\n",
    "    print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_cnn_lstm_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "    \n",
    "    print(\"\\nüìã Model Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nüöÄ Starting training for {EPOCHS} epochs...\")\n",
    "    history = model.fit(\n",
    "        X_train_final, y_train_final,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=create_callbacks(),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nüìä Evaluating model performance...\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n‚úÖ {symbol_name} Results:\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{symbol_name} - Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{symbol_name} - Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Train models for each symbol\n",
    "trained_models = {}\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    if symbol in prepared_data:\n",
    "        data = prepared_data[symbol]\n",
    "        results = train_and_evaluate_model(data['X'], data['y'], symbol)\n",
    "        trained_models[symbol] = results\n",
    "        print(f\"\\n‚úÖ {symbol} training completed\")\n",
    "\n",
    "print(\"\\nüéâ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Backtesting and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest(model_results, prices_df, symbol, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Run a simple backtest using model predictions.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary containing model and predictions\n",
    "        prices_df: Price data DataFrame\n",
    "        symbol: Trading symbol\n",
    "        confidence_threshold: Minimum prediction confidence for trades\n",
    "    \n",
    "    Returns:\n",
    "        dict: Backtest results\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìà Running backtest for {symbol}...\")\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    y_pred_proba = model_results['y_pred_proba']\n",
    "    \n",
    "    # Create trading signals based on confidence threshold\n",
    "    signals = np.where(\n",
    "        y_pred_proba > confidence_threshold, 1,  # Buy signal\n",
    "        np.where(y_pred_proba < (1 - confidence_threshold), -1, 0)  # Sell or No signal\n",
    "    )\n",
    "    \n",
    "    # Get corresponding price returns\n",
    "    close_prices = prices_df[(symbol, 'close')]\n",
    "    returns = close_prices.pct_change().fillna(0)\n",
    "    \n",
    "    # Align returns with predictions (take last N returns)\n",
    "    test_returns = returns.iloc[-len(signals):].values\n",
    "    \n",
    "    # Calculate strategy returns\n",
    "    strategy_returns = signals * test_returns\n",
    "    cumulative_strategy = np.cumsum(strategy_returns)\n",
    "    cumulative_benchmark = np.cumsum(test_returns)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    total_return = cumulative_strategy[-1]\n",
    "    benchmark_return = cumulative_benchmark[-1]\n",
    "    \n",
    "    # Sharpe ratio (annualized)\n",
    "    strategy_sharpe = np.sqrt(252 * 24) * np.mean(strategy_returns) / np.std(strategy_returns) if np.std(strategy_returns) > 0 else 0\n",
    "    benchmark_sharpe = np.sqrt(252 * 24) * np.mean(test_returns) / np.std(test_returns) if np.std(test_returns) > 0 else 0\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative_max = np.maximum.accumulate(cumulative_strategy)\n",
    "    drawdowns = cumulative_strategy - cumulative_max\n",
    "    max_drawdown = np.min(drawdowns)\n",
    "    \n",
    "    # Win rate\n",
    "    winning_trades = np.sum(strategy_returns > 0)\n",
    "    total_trades = np.sum(signals != 0)\n",
    "    win_rate = winning_trades / total_trades if total_trades > 0 else 0\n",
    "    \n",
    "    print(f\"üìä Backtest Results for {symbol}:\")\n",
    "    print(f\"  Total Return: {total_return:.4f} ({total_return*100:.2f}%)\")\n",
    "    print(f\"  Benchmark Return: {benchmark_return:.4f} ({benchmark_return*100:.2f}%)\")\n",
    "    print(f\"  Excess Return: {(total_return - benchmark_return)*100:.2f}%\")\n",
    "    print(f\"  Strategy Sharpe: {strategy_sharpe:.4f}\")\n",
    "    print(f\"  Benchmark Sharpe: {benchmark_sharpe:.4f}\")\n",
    "    print(f\"  Max Drawdown: {max_drawdown:.4f} ({max_drawdown*100:.2f}%)\")\n",
    "    print(f\"  Win Rate: {win_rate:.4f} ({win_rate*100:.2f}%)\")\n",
    "    print(f\"  Total Trades: {total_trades}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(cumulative_strategy, label=f'{symbol} Strategy', linewidth=2)\n",
    "    plt.plot(cumulative_benchmark, label='Buy & Hold', linewidth=2, alpha=0.7)\n",
    "    plt.title(f'{symbol} - Cumulative Returns Comparison')\n",
    "    plt.xlabel('Time Period')\n",
    "    plt.ylabel('Cumulative Return')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(drawdowns, label='Drawdown', color='red', alpha=0.7)\n",
    "    plt.fill_between(range(len(drawdowns)), drawdowns, 0, alpha=0.3, color='red')\n",
    "    plt.title(f'{symbol} - Strategy Drawdown')\n",
    "    plt.xlabel('Time Period')\n",
    "    plt.ylabel('Drawdown')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'benchmark_return': benchmark_return,\n",
    "        'excess_return': total_return - benchmark_return,\n",
    "        'strategy_sharpe': strategy_sharpe,\n",
    "        'benchmark_sharpe': benchmark_sharpe,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'win_rate': win_rate,\n",
    "        'total_trades': total_trades,\n",
    "        'signals': signals,\n",
    "        'strategy_returns': strategy_returns,\n",
    "        'cumulative_strategy': cumulative_strategy,\n",
    "        'cumulative_benchmark': cumulative_benchmark\n",
    "    }\n",
    "\n",
    "# Run backtests for all trained models\n",
    "backtest_results = {}\n",
    "\n",
    "for symbol in trained_models:\n",
    "    results = run_backtest(\n",
    "        trained_models[symbol], \n",
    "        prices, \n",
    "        symbol, \n",
    "        confidence_threshold=0.7\n",
    "    )\n",
    "    backtest_results[symbol] = results\n",
    "\n",
    "print(\"\\n‚úÖ All backtests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Export and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def export_model_with_metadata(model, symbol, feature_names, model_results, backtest_results, export_dir=\"exported_models\"):\n",
    "    \"\"\"\n",
    "    Export trained model in multiple formats with comprehensive metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        symbol: Trading symbol\n",
    "        feature_names: List of feature names\n",
    "        model_results: Model training and evaluation results\n",
    "        backtest_results: Backtesting results\n",
    "        export_dir: Directory to save models\n",
    "    \n",
    "    Returns:\n",
    "        dict: Export paths and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nüíæ Exporting model for {symbol}...\")\n",
    "    \n",
    "    # Create export directory if it doesn't exist\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_name = f\"{symbol}_CNN_LSTM_{timestamp}\"\n",
    "    \n",
    "    # Export paths\n",
    "    h5_path = os.path.join(export_dir, f\"{base_name}.h5\")\n",
    "    onnx_path = os.path.join(export_dir, f\"{base_name}.onnx\")\n",
    "    metadata_path = os.path.join(export_dir, f\"{base_name}_metadata.json\")\n",
    "    metrics_path = os.path.join(export_dir, f\"{base_name}_metrics.csv\")\n",
    "    \n",
    "    # Save Keras model\n",
    "    model.save(h5_path)\n",
    "    print(f\"‚úÖ Saved H5 model: {h5_path}\")\n",
    "    \n",
    "    # Save ONNX model\n",
    "    try:\n",
    "        import tf2onnx\n",
    "        import onnx\n",
    "        \n",
    "        spec = (tf.TensorSpec(model.input.shape, tf.float32, name=\"input\"),)\n",
    "        onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "        onnx.save(onnx_model, onnx_path)\n",
    "        print(f\"‚úÖ Saved ONNX model: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è ONNX export failed: {str(e)}\")\n",
    "        onnx_path = None\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"model_info\": {\n",
    "            \"symbol\": symbol,\n",
    "            \"model_type\": \"CNN_LSTM\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"lookback_window\": LOOKBACK_WINDOW,\n",
    "            \"num_features\": len(feature_names),\n",
    "            \"feature_names\": feature_names\n",
    "        },\n",
    "        \"training_config\": {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"dropout_rate\": DROPOUT_RATE,\n",
    "            \"early_stopping_patience\": EARLY_STOPPING_PATIENCE\n",
    "        },\n",
    "        \"model_performance\": {\n",
    "            \"test_accuracy\": float(model_results['accuracy']),\n",
    "            \"final_train_loss\": float(model_results['history'].history['loss'][-1]),\n",
    "            \"final_val_loss\": float(model_results['history'].history['val_loss'][-1]),\n",
    "            \"final_train_acc\": float(model_results['history'].history['accuracy'][-1]),\n",
    "            \"final_val_acc\": float(model_results['history'].history['val_accuracy'][-1])\n",
    "        },\n",
    "        \"backtest_performance\": {\n",
    "            \"total_return\": float(backtest_results['total_return']),\n",
    "            \"benchmark_return\": float(backtest_results['benchmark_return']),\n",
    "            \"excess_return\": float(backtest_results['excess_return']),\n",
    "            \"strategy_sharpe\": float(backtest_results['strategy_sharpe']),\n",
    "            \"max_drawdown\": float(backtest_results['max_drawdown']),\n",
    "            \"win_rate\": float(backtest_results['win_rate']),\n",
    "            \"total_trades\": int(backtest_results['total_trades'])\n",
    "        },\n",
    "        \"file_paths\": {\n",
    "            \"h5_model\": h5_path,\n",
    "            \"onnx_model\": onnx_path,\n",
    "            \"metadata\": metadata_path,\n",
    "            \"metrics\": metrics_path\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    import json\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"‚úÖ Saved metadata: {metadata_path}\")\n",
    "    \n",
    "    # Create metrics CSV\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'metric': ['test_accuracy', 'total_return', 'benchmark_return', 'excess_return',\n",
    "                  'strategy_sharpe', 'max_drawdown', 'win_rate', 'total_trades'],\n",
    "        'value': [\n",
    "            model_results['accuracy'],\n",
    "            backtest_results['total_return'],\n",
    "            backtest_results['benchmark_return'],\n",
    "            backtest_results['excess_return'],\n",
    "            backtest_results['strategy_sharpe'],\n",
    "            backtest_results['max_drawdown'],\n",
    "            backtest_results['win_rate'],\n",
    "            backtest_results['total_trades']\n",
    "        ]\n",
    "    })\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    print(f\"‚úÖ Saved metrics: {metrics_path}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Export all trained models\n",
    "exported_models = {}\n",
    "\n",
    "for symbol in trained_models:\n",
    "    if symbol in backtest_results:\n",
    "        metadata = export_model_with_metadata(\n",
    "            model=trained_models[symbol]['model'],\n",
    "            symbol=symbol,\n",
    "            feature_names=prepared_data[symbol]['feature_names'],\n",
    "            model_results=trained_models[symbol],\n",
    "            backtest_results=backtest_results[symbol]\n",
    "        )\n",
    "        exported_models[symbol] = metadata\n",
    "\n",
    "print(\"\\nüéâ All models exported successfully!\")\n",
    "\n",
    "# Summary of all results\n",
    "print(\"\\nüìä Final Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    if symbol in trained_models and symbol in backtest_results:\n",
    "        acc = trained_models[symbol]['accuracy']\n",
    "        ret = backtest_results[symbol]['total_return']\n",
    "        sharpe = backtest_results[symbol]['strategy_sharpe']\n",
    "        drawdown = backtest_results[symbol]['max_drawdown']\n",
    "        \n",
    "        print(f\"\\n{symbol}:\")\n",
    "        print(f\"  Test Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Total Return: {ret:.4f} ({ret*100:.2f}%)\")\n",
    "        print(f\"  Sharpe Ratio: {sharpe:.4f}\")\n",
    "        print(f\"  Max Drawdown: {drawdown:.4f} ({drawdown*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Notebook execution completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, X_test, feature_names, symbol, n_repeats=5):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using permutation importance.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        feature_names: List of feature names\n",
    "        symbol: Trading symbol\n",
    "        n_repeats: Number of permutation repeats\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Feature importance scores\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Analyzing feature importance for {symbol}...\")\n",
    "    \n",
    "    # Calculate baseline score\n",
    "    baseline_score = model.evaluate(X_test, trained_models[symbol]['y_test'], verbose=0)[1]\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    importance_scores = []\n",
    "    \n",
    "    for feature_idx in range(len(feature_names)):\n",
    "        scores = []\n",
    "        \n",
    "        for _ in range(n_repeats):\n",
    "            # Create a copy and permute the feature\n",
    "            X_permuted = X_test.copy()\n",
    "            np.random.shuffle(X_permuted[:, :, feature_idx])\n",
    "            \n",
    "            # Calculate score with permuted feature\n",
    "            permuted_score = model.evaluate(X_permuted, trained_models[symbol]['y_test'], verbose=0)[1]\n",
    "            \n",
    "            # Importance is the decrease in performance\n",
    "            importance = baseline_score - permuted_score\n",
    "            scores.append(importance)\n",
    "        \n",
    "        importance_scores.append({\n",
    "            'feature': feature_names[feature_idx],\n",
    "            'importance_mean': np.mean(scores),\n",
    "            'importance_std': np.std(scores)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and sort by importance\n",
    "    importance_df = pd.DataFrame(importance_scores)\n",
    "    importance_df = importance_df.sort_values('importance_mean', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    top_features = importance_df.head(20)\n",
    "    \n",
    "    plt.barh(range(len(top_features)), top_features['importance_mean'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance Score (Accuracy Decrease)')\n",
    "    plt.title(f'{symbol} - Top 20 Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Top 10 Important Features for {symbol}:\")\n",
    "    for i, row in importance_df.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance_mean']:.6f} (¬±{row['importance_std']:.6f})\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance for all models\n",
    "feature_importance_results = {}\n",
    "\n",
    "for symbol in trained_models:\n",
    "    if symbol in prepared_data:\n",
    "        importance_df = analyze_feature_importance(\n",
    "            model=trained_models[symbol]['model'],\n",
    "            X_test=trained_models[symbol]['X_test'],\n",
    "            feature_names=prepared_data[symbol]['feature_names'],\n",
    "            symbol=symbol\n",
    "        )\n",
    "        feature_importance_results[symbol] = importance_df\n",
    "        \n",
    "        # Save feature importance to CSV\n",
    "        importance_df.to_csv(f'feature_importance_{symbol}.csv', index=False)\n",
    "        print(f\"‚úÖ Saved feature importance to feature_importance_{symbol}.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature importance analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a complete implementation of a CNN-LSTM hybrid model for forex trading strategy development. Key features include:\n",
    "\n",
    "### ‚úÖ Implemented Features:\n",
    "- **Multi-source data loading** (MetaTrader 5, Yahoo Finance)\n",
    "- **Comprehensive feature engineering** (30+ technical indicators + RCS)\n",
    "- **Robust CNN-LSTM architecture** with regularization\n",
    "- **Proper data splits** and validation procedures\n",
    "- **Backtesting framework** with performance metrics\n",
    "- **Model export** in H5 and ONNX formats\n",
    "- **Feature importance analysis** using permutation importance\n",
    "- **Comprehensive logging** and visualization\n",
    "\n",
    "### üìä Performance Metrics:\n",
    "- Model accuracy on test set\n",
    "- Backtesting total returns vs benchmark\n",
    "- Sharpe ratio and maximum drawdown\n",
    "- Win rate and trade statistics\n",
    "- Feature importance rankings\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Hyperparameter optimization** using Optuna or similar\n",
    "2. **Advanced feature engineering** (wavelets, PCA, etc.)\n",
    "3. **Ensemble methods** combining multiple models\n",
    "4. **Real-time deployment** using exported ONNX models\n",
    "5. **Risk management** integration (position sizing, stop-loss)\n",
    "\n",
    "### ‚ö†Ô∏è Important Notes:\n",
    "- This is for educational purposes - always validate thoroughly before live trading\n",
    "- Consider transaction costs, slippage, and market impact\n",
    "- Past performance does not guarantee future results\n",
    "- Always use proper risk management in live trading\n",
    "\n",
    "---\n",
    "\n",
    "**Model Files Exported To:** `exported_models/`  \n",
    "**Feature Importance:** `feature_importance_*.csv`  \n",
    "**Metadata:** `*_metadata.json`  \n",
    "**Metrics:** `*_metrics.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}