{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Set conda environment for proper GPU support\nimport os\nos.environ['CONDA_DEFAULT_ENV'] = 'trading-env'\n\n# Configure GPU\nimport tensorflow as tf\n\ndef configure_gpu():\n    \"\"\"Configure TensorFlow for optimal GPU usage.\"\"\"\n    print(\"ðŸ”§ Configuring GPU settings...\")\n    \n    gpus = tf.config.list_physical_devices('GPU')\n    \n    if gpus:\n        try:\n            print(f\"ðŸŽ® Found {len(gpus)} GPU(s):\")\n            for i, gpu in enumerate(gpus):\n                print(f\"  GPU {i}: {gpu}\")\n            \n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n                print(f\"  âœ… Memory growth enabled for {gpu}\")\n            \n            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n            tf.keras.mixed_precision.set_global_policy(policy)\n            print(\"  âœ… Mixed precision enabled (float16)\")\n            \n            print(f\"  âœ… GPU acceleration: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n            print(f\"  âœ… GPU device name: {tf.config.list_physical_devices('GPU')[0].name if tf.config.list_physical_devices('GPU') else 'No GPU'}\")\n            \n            return True\n            \n        except RuntimeError as e:\n            print(f\"  âŒ GPU setup failed: {e}\")\n            return False\n    else:\n        print(\"  âš ï¸ No GPUs found, using CPU\")\n        return False\n\ndef verify_gpu_usage():\n    \"\"\"Verify that TensorFlow is actually using GPU.\"\"\"\n    print(\"\\nðŸ” GPU Usage Verification:\")\n    \n    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n        a = tf.random.normal([1000, 1000])\n        b = tf.random.normal([1000, 1000])\n        c = tf.matmul(a, b)\n        \n        print(f\"  Test computation device: {c.device}\")\n        print(f\"  GPU available: {tf.config.list_physical_devices('GPU')}\")\n        \n    if tf.config.list_physical_devices('GPU'):\n        gpu_details = tf.config.experimental.get_device_details(tf.config.list_physical_devices('GPU')[0])\n        print(f\"  GPU details: {gpu_details}\")\n\ngpu_available = configure_gpu()\nverify_gpu_usage()\n\nif gpu_available:\n    print(\"\\nâš¡ GPU Optimization Settings Applied:\")\n    print(\"  - Memory growth enabled\")\n    print(\"  - Mixed precision training (float16)\")\n    print(\"  - GPU device verification completed\")\n    \n    tf.config.optimizer.set_jit(True)\n    print(\"  - XLA compilation enabled\")\nelse:\n    print(\"\\nðŸ–¥ï¸ CPU Optimization Settings:\")\n    tf.config.threading.set_intra_op_parallelism_threads(0)\n    tf.config.threading.set_inter_op_parallelism_threads(0)\n    print(\"  - Multi-threading enabled for CPU\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Clean CNN-LSTM Forex Trading Strategy\n",
    "\n",
    "This notebook implements a clean, production-ready CNN+LSTM hybrid model for forex price direction prediction using technical indicators and relative currency strength (RCS).\n",
    "\n",
    "## Overview\n",
    "- **Architecture**: CNN layers for feature extraction + LSTM for temporal patterns\n",
    "- **Features**: Technical indicators (RSI, MACD, ATR, etc.) + Relative Currency Strength\n",
    "- **Target**: Binary classification (price direction prediction)\n",
    "- **Data Source**: MetaTrader 5 or Yahoo Finance\n",
    "- **Export**: Trained models in H5 and ONNX formats\n",
    "\n",
    "## Requirements\n",
    "- Python 3.8+\n",
    "- TensorFlow 2.x\n",
    "- scikit-learn\n",
    "- pandas, numpy\n",
    "- ta (technical analysis library)\n",
    "- matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration set\n",
      "Target symbols: ['EURUSD', 'GBPUSD']\n",
      "Lookback window: 20 periods\n",
      "Data provider: metatrader\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "SYMBOLS = ['EURUSD', 'GBPUSD']  # Main trading pairs to predict\n",
    "ALL_SYMBOLS = [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"AUDUSD\", \"USDCAD\", \"EURJPY\", \"GBPJPY\"]  # For RCS calculation\n",
    "LOOKBACK_WINDOW = 20  # Number of time steps for sequence input\n",
    "TEST_SIZE = 0.2  # Proportion of data for testing\n",
    "VALIDATION_SIZE = 0.15  # Proportion of training data for validation\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# Data source configuration\n",
    "PROVIDER = \"metatrader\"  # or \"yahoo\"\n",
    "BROKER = \"amp_global\"\n",
    "INTERVAL = \"H1\"\n",
    "\n",
    "print(\"âœ… Configuration set\")\n",
    "print(f\"Target symbols: {SYMBOLS}\")\n",
    "print(f\"Lookback window: {LOOKBACK_WINDOW} periods\")\n",
    "print(f\"Data provider: {PROVIDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Enable GPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Core libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine learning libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Deep learning libraries\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (\n    Input, Conv1D, LSTM, Dense, Dropout, \n    BatchNormalization, concatenate\n)\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l1_l2\n\n# Technical analysis\nimport ta\nfrom ta.volatility import BollingerBands, AverageTrueRange\nfrom ta.trend import ADXIndicator, MACD, CCIIndicator\nfrom ta.momentum import StochasticOscillator, ROCIndicator, RSIIndicator\n\n# GPU Configuration function\ndef configure_tensorflow_gpu():\n    \"\"\"Configure TensorFlow for optimal GPU usage.\"\"\"\n    print(\"ðŸ”§ GPU configuration utilities loaded\")\n    \n    gpus = tf.config.list_physical_devices('GPU')\n    \n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            \n            # Enable mixed precision\n            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n            tf.keras.mixed_precision.set_global_policy(policy)\n            \n            print(f\"ðŸŽ® GPU acceleration enabled: {len(gpus)} GPU(s)\")\n            return True\n            \n        except RuntimeError as e:\n            print(f\"âŒ GPU setup failed: {e}\")\n            return False\n    else:\n        print(\"âš ï¸ No GPU devices found\")\n        return False\n\n# Data loading functions (local implementation)\ndef load_or_fetch(symbol, provider, loader_func, api_key, **kwargs):\n    \"\"\"Load or fetch data with fallback mechanisms.\"\"\"\n    import os\n    \n    symbol_clean = symbol.replace(\"/\", \"\").upper()\n    base_path = f\"data/{provider}_{symbol_clean}\"\n    parquet_path = f\"{base_path}.parquet\"\n    h5_path = f\"{base_path}.h5\"\n    \n    # Try to load cached data first\n    if os.path.exists(parquet_path):\n        try:\n            return pd.read_parquet(parquet_path)\n        except:\n            pass\n            \n    if os.path.exists(h5_path):\n        try:\n            return pd.read_hdf(h5_path)\n        except:\n            pass\n    \n    # If no cached data, try the loader function\n    try:\n        return loader_func(symbol, **kwargs)\n    except Exception as e:\n        print(f\"âŒ Failed to load {symbol}: {e}\")\n        return pd.DataFrame()\n\ndef load_metatrader_data(symbol, **kwargs):\n    \"\"\"MetaTrader data loader with fallback.\"\"\"\n    try:\n        import MetaTrader5 as mt5\n        # Implementation would go here\n        # For now, return empty DataFrame\n        return pd.DataFrame()\n    except ImportError:\n        print(\"âš ï¸ MetaTrader5 not available\")\n        return pd.DataFrame()\n\n# Configure GPU if available\ngpu_configured = configure_tensorflow_gpu()\nif gpu_configured:\n    print(\"ðŸŽ® GPU acceleration enabled!\")\nelse:\n    print(\"ðŸ–¥ï¸ Using CPU fallback\")\n\nprint(\"âœ… Libraries imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "\n",
    "Load OHLC data for all currency pairs to calculate relative currency strength and technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def load_forex_data(symbols, provider=\"metatrader\", broker=\"amp_global\", interval=\"H1\"):\n    \"\"\"\n    Load forex data for multiple symbols and create a MultiIndex DataFrame.\n    \n    Args:\n        symbols: List of symbol names (e.g., ['EURUSD', 'GBPUSD'])\n        provider: Data provider ('metatrader' or 'yahoo')\n        broker: Broker name for MetaTrader data\n        interval: Time interval (e.g., 'H1', 'M15')\n    \n    Returns:\n        pandas.DataFrame: MultiIndex DataFrame with (symbol, field) columns\n    \"\"\"\n    data = {}\n    \n    for symbol in symbols:\n        print(f\"ðŸ“¥ Loading data for {symbol}...\")\n        \n        try:\n            if provider == \"metatrader\":\n                # Try to load from existing parquet files first\n                parquet_path = f\"data/metatrader_{symbol}.parquet\"\n                h5_path = f\"data/metatrader_{symbol}.h5\"\n                \n                if os.path.exists(parquet_path):\n                    df = pd.read_parquet(parquet_path)\n                    print(f\"âœ… Loaded cached data from {parquet_path}\")\n                elif os.path.exists(h5_path):\n                    df = pd.read_hdf(h5_path)\n                    print(f\"âœ… Loaded cached data from {h5_path}\")\n                else:\n                    print(f\"âš ï¸ No cached data found for {symbol}\")\n                    continue\n                    \n            else:\n                # Yahoo Finance fallback\n                import yfinance as yf\n                ticker = f\"{symbol}=X\" if len(symbol) == 6 else symbol\n                df = yf.download(ticker, period=\"2y\", interval=\"1h\")\n                df = df.reset_index()\n                df.columns = ['time', 'open', 'high', 'low', 'close', 'volume']\n                df['tick_volume'] = df['volume']\n            \n            # Check actual columns in the loaded data\n            print(f\"Actual columns in {symbol}: {df.columns.tolist()}\")\n            \n            # Standardize column names based on actual structure\n            if 'timestamp' in df.columns:\n                df = df.rename(columns={'timestamp': 'time'})\n            \n            # Check for required columns with flexible mapping\n            column_mapping = {\n                'time': ['time', 'timestamp', 'Date', 'Datetime'],\n                'open': ['open', 'Open'],\n                'high': ['high', 'High'], \n                'low': ['low', 'Low'],\n                'close': ['close', 'Close'],\n                'volume': ['volume', 'Volume', 'tick_volume', 'Tick_volume']\n            }\n            \n            # Apply column mapping\n            for target_col, possible_cols in column_mapping.items():\n                for possible_col in possible_cols:\n                    if possible_col in df.columns:\n                        df = df.rename(columns={possible_col: target_col})\n                        break\n            \n            # Ensure we have the basic required columns\n            required_cols = ['time', 'open', 'high', 'low', 'close']\n            if not all(col in df.columns for col in required_cols):\n                print(f\"âš ï¸ Missing required columns for {symbol}\")\n                print(f\"Available: {df.columns.tolist()}\")\n                print(f\"Required: {required_cols}\")\n                \n                # Try to use available columns as fallback\n                if 'close' in df.columns:\n                    # Use close price for missing OHLC components\n                    for col in ['open', 'high', 'low']:\n                        if col not in df.columns:\n                            df[col] = df['close']\n                            print(f\"  Using 'close' as fallback for '{col}'\")\n                else:\n                    print(f\"âŒ Cannot proceed without price data for {symbol}\")\n                    continue\n            \n            # Add volume if missing\n            if 'volume' not in df.columns:\n                df['volume'] = 0\n                print(f\"  Added zero volume for {symbol}\")\n            \n            # Clean and process data\n            df = df[['time', 'open', 'high', 'low', 'close', 'volume']].dropna()\n            df['time'] = pd.to_datetime(df['time'])\n            df = df.set_index('time')\n            \n            # Add to MultiIndex structure\n            for col in ['open', 'high', 'low', 'close', 'volume']:\n                data[(symbol, col)] = df[col]\n                \n            print(f\"âœ… Processed {len(df)} records for {symbol}\")\n                \n        except Exception as e:\n            print(f\"âŒ Failed to load {symbol}: {str(e)}\")\n            continue\n    \n    if not data:\n        # Create fallback synthetic data for testing\n        print(\"âš ï¸ Creating synthetic data for testing...\")\n        import numpy as np\n        from datetime import datetime, timedelta\n        \n        # Generate 1000 hours of synthetic forex data\n        dates = pd.date_range(start=datetime.now() - timedelta(hours=1000), \n                             end=datetime.now(), freq='H')\n        \n        for symbol in symbols[:2]:  # Only first 2 symbols for demo\n            # Generate realistic forex price movements\n            base_price = 1.1000 if 'EUR' in symbol else 1.2000\n            \n            # Random walk with mean reversion\n            returns = np.random.normal(0, 0.001, len(dates))\n            prices = base_price * np.exp(np.cumsum(returns))\n            \n            # Create OHLC from prices\n            opens = prices\n            highs = prices * (1 + np.abs(np.random.normal(0, 0.0005, len(dates))))\n            lows = prices * (1 - np.abs(np.random.normal(0, 0.0005, len(dates))))\n            closes = opens + np.random.normal(0, 0.0002, len(dates))\n            volumes = np.random.randint(100, 1000, len(dates))\n            \n            # Add to data dictionary\n            data[(symbol, 'open')] = pd.Series(opens, index=dates)\n            data[(symbol, 'high')] = pd.Series(highs, index=dates)\n            data[(symbol, 'low')] = pd.Series(lows, index=dates)\n            data[(symbol, 'close')] = pd.Series(closes, index=dates)\n            data[(symbol, 'volume')] = pd.Series(volumes, index=dates)\n            \n            print(f\"âœ… Generated synthetic data for {symbol}\")\n    \n    prices_df = pd.DataFrame(data)\n    print(f\"\\nðŸ“Š Final dataset shape: {prices_df.shape}\")\n    if not prices_df.empty:\n        print(f\"Date range: {prices_df.index.min()} to {prices_df.index.max()}\")\n    \n    return prices_df\n\n# Load data for all symbols\nprint(\"ðŸ”„ Loading forex data...\")\ntry:\n    prices = load_forex_data(\n        symbols=ALL_SYMBOLS,\n        provider=PROVIDER,\n        broker=BROKER,\n        interval=INTERVAL\n    )\n\n    print(\"\\nðŸ“ˆ Sample data:\")\n    print(prices.head())\n    \nexcept Exception as e:\n    print(f\"âŒ Data loading failed: {e}\")\n    print(\"Creating minimal synthetic dataset...\")\n    \n    # Fallback: create minimal synthetic data\n    dates = pd.date_range(start='2023-01-01', end='2024-01-01', freq='H')\n    data = {}\n    \n    for symbol in ['EURUSD', 'GBPUSD']:\n        base_price = 1.1 if symbol == 'EURUSD' else 1.25\n        prices_series = base_price + np.cumsum(np.random.normal(0, 0.001, len(dates)))\n        \n        data[(symbol, 'open')] = pd.Series(prices_series, index=dates)\n        data[(symbol, 'high')] = pd.Series(prices_series * 1.001, index=dates)\n        data[(symbol, 'low')] = pd.Series(prices_series * 0.999, index=dates)\n        data[(symbol, 'close')] = pd.Series(prices_series, index=dates)\n        data[(symbol, 'volume')] = pd.Series(np.random.randint(100, 1000, len(dates)), index=dates)\n    \n    prices = pd.DataFrame(data)\n    print(f\"âœ… Created synthetic dataset: {prices.shape}\")\n    print(prices.head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Calculate technical indicators and relative currency strength for enhanced predictive power."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def calculate_relative_currency_strength(prices_df):\n    \"\"\"\n    Calculate Relative Currency Strength (RCS) with robust error handling.\n    \n    Args:\n        prices_df: MultiIndex DataFrame with (symbol, 'close') columns\n    \n    Returns:\n        pandas.DataFrame: RCS values for each currency\n    \"\"\"\n    logger.info(\"ðŸ§® Calculating Relative Currency Strength...\")\n    \n    try:\n        # Extract available close prices only\n        close_prices = {}\n        available_symbols = []\n        \n        for col in prices_df.columns:\n            if len(col) == 2 and col[1] == 'close':  # (symbol, 'close')\n                symbol = col[0]\n                if len(symbol) == 6 and symbol.isalpha():  # Valid forex pair\n                    available_symbols.append(symbol)\n                    close_prices[symbol] = prices_df[col]\n        \n        if not close_prices:\n            raise ValueError(\"No valid close price data available for RCS calculation\")\n        \n        logger.info(f\"Calculating RCS for symbols: {available_symbols}\")\n        \n        close_df = pd.DataFrame(close_prices)\n        log_returns = np.log(close_df / close_df.shift(1)).dropna()\n        \n        if log_returns.empty:\n            raise ValueError(\"No valid log returns calculated\")\n        \n        # Extract unique currencies from available symbols\n        currencies = list(set([s[:3] for s in available_symbols] + [s[3:6] for s in available_symbols]))\n        currencies = [c for c in currencies if len(c) == 3 and c.isalpha()]\n        \n        logger.info(f\"Identified currencies: {currencies}\")\n        \n        # Calculate RCS\n        rcs_data = {c: [] for c in currencies}\n        \n        for i in range(len(log_returns)):\n            row = log_returns.iloc[i]\n            daily_strength = {c: 0 for c in currencies}\n            counts = {c: 0 for c in currencies}\n            \n            for pair, ret in row.items():\n                if pd.notna(ret) and len(pair) == 6:\n                    base, quote = pair[:3], pair[3:]\n                    if base in daily_strength and quote in daily_strength:\n                        daily_strength[base] += ret\n                        daily_strength[quote] -= ret\n                        counts[base] += 1\n                        counts[quote] += 1\n            \n            for c in currencies:\n                avg_strength = daily_strength[c] / counts[c] if counts[c] > 0 else 0\n                rcs_data[c].append(avg_strength)\n        \n        rcs_df = pd.DataFrame(rcs_data, index=log_returns.index)\n        \n        # Validate RCS data\n        if rcs_df.empty:\n            raise ValueError(\"RCS calculation resulted in empty DataFrame\")\n        \n        # Check for excessive NaN values\n        nan_ratio = rcs_df.isnull().sum().sum() / (len(rcs_df) * len(rcs_df.columns))\n        if nan_ratio > MAX_NAN_RATIO:\n            logger.warning(f\"High NaN ratio in RCS data: {nan_ratio:.2%}\")\n        \n        # Clean RCS data using modern pandas methods\n        rcs_df = rcs_df.ffill().bfill().fillna(0)\n        \n        logger.info(f\"âœ… RCS calculated successfully for {len(currencies)} currencies\")\n        logger.info(f\"RCS data shape: {rcs_df.shape}\")\n        \n        return rcs_df\n        \n    except Exception as e:\n        logger.error(f\"RCS calculation failed: {str(e)}\")\n        # Return empty DataFrame with proper structure\n        return pd.DataFrame()\n\ndef calculate_technical_indicators(prices_df, symbol):\n    \"\"\"\n    Calculate technical indicators with comprehensive error handling.\n    \n    Args:\n        prices_df: MultiIndex DataFrame with OHLC data\n        symbol: Symbol name\n    \n    Returns:\n        pandas.DataFrame: Technical indicators\n    \"\"\"\n    logger.info(f\"ðŸ“Š Calculating technical indicators for {symbol}...\")\n    \n    try:\n        # Extract OHLC data with fallbacks\n        ohlc_data = {}\n        for field in ['open', 'high', 'low', 'close']:\n            if (symbol, field) in prices_df.columns:\n                ohlc_data[field] = prices_df[(symbol, field)]\n            elif (symbol, 'close') in prices_df.columns:\n                # Fallback to close price if field missing\n                ohlc_data[field] = prices_df[(symbol, 'close')]\n                if field != 'close':\n                    logger.warning(f\"Using close price as fallback for {field} in {symbol}\")\n            else:\n                raise ValueError(f\"No price data available for {symbol}\")\n        \n        close = ohlc_data['close'].dropna()\n        high = ohlc_data['high'].dropna()\n        low = ohlc_data['low'].dropna()\n        \n        if len(close) < 50:  # Need minimum data for indicators\n            raise ValueError(f\"Insufficient data for {symbol}: only {len(close)} points\")\n        \n        indicators = pd.DataFrame(index=close.index)\n        \n        # Momentum indicators with error handling\n        try:\n            indicators['rsi'] = RSIIndicator(close=close, window=14).rsi()\n        except Exception as e:\n            logger.warning(f\"RSI calculation failed for {symbol}: {e}\")\n            indicators['rsi'] = 50  # Neutral RSI\n        \n        try:\n            indicators['roc'] = ROCIndicator(close=close, window=10).roc()\n        except Exception as e:\n            logger.warning(f\"ROC calculation failed for {symbol}: {e}\")\n            indicators['roc'] = 0\n        \n        indicators['momentum'] = close.pct_change(periods=10).fillna(0)\n        \n        # Trend indicators\n        try:\n            macd = MACD(close=close)\n            indicators['macd'] = macd.macd()\n            indicators['macd_signal'] = macd.macd_signal()\n            indicators['macd_histogram'] = macd.macd_diff()\n        except Exception as e:\n            logger.warning(f\"MACD calculation failed for {symbol}: {e}\")\n            indicators['macd'] = 0\n            indicators['macd_signal'] = 0\n            indicators['macd_histogram'] = 0\n        \n        try:\n            indicators['cci'] = CCIIndicator(high=high, low=low, close=close).cci()\n        except Exception as e:\n            logger.warning(f\"CCI calculation failed for {symbol}: {e}\")\n            indicators['cci'] = 0\n        \n        try:\n            indicators['adx'] = ADXIndicator(high=high, low=low, close=close).adx()\n        except Exception as e:\n            logger.warning(f\"ADX calculation failed for {symbol}: {e}\")\n            indicators['adx'] = 25  # Neutral ADX\n        \n        # Volatility indicators\n        try:\n            indicators['atr'] = AverageTrueRange(high=high, low=low, close=close).average_true_range()\n        except Exception as e:\n            logger.warning(f\"ATR calculation failed for {symbol}: {e}\")\n            indicators['atr'] = close.rolling(20).std().fillna(0)\n        \n        try:\n            bb = BollingerBands(close=close)\n            indicators['bb_upper'] = bb.bollinger_hband()\n            indicators['bb_lower'] = bb.bollinger_lband()\n            indicators['bb_width'] = (bb.bollinger_hband() - bb.bollinger_lband()) / bb.bollinger_mavg()\n            indicators['bb_position'] = (close - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())\n        except Exception as e:\n            logger.warning(f\"Bollinger Bands calculation failed for {symbol}: {e}\")\n            sma = close.rolling(20).mean()\n            std = close.rolling(20).std()\n            indicators['bb_upper'] = sma + 2 * std\n            indicators['bb_lower'] = sma - 2 * std\n            indicators['bb_width'] = (2 * std) / sma\n            indicators['bb_position'] = 0.5\n        \n        # Stochastic oscillator\n        try:\n            stoch = StochasticOscillator(high=high, low=low, close=close)\n            indicators['stoch_k'] = stoch.stoch()\n            indicators['stoch_d'] = stoch.stoch_signal()\n        except Exception as e:\n            logger.warning(f\"Stochastic calculation failed for {symbol}: {e}\")\n            indicators['stoch_k'] = 50\n            indicators['stoch_d'] = 50\n        \n        # Price-based features\n        indicators['return_1h'] = close.pct_change(1).fillna(0)\n        indicators['return_4h'] = close.pct_change(4).fillna(0)\n        indicators['return_24h'] = close.pct_change(24).fillna(0)\n        \n        # Rolling statistics\n        indicators['sma_5'] = close.rolling(window=5).mean()\n        indicators['sma_20'] = close.rolling(window=20).mean()\n        indicators['ema_12'] = close.ewm(span=12).mean()\n        indicators['ema_26'] = close.ewm(span=26).mean()\n        \n        indicators['volatility_5'] = close.rolling(window=5).std()\n        indicators['volatility_20'] = close.rolling(window=20).std()\n        \n        # Price position indicators\n        rolling_min_5 = close.rolling(5).min()\n        rolling_max_5 = close.rolling(5).max()\n        indicators['price_position_5'] = ((close - rolling_min_5) / \n                                        (rolling_max_5 - rolling_min_5).replace(0, np.nan)).fillna(0.5)\n        \n        rolling_min_20 = close.rolling(20).min()\n        rolling_max_20 = close.rolling(20).max()\n        indicators['price_position_20'] = ((close - rolling_min_20) / \n                                         (rolling_max_20 - rolling_min_20).replace(0, np.nan)).fillna(0.5)\n        \n        # Time-based features\n        indicators['hour'] = indicators.index.hour\n        indicators['day_of_week'] = indicators.index.dayofweek\n        indicators['month'] = indicators.index.month\n        \n        # Clean indicators data using modern pandas methods\n        indicators = indicators.ffill().bfill()\n        \n        # Replace any remaining infinite values\n        indicators = indicators.replace([np.inf, -np.inf], np.nan).fillna(0)\n        \n        logger.info(f\"âœ… Calculated {len(indicators.columns)} technical indicators for {symbol}\")\n        logger.info(f\"Indicators data shape: {indicators.shape}\")\n        \n        return indicators\n        \n    except Exception as e:\n        logger.error(f\"Technical indicators calculation failed for {symbol}: {str(e)}\")\n        raise\n\nprint(\"âœ… Feature engineering functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Variable Creation and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def create_target_variable(prices_df, symbol, prediction_horizon=1):\n    \"\"\"\n    Create binary target variable for price direction prediction with validation.\n    \n    Args:\n        prices_df: MultiIndex DataFrame with price data\n        symbol: Symbol name\n        prediction_horizon: Number of periods ahead to predict\n    \n    Returns:\n        pandas.Series: Binary target (1 = price up, 0 = price down)\n    \"\"\"\n    logger.info(f\"Creating target variable for {symbol}...\")\n    \n    try:\n        if (symbol, 'close') not in prices_df.columns:\n            raise ValueError(f\"No close price data for {symbol}\")\n        \n        close_prices = prices_df[(symbol, 'close')].dropna()\n        \n        if len(close_prices) < prediction_horizon + 1:\n            raise ValueError(f\"Insufficient data for target creation: {len(close_prices)} points\")\n        \n        future_prices = close_prices.shift(-prediction_horizon)\n        target = (future_prices > close_prices).astype(int)\n        \n        # Remove NaN values\n        target = target.dropna()\n        \n        if target.empty:\n            raise ValueError(\"Target variable is empty after processing\")\n        \n        # Check class balance\n        class_counts = target.value_counts()\n        if len(class_counts) < 2:\n            raise ValueError(f\"Target has only one class: {class_counts.index[0]}\")\n        \n        class_balance = class_counts.min() / class_counts.sum()\n        logger.info(f\"Target class balance for {symbol}: {class_balance:.2%} minority class\")\n        \n        if class_balance < 0.05:\n            logger.warning(f\"Severe class imbalance detected for {symbol}: {class_counts.to_dict()}\")\n        \n        target.name = symbol\n        logger.info(f\"âœ… Target created for {symbol}: {len(target)} samples\")\n        \n        return target\n        \n    except Exception as e:\n        logger.error(f\"Target creation failed for {symbol}: {str(e)}\")\n        raise\n\ndef prepare_features_and_target(indicators_df, rcs_df, target_series, lookback_window):\n    \"\"\"\n    Prepare feature matrix and target with comprehensive validation.\n    \n    Args:\n        indicators_df: Technical indicators DataFrame\n        rcs_df: Relative Currency Strength DataFrame\n        target_series: Target variable Series\n        lookback_window: Number of time steps for sequences\n    \n    Returns:\n        tuple: (X, y, feature_names, scaler)\n    \"\"\"\n    logger.info(f\"ðŸ”§ Preparing features and target for {target_series.name}...\")\n    \n    try:\n        # Validate inputs\n        if indicators_df.empty:\n            raise ValueError(\"Indicators DataFrame is empty\")\n        \n        if len(indicators_df) < lookback_window * 2:\n            raise ValueError(f\"Insufficient indicator data: {len(indicators_df)} rows, need at least {lookback_window * 2}\")\n        \n        if target_series.empty:\n            raise ValueError(\"Target series is empty\")\n        \n        # Start with indicators as base features\n        combined_features = indicators_df.copy()\n        \n        # Add RCS features if available\n        if not rcs_df.empty:\n            # Align indices first\n            common_rcs_index = indicators_df.index.intersection(rcs_df.index)\n            \n            if len(common_rcs_index) > lookback_window:\n                # Add available RCS features\n                available_currencies = [col for col in rcs_df.columns \n                                      if col in ['USD', 'EUR', 'GBP', 'JPY', 'AUD', 'CAD', 'CHF', 'NZD']]\n                \n                for currency in available_currencies:\n                    if currency in rcs_df.columns:\n                        rcs_feature_name = f'rcs_{currency}'\n                        # Align RCS data with indicators\n                        aligned_rcs = rcs_df[currency].reindex(indicators_df.index)\n                        combined_features[rcs_feature_name] = aligned_rcs\n                \n                logger.info(f\"Added {len(available_currencies)} RCS features\")\n            else:\n                logger.warning(\"Insufficient RCS data overlap, skipping RCS features\")\n        else:\n            logger.info(\"No RCS data available, using only technical indicators\")\n        \n        # Align with target\n        common_index = combined_features.index.intersection(target_series.index)\n        \n        if len(common_index) < lookback_window * 2:\n            raise ValueError(f\"Insufficient aligned data: {len(common_index)} points, need at least {lookback_window * 2}\")\n        \n        # Select aligned data\n        X_df = combined_features.loc[common_index].copy()\n        y_series = target_series.loc[common_index].copy()\n        \n        # Clean features data\n        logger.info(f\"Cleaning features data...\")\n        \n        # Forward fill and backward fill using modern pandas methods\n        X_df = X_df.ffill(limit=5).bfill(limit=5)\n        \n        # Replace infinite values\n        X_df = X_df.replace([np.inf, -np.inf], np.nan)\n        \n        # Drop columns with too many NaN values\n        nan_threshold = len(X_df) * MAX_NAN_RATIO\n        X_df = X_df.dropna(axis=1, thresh=len(X_df) - nan_threshold)\n        \n        # Drop rows with any remaining NaN values\n        initial_rows = len(X_df)\n        X_df = X_df.dropna()\n        y_series = y_series.loc[X_df.index]\n        \n        if len(X_df) != initial_rows:\n            logger.warning(f\"Removed {initial_rows - len(X_df)} rows with NaN values\")\n        \n        if len(X_df) < lookback_window:\n            raise ValueError(f\"After cleaning, insufficient data: {len(X_df)} rows\")\n        \n        logger.info(f\"Final aligned data: {len(X_df)} rows, {len(X_df.columns)} features\")\n        \n        # Scale features\n        logger.info(\"Scaling features...\")\n        scaler = StandardScaler()\n        \n        try:\n            X_scaled = scaler.fit_transform(X_df)\n        except Exception as e:\n            logger.error(f\"Feature scaling failed: {str(e)}\")\n            # Try robust scaling as fallback\n            from sklearn.preprocessing import RobustScaler\n            scaler = RobustScaler()\n            X_scaled = scaler.fit_transform(X_df)\n            logger.info(\"Used RobustScaler as fallback\")\n        \n        # Create sequences for LSTM\n        logger.info(f\"Creating sequences with lookback window: {lookback_window}...\")\n        \n        X_sequences = []\n        y_sequences = []\n        \n        for i in range(lookback_window, len(X_scaled)):\n            X_sequences.append(X_scaled[i-lookback_window:i])\n            y_sequences.append(y_series.iloc[i])\n        \n        if len(X_sequences) == 0:\n            raise ValueError(\"No sequences created - check data length and lookback window\")\n        \n        X = np.array(X_sequences)\n        y = np.array(y_sequences)\n        \n        feature_names = X_df.columns.tolist()\n        \n        # Final validation\n        if X.shape[0] < MIN_TRAINING_SAMPLES:\n            raise ValueError(f\"Insufficient sequences for training: {X.shape[0]}, need at least {MIN_TRAINING_SAMPLES}\")\n        \n        # Check for feature variance\n        feature_vars = np.var(X_scaled, axis=0)\n        zero_var_features = np.sum(feature_vars == 0)\n        if zero_var_features > 0:\n            logger.warning(f\"Found {zero_var_features} features with zero variance\")\n        \n        logger.info(f\"âœ… Prepared sequences successfully:\")\n        logger.info(f\"  X shape: {X.shape}\")\n        logger.info(f\"  y shape: {y.shape}\")\n        logger.info(f\"  Features: {len(feature_names)}\")\n        \n        return X, y, feature_names, scaler\n        \n    except Exception as e:\n        logger.error(f\"Feature preparation failed: {str(e)}\")\n        raise\n\nprint(\"âœ… Data preparation functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(input_shape, num_classes=1):\n",
    "    \"\"\"\n",
    "    Create CNN+LSTM hybrid model for forex price direction prediction.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input sequences (timesteps, features)\n",
    "        num_classes: Number of output classes (1 for binary classification)\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ—ï¸ Building CNN-LSTM model for input shape: {input_shape}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # CNN layers for feature extraction\n",
    "        Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu',\n",
    "            input_shape=input_shape,\n",
    "            kernel_regularizer=l1_l2(l1=0.01, l2=0.01)\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        \n",
    "        Conv1D(\n",
    "            filters=32, \n",
    "            kernel_size=3, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=l1_l2(l1=0.01, l2=0.01)\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        \n",
    "        # LSTM layer for temporal patterns\n",
    "        LSTM(\n",
    "            units=50, \n",
    "            return_sequences=False,\n",
    "            kernel_regularizer=l1_l2(l1=0.01, l2=0.01),\n",
    "            recurrent_regularizer=l1_l2(l1=0.01, l2=0.01)\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(\n",
    "            units=num_classes, \n",
    "            activation='sigmoid',\n",
    "            kernel_regularizer=l1_l2(l1=0.01, l2=0.01)\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Model compiled successfully\")\n",
    "    return model\n",
    "\n",
    "def create_callbacks():\n",
    "    \"\"\"\n",
    "    Create training callbacks for model optimization.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of Keras callbacks\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=EARLY_STOPPING_PATIENCE,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "print(\"âœ… Model architecture functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X, y, symbol_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate CNN-LSTM model for a specific symbol.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature sequences\n",
    "        y: Target values\n",
    "        symbol_name: Name of the trading symbol\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training results including model, history, and metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸŽ¯ Training model for {symbol_name}...\")\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(len(X) * (1 - TEST_SIZE))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Further split training data for validation\n",
    "    val_split_idx = int(len(X_train) * (1 - VALIDATION_SIZE))\n",
    "    X_train_final = X_train[:val_split_idx]\n",
    "    X_val = X_train[val_split_idx:]\n",
    "    y_train_final = y_train[:val_split_idx]\n",
    "    y_val = y_train[val_split_idx:]\n",
    "    \n",
    "    print(f\"ðŸ“Š Data splits:\")\n",
    "    print(f\"  Training: {X_train_final.shape[0]} samples\")\n",
    "    print(f\"  Validation: {X_val.shape[0]} samples\")\n",
    "    print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_cnn_lstm_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Model Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nðŸš€ Starting training for {EPOCHS} epochs...\")\n",
    "    history = model.fit(\n",
    "        X_train_final, y_train_final,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=create_callbacks(),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nðŸ“Š Evaluating model performance...\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nâœ… {symbol_name} Results:\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{symbol_name} - Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{symbol_name} - Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Train models for each symbol\n",
    "trained_models = {}\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    if symbol in prepared_data:\n",
    "        data = prepared_data[symbol]\n",
    "        results = train_and_evaluate_model(data['X'], data['y'], symbol)\n",
    "        trained_models[symbol] = results\n",
    "        print(f\"\\nâœ… {symbol} training completed\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Backtesting and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest(model_results, prices_df, symbol, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Run a simple backtest using model predictions.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary containing model and predictions\n",
    "        prices_df: Price data DataFrame\n",
    "        symbol: Trading symbol\n",
    "        confidence_threshold: Minimum prediction confidence for trades\n",
    "    \n",
    "    Returns:\n",
    "        dict: Backtest results\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ“ˆ Running backtest for {symbol}...\")\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    y_pred_proba = model_results['y_pred_proba']\n",
    "    \n",
    "    # Create trading signals based on confidence threshold\n",
    "    signals = np.where(\n",
    "        y_pred_proba > confidence_threshold, 1,  # Buy signal\n",
    "        np.where(y_pred_proba < (1 - confidence_threshold), -1, 0)  # Sell or No signal\n",
    "    )\n",
    "    \n",
    "    # Get corresponding price returns\n",
    "    close_prices = prices_df[(symbol, 'close')]\n",
    "    returns = close_prices.pct_change().fillna(0)\n",
    "    \n",
    "    # Align returns with predictions (take last N returns)\n",
    "    test_returns = returns.iloc[-len(signals):].values\n",
    "    \n",
    "    # Calculate strategy returns\n",
    "    strategy_returns = signals * test_returns\n",
    "    cumulative_strategy = np.cumsum(strategy_returns)\n",
    "    cumulative_benchmark = np.cumsum(test_returns)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    total_return = cumulative_strategy[-1]\n",
    "    benchmark_return = cumulative_benchmark[-1]\n",
    "    \n",
    "    # Sharpe ratio (annualized)\n",
    "    strategy_sharpe = np.sqrt(252 * 24) * np.mean(strategy_returns) / np.std(strategy_returns) if np.std(strategy_returns) > 0 else 0\n",
    "    benchmark_sharpe = np.sqrt(252 * 24) * np.mean(test_returns) / np.std(test_returns) if np.std(test_returns) > 0 else 0\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative_max = np.maximum.accumulate(cumulative_strategy)\n",
    "    drawdowns = cumulative_strategy - cumulative_max\n",
    "    max_drawdown = np.min(drawdowns)\n",
    "    \n",
    "    # Win rate\n",
    "    winning_trades = np.sum(strategy_returns > 0)\n",
    "    total_trades = np.sum(signals != 0)\n",
    "    win_rate = winning_trades / total_trades if total_trades > 0 else 0\n",
    "    \n",
    "    print(f\"ðŸ“Š Backtest Results for {symbol}:\")\n",
    "    print(f\"  Total Return: {total_return:.4f} ({total_return*100:.2f}%)\")\n",
    "    print(f\"  Benchmark Return: {benchmark_return:.4f} ({benchmark_return*100:.2f}%)\")\n",
    "    print(f\"  Excess Return: {(total_return - benchmark_return)*100:.2f}%\")\n",
    "    print(f\"  Strategy Sharpe: {strategy_sharpe:.4f}\")\n",
    "    print(f\"  Benchmark Sharpe: {benchmark_sharpe:.4f}\")\n",
    "    print(f\"  Max Drawdown: {max_drawdown:.4f} ({max_drawdown*100:.2f}%)\")\n",
    "    print(f\"  Win Rate: {win_rate:.4f} ({win_rate*100:.2f}%)\")\n",
    "    print(f\"  Total Trades: {total_trades}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(cumulative_strategy, label=f'{symbol} Strategy', linewidth=2)\n",
    "    plt.plot(cumulative_benchmark, label='Buy & Hold', linewidth=2, alpha=0.7)\n",
    "    plt.title(f'{symbol} - Cumulative Returns Comparison')\n",
    "    plt.xlabel('Time Period')\n",
    "    plt.ylabel('Cumulative Return')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(drawdowns, label='Drawdown', color='red', alpha=0.7)\n",
    "    plt.fill_between(range(len(drawdowns)), drawdowns, 0, alpha=0.3, color='red')\n",
    "    plt.title(f'{symbol} - Strategy Drawdown')\n",
    "    plt.xlabel('Time Period')\n",
    "    plt.ylabel('Drawdown')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'benchmark_return': benchmark_return,\n",
    "        'excess_return': total_return - benchmark_return,\n",
    "        'strategy_sharpe': strategy_sharpe,\n",
    "        'benchmark_sharpe': benchmark_sharpe,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'win_rate': win_rate,\n",
    "        'total_trades': total_trades,\n",
    "        'signals': signals,\n",
    "        'strategy_returns': strategy_returns,\n",
    "        'cumulative_strategy': cumulative_strategy,\n",
    "        'cumulative_benchmark': cumulative_benchmark\n",
    "    }\n",
    "\n",
    "# Run backtests for all trained models\n",
    "backtest_results = {}\n",
    "\n",
    "for symbol in trained_models:\n",
    "    results = run_backtest(\n",
    "        trained_models[symbol], \n",
    "        prices, \n",
    "        symbol, \n",
    "        confidence_threshold=0.7\n",
    "    )\n",
    "    backtest_results[symbol] = results\n",
    "\n",
    "print(\"\\nâœ… All backtests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Export and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def export_model_with_metadata(model, symbol, feature_names, model_results, backtest_results, export_dir=\"exported_models\"):\n",
    "    \"\"\"\n",
    "    Export trained model in multiple formats with comprehensive metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        symbol: Trading symbol\n",
    "        feature_names: List of feature names\n",
    "        model_results: Model training and evaluation results\n",
    "        backtest_results: Backtesting results\n",
    "        export_dir: Directory to save models\n",
    "    \n",
    "    Returns:\n",
    "        dict: Export paths and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ’¾ Exporting model for {symbol}...\")\n",
    "    \n",
    "    # Create export directory if it doesn't exist\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_name = f\"{symbol}_CNN_LSTM_{timestamp}\"\n",
    "    \n",
    "    # Export paths\n",
    "    h5_path = os.path.join(export_dir, f\"{base_name}.h5\")\n",
    "    onnx_path = os.path.join(export_dir, f\"{base_name}.onnx\")\n",
    "    metadata_path = os.path.join(export_dir, f\"{base_name}_metadata.json\")\n",
    "    metrics_path = os.path.join(export_dir, f\"{base_name}_metrics.csv\")\n",
    "    \n",
    "    # Save Keras model\n",
    "    model.save(h5_path)\n",
    "    print(f\"âœ… Saved H5 model: {h5_path}\")\n",
    "    \n",
    "    # Save ONNX model\n",
    "    try:\n",
    "        import tf2onnx\n",
    "        import onnx\n",
    "        \n",
    "        spec = (tf.TensorSpec(model.input.shape, tf.float32, name=\"input\"),)\n",
    "        onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "        onnx.save(onnx_model, onnx_path)\n",
    "        print(f\"âœ… Saved ONNX model: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ONNX export failed: {str(e)}\")\n",
    "        onnx_path = None\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"model_info\": {\n",
    "            \"symbol\": symbol,\n",
    "            \"model_type\": \"CNN_LSTM\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"lookback_window\": LOOKBACK_WINDOW,\n",
    "            \"num_features\": len(feature_names),\n",
    "            \"feature_names\": feature_names\n",
    "        },\n",
    "        \"training_config\": {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"dropout_rate\": DROPOUT_RATE,\n",
    "            \"early_stopping_patience\": EARLY_STOPPING_PATIENCE\n",
    "        },\n",
    "        \"model_performance\": {\n",
    "            \"test_accuracy\": float(model_results['accuracy']),\n",
    "            \"final_train_loss\": float(model_results['history'].history['loss'][-1]),\n",
    "            \"final_val_loss\": float(model_results['history'].history['val_loss'][-1]),\n",
    "            \"final_train_acc\": float(model_results['history'].history['accuracy'][-1]),\n",
    "            \"final_val_acc\": float(model_results['history'].history['val_accuracy'][-1])\n",
    "        },\n",
    "        \"backtest_performance\": {\n",
    "            \"total_return\": float(backtest_results['total_return']),\n",
    "            \"benchmark_return\": float(backtest_results['benchmark_return']),\n",
    "            \"excess_return\": float(backtest_results['excess_return']),\n",
    "            \"strategy_sharpe\": float(backtest_results['strategy_sharpe']),\n",
    "            \"max_drawdown\": float(backtest_results['max_drawdown']),\n",
    "            \"win_rate\": float(backtest_results['win_rate']),\n",
    "            \"total_trades\": int(backtest_results['total_trades'])\n",
    "        },\n",
    "        \"file_paths\": {\n",
    "            \"h5_model\": h5_path,\n",
    "            \"onnx_model\": onnx_path,\n",
    "            \"metadata\": metadata_path,\n",
    "            \"metrics\": metrics_path\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    import json\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"âœ… Saved metadata: {metadata_path}\")\n",
    "    \n",
    "    # Create metrics CSV\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'metric': ['test_accuracy', 'total_return', 'benchmark_return', 'excess_return',\n",
    "                  'strategy_sharpe', 'max_drawdown', 'win_rate', 'total_trades'],\n",
    "        'value': [\n",
    "            model_results['accuracy'],\n",
    "            backtest_results['total_return'],\n",
    "            backtest_results['benchmark_return'],\n",
    "            backtest_results['excess_return'],\n",
    "            backtest_results['strategy_sharpe'],\n",
    "            backtest_results['max_drawdown'],\n",
    "            backtest_results['win_rate'],\n",
    "            backtest_results['total_trades']\n",
    "        ]\n",
    "    })\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    print(f\"âœ… Saved metrics: {metrics_path}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Export all trained models\n",
    "exported_models = {}\n",
    "\n",
    "for symbol in trained_models:\n",
    "    if symbol in backtest_results:\n",
    "        metadata = export_model_with_metadata(\n",
    "            model=trained_models[symbol]['model'],\n",
    "            symbol=symbol,\n",
    "            feature_names=prepared_data[symbol]['feature_names'],\n",
    "            model_results=trained_models[symbol],\n",
    "            backtest_results=backtest_results[symbol]\n",
    "        )\n",
    "        exported_models[symbol] = metadata\n",
    "\n",
    "print(\"\\nðŸŽ‰ All models exported successfully!\")\n",
    "\n",
    "# Summary of all results\n",
    "print(\"\\nðŸ“Š Final Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    if symbol in trained_models and symbol in backtest_results:\n",
    "        acc = trained_models[symbol]['accuracy']\n",
    "        ret = backtest_results[symbol]['total_return']\n",
    "        sharpe = backtest_results[symbol]['strategy_sharpe']\n",
    "        drawdown = backtest_results[symbol]['max_drawdown']\n",
    "        \n",
    "        print(f\"\\n{symbol}:\")\n",
    "        print(f\"  Test Accuracy: {acc:.4f}\")\n",
    "        print(f\"  Total Return: {ret:.4f} ({ret*100:.2f}%)\")\n",
    "        print(f\"  Sharpe Ratio: {sharpe:.4f}\")\n",
    "        print(f\"  Max Drawdown: {drawdown:.4f} ({drawdown*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Notebook execution completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, X_test, feature_names, symbol, n_repeats=5):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using permutation importance.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        feature_names: List of feature names\n",
    "        symbol: Trading symbol\n",
    "        n_repeats: Number of permutation repeats\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Feature importance scores\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ” Analyzing feature importance for {symbol}...\")\n",
    "    \n",
    "    # Calculate baseline score\n",
    "    baseline_score = model.evaluate(X_test, trained_models[symbol]['y_test'], verbose=0)[1]\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    importance_scores = []\n",
    "    \n",
    "    for feature_idx in range(len(feature_names)):\n",
    "        scores = []\n",
    "        \n",
    "        for _ in range(n_repeats):\n",
    "            # Create a copy and permute the feature\n",
    "            X_permuted = X_test.copy()\n",
    "            np.random.shuffle(X_permuted[:, :, feature_idx])\n",
    "            \n",
    "            # Calculate score with permuted feature\n",
    "            permuted_score = model.evaluate(X_permuted, trained_models[symbol]['y_test'], verbose=0)[1]\n",
    "            \n",
    "            # Importance is the decrease in performance\n",
    "            importance = baseline_score - permuted_score\n",
    "            scores.append(importance)\n",
    "        \n",
    "        importance_scores.append({\n",
    "            'feature': feature_names[feature_idx],\n",
    "            'importance_mean': np.mean(scores),\n",
    "            'importance_std': np.std(scores)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and sort by importance\n",
    "    importance_df = pd.DataFrame(importance_scores)\n",
    "    importance_df = importance_df.sort_values('importance_mean', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    top_features = importance_df.head(20)\n",
    "    \n",
    "    plt.barh(range(len(top_features)), top_features['importance_mean'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance Score (Accuracy Decrease)')\n",
    "    plt.title(f'{symbol} - Top 20 Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Top 10 Important Features for {symbol}:\")\n",
    "    for i, row in importance_df.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance_mean']:.6f} (Â±{row['importance_std']:.6f})\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance for all models\n",
    "feature_importance_results = {}\n",
    "\n",
    "for symbol in trained_models:\n",
    "    if symbol in prepared_data:\n",
    "        importance_df = analyze_feature_importance(\n",
    "            model=trained_models[symbol]['model'],\n",
    "            X_test=trained_models[symbol]['X_test'],\n",
    "            feature_names=prepared_data[symbol]['feature_names'],\n",
    "            symbol=symbol\n",
    "        )\n",
    "        feature_importance_results[symbol] = importance_df\n",
    "        \n",
    "        # Save feature importance to CSV\n",
    "        importance_df.to_csv(f'feature_importance_{symbol}.csv', index=False)\n",
    "        print(f\"âœ… Saved feature importance to feature_importance_{symbol}.csv\")\n",
    "\n",
    "print(\"\\nâœ… Feature importance analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a complete implementation of a CNN-LSTM hybrid model for forex trading strategy development. Key features include:\n",
    "\n",
    "### âœ… Implemented Features:\n",
    "- **Multi-source data loading** (MetaTrader 5, Yahoo Finance)\n",
    "- **Comprehensive feature engineering** (30+ technical indicators + RCS)\n",
    "- **Robust CNN-LSTM architecture** with regularization\n",
    "- **Proper data splits** and validation procedures\n",
    "- **Backtesting framework** with performance metrics\n",
    "- **Model export** in H5 and ONNX formats\n",
    "- **Feature importance analysis** using permutation importance\n",
    "- **Comprehensive logging** and visualization\n",
    "\n",
    "### ðŸ“Š Performance Metrics:\n",
    "- Model accuracy on test set\n",
    "- Backtesting total returns vs benchmark\n",
    "- Sharpe ratio and maximum drawdown\n",
    "- Win rate and trade statistics\n",
    "- Feature importance rankings\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "1. **Hyperparameter optimization** using Optuna or similar\n",
    "2. **Advanced feature engineering** (wavelets, PCA, etc.)\n",
    "3. **Ensemble methods** combining multiple models\n",
    "4. **Real-time deployment** using exported ONNX models\n",
    "5. **Risk management** integration (position sizing, stop-loss)\n",
    "\n",
    "### âš ï¸ Important Notes:\n",
    "- This is for educational purposes - always validate thoroughly before live trading\n",
    "- Consider transaction costs, slippage, and market impact\n",
    "- Past performance does not guarantee future results\n",
    "- Always use proper risk management in live trading\n",
    "\n",
    "---\n",
    "\n",
    "**Model Files Exported To:** `exported_models/`  \n",
    "**Feature Importance:** `feature_importance_*.csv`  \n",
    "**Metadata:** `*_metadata.json`  \n",
    "**Metrics:** `*_metrics.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}