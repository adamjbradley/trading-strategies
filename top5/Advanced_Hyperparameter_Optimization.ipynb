{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Advanced Hyperparameter Optimization System\n",
    "\n",
    "## Enhanced optimization framework with:\n",
    "- **Study Resumption**: Load and continue existing optimizations\n",
    "- **Multi-Symbol Optimization**: Optimize across all 7 currency pairs\n",
    "- **Parameter Transfer**: Apply successful parameters across symbols\n",
    "- **Benchmarking Dashboard**: Compare optimization performance\n",
    "- **Ensemble Methods**: Combine multiple best models\n",
    "- **Adaptive Systems**: Market regime detection and switching\n",
    "\n",
    "Built on existing optimization results from previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optuna available\n",
      "üéØ Advanced Optimization System Initialized\n",
      "Target symbols: ['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD', 'USDCAD', 'EURJPY', 'GBPJPY']\n",
      "Configuration: {'n_trials_per_symbol': 50, 'n_trials_ensemble': 25, 'cv_splits': 5, 'timeout_per_symbol': 1800, 'n_jobs': 1, 'study_storage': None, 'enable_pruning': True, 'enable_warm_start': True, 'enable_transfer_learning': True}\n"
     ]
    }
   ],
   "source": [
    "# Advanced Hyperparameter Optimization Framework\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup enhanced logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('advanced_optimization.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import optimization libraries\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler, CmaEsSampler\n",
    "    from optuna.pruners import MedianPruner, HyperbandPruner\n",
    "    from optuna.study import MaxTrialsCallback\n",
    "    from optuna.trial import TrialState\n",
    "    print(\"‚úÖ Optuna available\")\n",
    "except ImportError:\n",
    "    print(\"Installing Optuna with advanced features...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna[optional]\"])\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler, CmaEsSampler\n",
    "    from optuna.pruners import MedianPruner, HyperbandPruner\n",
    "    print(\"‚úÖ Optuna installed with advanced features\")\n",
    "\n",
    "# ML and deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold, RFE\n",
    "\n",
    "# Technical analysis\n",
    "import ta\n",
    "from ta.volatility import BollingerBands, AverageTrueRange\n",
    "from ta.trend import ADXIndicator, MACD, CCIIndicator\n",
    "from ta.momentum import StochasticOscillator, ROCIndicator, RSIIndicator\n",
    "\n",
    "# Configuration\n",
    "SYMBOLS = ['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD', 'USDCAD', 'EURJPY', 'GBPJPY']\n",
    "DATA_PATH = \"data\"\n",
    "RESULTS_PATH = \"optimization_results\"\n",
    "MODELS_PATH = \"exported_models\"\n",
    "\n",
    "# Advanced optimization settings\n",
    "ADVANCED_CONFIG = {\n",
    "    'n_trials_per_symbol': 50,\n",
    "    'n_trials_ensemble': 25,\n",
    "    'cv_splits': 5,\n",
    "    'timeout_per_symbol': 1800,  # 30 minutes per symbol\n",
    "    'n_jobs': 1,  # Sequential for stability\n",
    "    'study_storage': None,  # In-memory for now\n",
    "    'enable_pruning': True,\n",
    "    'enable_warm_start': True,\n",
    "    'enable_transfer_learning': True\n",
    "}\n",
    "\n",
    "print(f\"üéØ Advanced Optimization System Initialized\")\n",
    "print(f\"Target symbols: {SYMBOLS}\")\n",
    "print(f\"Configuration: {ADVANCED_CONFIG}\")\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data classes defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Data Classes for Optimization Results\n",
    "@dataclass\n",
    "class OptimizationResult:\n",
    "    \"\"\"Data class to store optimization results\"\"\"\n",
    "    symbol: str\n",
    "    timestamp: str\n",
    "    objective_value: float\n",
    "    best_params: Dict[str, Any]\n",
    "    mean_accuracy: float\n",
    "    mean_sharpe: float\n",
    "    std_accuracy: float\n",
    "    std_sharpe: float\n",
    "    num_features: int\n",
    "    total_trials: int\n",
    "    completed_trials: int\n",
    "    study_name: str\n",
    "    \n",
    "@dataclass\n",
    "class BenchmarkMetrics:\n",
    "    \"\"\"Benchmark comparison metrics\"\"\"\n",
    "    symbol: str\n",
    "    current_score: float\n",
    "    previous_best: float\n",
    "    improvement: float\n",
    "    rank: int\n",
    "    percentile: float\n",
    "\n",
    "print(\"‚úÖ Data classes defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 01:06:58,582 - __main__ - WARNING - Failed to load optimization_results/best_params_EURUSD_20250612_101803.json: Expecting value: line 1 column 1 (char 0)\n",
      "2025-06-13 01:06:58,586 - __main__ - WARNING - Failed to load optimization_results/best_params_EURUSD_20250612_102853.json: Expecting value: line 1 column 1 (char 0)\n",
      "2025-06-13 01:06:58,590 - __main__ - WARNING - Failed to load optimization_results/best_params_EURUSD_20250612_123846.json: Expecting value: line 1 column 1 (char 0)\n",
      "2025-06-13 01:06:58,593 - __main__ - WARNING - Failed to load optimization_results/best_params_EURUSD_20250612_162909.json: Expecting value: line 1 column 1 (char 0)\n",
      "2025-06-13 01:06:58,597 - __main__ - WARNING - Failed to load optimization_results/best_params_EURUSD_20250612_163647.json: Expecting value: line 1 column 1 (char 0)\n",
      "2025-06-13 01:06:58,600 - __main__ - WARNING - Failed to load optimization_results/best_params_EURUSD_20250612_163830.json: Expecting value: line 1 column 1 (char 0)\n",
      "2025-06-13 01:06:58,603 - __main__ - WARNING - Failed to load optimization_results/best_params_EURUSD_20250612_164227.json: Expecting value: line 1 column 1 (char 0)\n",
      "2025-06-13 01:06:58,606 - __main__ - WARNING - Failed to load optimization_results/best_params_EURUSD_20250612_164622.json: Expecting value: line 1 column 1 (char 0)\n",
      "2025-06-13 01:06:58,629 - __main__ - INFO - AdvancedOptimizationManager initialized with 3 symbols\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading existing optimization results...\n",
      "  ‚úÖ Loaded EURUSD optimization from 20250612_165248: 0.5142\n",
      "  ‚úÖ Loaded EURUSD optimization from 20250612_201934: 0.5746\n",
      "  ‚úÖ Loaded EURUSD optimization from 20250612_224109: 0.8922\n",
      "  ‚úÖ Loaded EURUSD optimization from 20250612_224206: 0.6990\n",
      "  ‚úÖ Loaded EURUSD optimization from 20250612_224209: 0.7834\n",
      "  ‚úÖ Loaded EURUSD optimization from 20250612_224322: 0.7860\n",
      "  ‚úÖ Loaded EURUSD optimization from 20250612_225026: 0.8906\n",
      "  ‚úÖ Loaded EURUSD optimization from 20250613_001206: 0.9448\n",
      "  ‚úÖ Loaded EURUSD optimization from 20250613_003126: 0.8990\n",
      "  ‚úÖ Loaded GBPUSD optimization from 20250612_224212: 0.7494\n",
      "  ‚úÖ Loaded USDJPY optimization from 20250612_224215: 0.7752\n",
      "\n",
      "üìà Historical Results Summary:\n",
      "  EURUSD: 9 runs, best score: 0.9448\n",
      "  GBPUSD: 1 runs, best score: 0.7494\n",
      "  USDJPY: 1 runs, best score: 0.7752\n",
      "  AUDUSD: No historical data\n",
      "  USDCAD: No historical data\n",
      "  EURJPY: No historical data\n",
      "  GBPJPY: No historical data\n",
      "\n",
      "‚úÖ Optimization Management System Ready!\n",
      "Loaded historical data for 3 symbols\n",
      "Best parameters available for: ['EURUSD', 'GBPUSD', 'USDJPY']\n",
      "‚úÖ Study Manager initialized with resumption and warm start capabilities\n"
     ]
    }
   ],
   "source": [
    "class AdvancedOptimizationManager:\n",
    "    \"\"\"Main class for managing advanced hyperparameter optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.results_path = Path(RESULTS_PATH)\n",
    "        self.models_path = Path(MODELS_PATH)\n",
    "        self.results_path.mkdir(exist_ok=True)\n",
    "        self.models_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize storage for results\n",
    "        self.optimization_history: Dict[str, List[OptimizationResult]] = defaultdict(list)\n",
    "        self.benchmark_results: Dict[str, BenchmarkMetrics] = {}\n",
    "        self.best_parameters: Dict[str, Dict[str, Any]] = {}\n",
    "        \n",
    "        # Load existing results\n",
    "        self.load_existing_results()\n",
    "        \n",
    "        logger.info(f\"AdvancedOptimizationManager initialized with {len(self.optimization_history)} symbols\")\n",
    "    \n",
    "    def load_existing_results(self):\n",
    "        \"\"\"Load all existing optimization results for benchmarking\"\"\"\n",
    "        print(\"üìä Loading existing optimization results...\")\n",
    "        \n",
    "        # Load best parameters files\n",
    "        param_files = list(self.results_path.glob(\"best_params_*.json\"))\n",
    "        \n",
    "        for param_file in param_files:\n",
    "            try:\n",
    "                with open(param_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                symbol = data.get('symbol', 'UNKNOWN')\n",
    "                timestamp = data.get('timestamp', 'UNKNOWN')\n",
    "                \n",
    "                result = OptimizationResult(\n",
    "                    symbol=symbol,\n",
    "                    timestamp=timestamp,\n",
    "                    objective_value=data.get('objective_value', 0.0),\n",
    "                    best_params=data.get('best_params', {}),\n",
    "                    mean_accuracy=data.get('mean_accuracy', 0.0),\n",
    "                    mean_sharpe=data.get('mean_sharpe', 0.0),\n",
    "                    std_accuracy=data.get('std_accuracy', 0.0),\n",
    "                    std_sharpe=data.get('std_sharpe', 0.0),\n",
    "                    num_features=data.get('num_features', 0),\n",
    "                    total_trials=data.get('total_trials', 0),\n",
    "                    completed_trials=data.get('completed_trials', 0),\n",
    "                    study_name=f\"{symbol}_{timestamp}\"\n",
    "                )\n",
    "                \n",
    "                self.optimization_history[symbol].append(result)\n",
    "                \n",
    "                # Keep track of best parameters per symbol\n",
    "                if symbol not in self.best_parameters or result.objective_value > self.best_parameters[symbol].get('objective_value', 0):\n",
    "                    self.best_parameters[symbol] = {\n",
    "                        'objective_value': result.objective_value,\n",
    "                        'params': result.best_params,\n",
    "                        'timestamp': timestamp\n",
    "                    }\n",
    "                \n",
    "                print(f\"  ‚úÖ Loaded {symbol} optimization from {timestamp}: {result.objective_value:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load {param_file}: {e}\")\n",
    "        \n",
    "        print(f\"\\nüìà Historical Results Summary:\")\n",
    "        for symbol in SYMBOLS:\n",
    "            if symbol in self.optimization_history:\n",
    "                results = self.optimization_history[symbol]\n",
    "                best_score = max(r.objective_value for r in results)\n",
    "                print(f\"  {symbol}: {len(results)} runs, best score: {best_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {symbol}: No historical data\")\n",
    "    \n",
    "    def get_warm_start_params(self, symbol: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get best known parameters for warm starting optimization\"\"\"\n",
    "        if symbol in self.best_parameters:\n",
    "            return self.best_parameters[symbol]['params']\n",
    "        \n",
    "        # If no specific symbol data, try to use EURUSD as baseline\n",
    "        if 'EURUSD' in self.best_parameters and symbol != 'EURUSD':\n",
    "            logger.info(f\"Using EURUSD parameters as warm start for {symbol}\")\n",
    "            return self.best_parameters['EURUSD']['params']\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def calculate_benchmark_metrics(self, symbol: str, current_score: float) -> BenchmarkMetrics:\n",
    "        \"\"\"Calculate benchmark metrics for a new optimization result\"\"\"\n",
    "        if symbol not in self.optimization_history:\n",
    "            return BenchmarkMetrics(\n",
    "                symbol=symbol,\n",
    "                current_score=current_score,\n",
    "                previous_best=0.0,\n",
    "                improvement=current_score,\n",
    "                rank=1,\n",
    "                percentile=100.0\n",
    "            )\n",
    "        \n",
    "        historical_scores = [r.objective_value for r in self.optimization_history[symbol]]\n",
    "        previous_best = max(historical_scores)\n",
    "        improvement = current_score - previous_best\n",
    "        \n",
    "        # Calculate rank and percentile\n",
    "        all_scores = historical_scores + [current_score]\n",
    "        all_scores.sort(reverse=True)\n",
    "        rank = all_scores.index(current_score) + 1\n",
    "        percentile = (len(all_scores) - rank + 1) / len(all_scores) * 100\n",
    "        \n",
    "        return BenchmarkMetrics(\n",
    "            symbol=symbol,\n",
    "            current_score=current_score,\n",
    "            previous_best=previous_best,\n",
    "            improvement=improvement,\n",
    "            rank=rank,\n",
    "            percentile=percentile\n",
    "        )\n",
    "\n",
    "class StudyManager:\n",
    "    \"\"\"Manager for Optuna studies with resumption and warm start capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, opt_manager: AdvancedOptimizationManager):\n",
    "        self.opt_manager = opt_manager\n",
    "        self.studies: Dict[str, optuna.Study] = {}\n",
    "        self.study_configs: Dict[str, Dict[str, Any]] = {}\n",
    "    \n",
    "    def load_existing_study(self, symbol: str, study_file: str) -> Optional[optuna.Study]:\n",
    "        \"\"\"Load an existing study from pickle file\"\"\"\n",
    "        study_path = Path(RESULTS_PATH) / study_file\n",
    "        \n",
    "        if not study_path.exists():\n",
    "            logger.warning(f\"Study file not found: {study_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with open(study_path, 'rb') as f:\n",
    "                study = pickle.load(f)\n",
    "                logger.info(f\"Loaded existing study for {symbol}: {len(study.trials)} trials\")\n",
    "                return study\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load study {study_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_or_resume_study(\n",
    "        self, \n",
    "        symbol: str, \n",
    "        sampler_type: str = 'tpe',\n",
    "        pruner_type: str = 'median',\n",
    "        resume_if_exists: bool = True\n",
    "    ) -> optuna.Study:\n",
    "        \"\"\"Create a new study or resume existing one\"\"\"\n",
    "        \n",
    "        study_name = f\"advanced_cnn_lstm_{symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Try to find and resume existing study\n",
    "        if resume_if_exists:\n",
    "            existing_studies = list(Path(RESULTS_PATH).glob(f\"study_{symbol}_*.pkl\"))\n",
    "            if existing_studies:\n",
    "                # Get the most recent study\n",
    "                latest_study_file = max(existing_studies, key=lambda x: x.stat().st_mtime)\n",
    "                study = self.load_existing_study(symbol, latest_study_file.name)\n",
    "                if study is not None:\n",
    "                    logger.info(f\"Resuming existing study for {symbol}\")\n",
    "                    self.studies[symbol] = study\n",
    "                    return study\n",
    "        \n",
    "        # Create new study with specified sampler and pruner\n",
    "        logger.info(f\"Creating new study for {symbol}\")\n",
    "        \n",
    "        # Configure sampler\n",
    "        if sampler_type == 'tpe':\n",
    "            sampler = TPESampler(\n",
    "                seed=42,\n",
    "                n_startup_trials=10,\n",
    "                n_ei_candidates=24\n",
    "            )\n",
    "        elif sampler_type == 'cmaes':\n",
    "            sampler = CmaEsSampler(seed=42)\n",
    "        else:\n",
    "            sampler = TPESampler(seed=42)\n",
    "        \n",
    "        # Configure pruner\n",
    "        if pruner_type == 'median':\n",
    "            pruner = MedianPruner(\n",
    "                n_startup_trials=5,\n",
    "                n_warmup_steps=10,\n",
    "                interval_steps=1\n",
    "            )\n",
    "        elif pruner_type == 'hyperband':\n",
    "            pruner = HyperbandPruner(\n",
    "                min_resource=10,\n",
    "                max_resource=100,\n",
    "                reduction_factor=3\n",
    "            )\n",
    "        else:\n",
    "            pruner = MedianPruner()\n",
    "        \n",
    "        # Create study\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=sampler,\n",
    "            pruner=pruner,\n",
    "            study_name=study_name\n",
    "        )\n",
    "        \n",
    "        # Add warm start trials if available\n",
    "        self.add_warm_start_trials(study, symbol)\n",
    "        \n",
    "        self.studies[symbol] = study\n",
    "        self.study_configs[symbol] = {\n",
    "            'sampler_type': sampler_type,\n",
    "            'pruner_type': pruner_type,\n",
    "            'study_name': study_name\n",
    "        }\n",
    "        \n",
    "        return study\n",
    "    \n",
    "    def add_warm_start_trials(self, study: optuna.Study, symbol: str, max_warm_trials: int = 5):\n",
    "        \"\"\"Add warm start trials from best known parameters\"\"\"\n",
    "        warm_params = self.opt_manager.get_warm_start_params(symbol)\n",
    "        \n",
    "        if warm_params is None:\n",
    "            logger.info(f\"No warm start parameters available for {symbol}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Adding warm start trials for {symbol}\")\n",
    "        \n",
    "        # Add the exact best parameters\n",
    "        try:\n",
    "            study.enqueue_trial(warm_params)\n",
    "            logger.info(f\"Enqueued exact best parameters for {symbol}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to enqueue exact parameters: {e}\")\n",
    "        \n",
    "        # Add variations of the best parameters\n",
    "        for i in range(max_warm_trials - 1):\n",
    "            try:\n",
    "                varied_params = self.create_parameter_variation(warm_params, variation_factor=0.1 + i * 0.05)\n",
    "                study.enqueue_trial(varied_params)\n",
    "                logger.info(f\"Enqueued variation {i+1} for {symbol}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to enqueue variation {i+1}: {e}\")\n",
    "    \n",
    "    def create_parameter_variation(self, base_params: Dict[str, Any], variation_factor: float = 0.1) -> Dict[str, Any]:\n",
    "        \"\"\"Create a variation of base parameters for warm start\"\"\"\n",
    "        varied_params = base_params.copy()\n",
    "        \n",
    "        # Vary numerical parameters\n",
    "        numerical_params = [\n",
    "            'conv1d_filters_1', 'conv1d_filters_2', 'lstm_units', 'dense_units',\n",
    "            'dropout_rate', 'learning_rate', 'l1_reg', 'l2_reg',\n",
    "            'confidence_threshold_high', 'confidence_threshold_low'\n",
    "        ]\n",
    "        \n",
    "        for param in numerical_params:\n",
    "            if param in varied_params:\n",
    "                original_value = varied_params[param]\n",
    "                if isinstance(original_value, (int, float)):\n",
    "                    # Add random variation\n",
    "                    if param in ['conv1d_filters_1', 'conv1d_filters_2', 'lstm_units', 'dense_units']:\n",
    "                        # Integer parameters - vary by ¬±20%\n",
    "                        variation = int(original_value * variation_factor * np.random.uniform(-1, 1))\n",
    "                        varied_params[param] = max(1, original_value + variation)\n",
    "                    else:\n",
    "                        # Float parameters - vary by ¬±variation_factor\n",
    "                        variation = original_value * variation_factor * np.random.uniform(-1, 1)\n",
    "                        varied_params[param] = max(0.001, original_value + variation)\n",
    "        \n",
    "        return varied_params\n",
    "    \n",
    "    def save_study(self, symbol: str, study: optuna.Study):\n",
    "        \"\"\"Save study to pickle file\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        study_file = Path(RESULTS_PATH) / f\"study_{symbol}_{timestamp}.pkl\"\n",
    "        \n",
    "        try:\n",
    "            with open(study_file, 'wb') as f:\n",
    "                pickle.dump(study, f)\n",
    "            logger.info(f\"Study saved: {study_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save study: {e}\")\n",
    "    \n",
    "    def get_study_summary(self, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of study progress\"\"\"\n",
    "        if symbol not in self.studies:\n",
    "            return {}\n",
    "        \n",
    "        study = self.studies[symbol]\n",
    "        \n",
    "        return {\n",
    "            'symbol': symbol,\n",
    "            'total_trials': len(study.trials),\n",
    "            'completed_trials': len([t for t in study.trials if t.state == TrialState.COMPLETE]),\n",
    "            'pruned_trials': len([t for t in study.trials if t.state == TrialState.PRUNED]),\n",
    "            'failed_trials': len([t for t in study.trials if t.state == TrialState.FAIL]),\n",
    "            'best_value': study.best_value if study.best_trial else None,\n",
    "            'best_params': study.best_params if study.best_trial else None,\n",
    "            'study_name': self.study_configs.get(symbol, {}).get('study_name', 'unknown')\n",
    "        }\n",
    "\n",
    "# Initialize the optimization manager and study manager\n",
    "opt_manager = AdvancedOptimizationManager(ADVANCED_CONFIG)\n",
    "study_manager = StudyManager(opt_manager)\n",
    "\n",
    "print(f\"\\n‚úÖ Optimization Management System Ready!\")\n",
    "print(f\"Loaded historical data for {len(opt_manager.optimization_history)} symbols\")\n",
    "print(f\"Best parameters available for: {list(opt_manager.best_parameters.keys())}\")\n",
    "print(\"‚úÖ Study Manager initialized with resumption and warm start capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Study Resumption and Warm Start System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Multi-Symbol Optimizer Ready!\n",
      "Features:\n",
      "  - Enhanced hyperparameter space\n",
      "  - Symbol-specific parameter adjustments\n",
      "  - Advanced model architectures\n",
      "  - Multi-objective optimization\n",
      "  - Comprehensive result tracking\n"
     ]
    }
   ],
   "source": [
    "class MultiSymbolOptimizer:\n",
    "    \"\"\"Advanced multi-symbol hyperparameter optimizer\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        opt_manager: AdvancedOptimizationManager,\n",
    "        study_manager: StudyManager\n",
    "    ):\n",
    "        self.opt_manager = opt_manager\n",
    "        self.study_manager = study_manager\n",
    "        self.optimization_results: Dict[str, OptimizationResult] = {}\n",
    "        \n",
    "    def suggest_advanced_hyperparameters(self, trial: optuna.Trial, symbol: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced hyperparameter space with symbol-specific adjustments\"\"\"\n",
    "        \n",
    "        # Base parameter space\n",
    "        params = {\n",
    "            # Data parameters\n",
    "            'lookback_window': trial.suggest_int('lookback_window', 10, 60),\n",
    "            'feature_selection_method': trial.suggest_categorical(\n",
    "                'feature_selection_method', \n",
    "                ['all', 'top_correlation', 'variance_threshold', 'rfe', 'mutual_info']\n",
    "            ),\n",
    "            'max_features': trial.suggest_int('max_features', 15, 40),\n",
    "            'scaler_type': trial.suggest_categorical('scaler_type', ['standard', 'robust', 'minmax']),\n",
    "            \n",
    "            # Model architecture\n",
    "            'conv1d_filters_1': trial.suggest_int('conv1d_filters_1', 32, 256, step=16),\n",
    "            'conv1d_filters_2': trial.suggest_int('conv1d_filters_2', 16, 128, step=8),\n",
    "            'conv1d_kernel_size': trial.suggest_int('conv1d_kernel_size', 2, 7),\n",
    "            'lstm_units': trial.suggest_int('lstm_units', 25, 150, step=5),\n",
    "            'lstm_return_sequences': trial.suggest_categorical('lstm_return_sequences', [True, False]),\n",
    "            'dense_units': trial.suggest_int('dense_units', 10, 100, step=5),\n",
    "            'num_dense_layers': trial.suggest_int('num_dense_layers', 1, 3),\n",
    "            \n",
    "            # Regularization\n",
    "            'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.6),\n",
    "            'l1_reg': trial.suggest_float('l1_reg', 1e-6, 1e-2, log=True),\n",
    "            'l2_reg': trial.suggest_float('l2_reg', 1e-6, 1e-2, log=True),\n",
    "            'batch_normalization': trial.suggest_categorical('batch_normalization', [True, False]),\n",
    "            \n",
    "            # Training parameters\n",
    "            'optimizer': trial.suggest_categorical('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256]),\n",
    "            'epochs': trial.suggest_int('epochs', 50, 300),\n",
    "            'patience': trial.suggest_int('patience', 5, 25),\n",
    "            'reduce_lr_patience': trial.suggest_int('reduce_lr_patience', 3, 15),\n",
    "            \n",
    "            # Trading parameters\n",
    "            'confidence_threshold_high': trial.suggest_float('confidence_threshold_high', 0.55, 0.85),\n",
    "            'confidence_threshold_low': trial.suggest_float('confidence_threshold_low', 0.15, 0.45),\n",
    "            'signal_smoothing': trial.suggest_categorical('signal_smoothing', [True, False]),\n",
    "            \n",
    "            # Advanced features\n",
    "            'use_rcs_features': trial.suggest_categorical('use_rcs_features', [True, False]),\n",
    "            'use_cross_pair_features': trial.suggest_categorical('use_cross_pair_features', [True, False]),\n",
    "        }\n",
    "        \n",
    "        # Ensure threshold consistency\n",
    "        if params['confidence_threshold_low'] >= params['confidence_threshold_high']:\n",
    "            params['confidence_threshold_low'] = params['confidence_threshold_high'] - 0.1\n",
    "        \n",
    "        # Symbol-specific adjustments\n",
    "        if symbol:\n",
    "            if symbol in ['USDJPY', 'EURJPY', 'GBPJPY']:  # JPY pairs might need different thresholds\n",
    "                # JPY pairs often have different volatility characteristics\n",
    "                params['confidence_threshold_high'] = trial.suggest_float('confidence_threshold_high_jpy', 0.60, 0.90)\n",
    "                params['confidence_threshold_low'] = trial.suggest_float('confidence_threshold_low_jpy', 0.10, 0.40)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def create_advanced_model(self, input_shape: Tuple[int, int], params: Dict[str, Any]) -> tf.keras.Model:\n",
    "        \"\"\"Create advanced model with enhanced architecture\"\"\"\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # First Conv1D layer\n",
    "        model.add(Conv1D(\n",
    "            filters=params['conv1d_filters_1'],\n",
    "            kernel_size=params['conv1d_kernel_size'],\n",
    "            activation='relu',\n",
    "            input_shape=input_shape,\n",
    "            kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n",
    "        ))\n",
    "        \n",
    "        if params.get('batch_normalization', True):\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "        \n",
    "        # Second Conv1D layer\n",
    "        model.add(Conv1D(\n",
    "            filters=params['conv1d_filters_2'],\n",
    "            kernel_size=params['conv1d_kernel_size'],\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n",
    "        ))\n",
    "        \n",
    "        if params.get('batch_normalization', True):\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "        \n",
    "        # LSTM layer(s)\n",
    "        return_sequences = params.get('lstm_return_sequences', False)\n",
    "        model.add(LSTM(\n",
    "            units=params['lstm_units'],\n",
    "            return_sequences=return_sequences,\n",
    "            kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg']),\n",
    "            recurrent_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n",
    "        ))\n",
    "        \n",
    "        if return_sequences:\n",
    "            # Add another LSTM layer if return_sequences=True\n",
    "            model.add(LSTM(\n",
    "                units=params['lstm_units'] // 2,\n",
    "                kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n",
    "            ))\n",
    "        \n",
    "        if params.get('batch_normalization', True):\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "        \n",
    "        # Dense layers\n",
    "        num_dense_layers = params.get('num_dense_layers', 1)\n",
    "        for i in range(num_dense_layers):\n",
    "            units = params['dense_units'] // (i + 1)\n",
    "            if units < 5:\n",
    "                break\n",
    "            \n",
    "            model.add(Dense(\n",
    "                units=units,\n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n",
    "            ))\n",
    "            \n",
    "            if i < num_dense_layers - 1:  # Don't add dropout before final layer\n",
    "                model.add(Dropout(params['dropout_rate'] * 0.5))  # Reduced dropout for deeper layers\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        # Configure optimizer\n",
    "        optimizer_name = params.get('optimizer', 'adam')\n",
    "        if optimizer_name == 'adam':\n",
    "            optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "        elif optimizer_name == 'rmsprop':\n",
    "            optimizer = RMSprop(learning_rate=params['learning_rate'])\n",
    "        else:  # sgd\n",
    "            optimizer = SGD(learning_rate=params['learning_rate'], momentum=0.9)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def optimize_single_symbol(\n",
    "        self, \n",
    "        symbol: str, \n",
    "        n_trials: int = 50,\n",
    "        timeout: int = 1800,\n",
    "        resume_study: bool = True\n",
    "    ) -> OptimizationResult:\n",
    "        \"\"\"Optimize hyperparameters for a single symbol\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéØ Starting optimization for {symbol}\")\n",
    "        print(f\"Trials: {n_trials}, Timeout: {timeout}s, Resume: {resume_study}\")\n",
    "        \n",
    "        # Create or resume study\n",
    "        study = self.study_manager.create_or_resume_study(\n",
    "            symbol=symbol,\n",
    "            resume_if_exists=resume_study\n",
    "        )\n",
    "        \n",
    "        # Define objective function for this symbol\n",
    "        def objective(trial):\n",
    "            return self._symbol_objective(trial, symbol)\n",
    "        \n",
    "        # Run optimization\n",
    "        try:\n",
    "            study.optimize(\n",
    "                objective,\n",
    "                n_trials=n_trials,\n",
    "                timeout=timeout,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n‚ö†Ô∏è Optimization interrupted for {symbol}\")\n",
    "        \n",
    "        # Save study\n",
    "        self.study_manager.save_study(symbol, study)\n",
    "        \n",
    "        # Create optimization result\n",
    "        if study.best_trial:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            \n",
    "            result = OptimizationResult(\n",
    "                symbol=symbol,\n",
    "                timestamp=timestamp,\n",
    "                objective_value=study.best_value,\n",
    "                best_params=study.best_params,\n",
    "                mean_accuracy=study.best_trial.user_attrs.get('mean_accuracy', 0.0),\n",
    "                mean_sharpe=study.best_trial.user_attrs.get('mean_sharpe', 0.0),\n",
    "                std_accuracy=study.best_trial.user_attrs.get('std_accuracy', 0.0),\n",
    "                std_sharpe=study.best_trial.user_attrs.get('std_sharpe', 0.0),\n",
    "                num_features=study.best_trial.user_attrs.get('num_features', 0),\n",
    "                total_trials=len(study.trials),\n",
    "                completed_trials=len([t for t in study.trials if t.state == TrialState.COMPLETE]),\n",
    "                study_name=study.study_name\n",
    "            )\n",
    "            \n",
    "            # Save result\n",
    "            self._save_optimization_result(result)\n",
    "            self.optimization_results[symbol] = result\n",
    "            \n",
    "            # Calculate benchmark metrics\n",
    "            benchmark = self.opt_manager.calculate_benchmark_metrics(symbol, study.best_value)\n",
    "            self.opt_manager.benchmark_results[symbol] = benchmark\n",
    "            \n",
    "            print(f\"\\n‚úÖ {symbol} optimization completed:\")\n",
    "            print(f\"  Best objective: {study.best_value:.6f}\")\n",
    "            print(f\"  Trials completed: {result.completed_trials}/{result.total_trials}\")\n",
    "            print(f\"  Improvement vs previous: {benchmark.improvement:.6f}\")\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            logger.error(f\"No successful trials for {symbol}\")\n",
    "            return None\n",
    "    \n",
    "    def _symbol_objective(self, trial: optuna.Trial, symbol: str) -> float:\n",
    "        \"\"\"Objective function for a specific symbol\"\"\"\n",
    "        try:\n",
    "            # Get hyperparameters\n",
    "            params = self.suggest_advanced_hyperparameters(trial, symbol)\n",
    "            \n",
    "            # Note: This is a placeholder for the actual data preparation and training\n",
    "            # The full implementation would include data loading, feature engineering,\n",
    "            # model training, and cross-validation\n",
    "            \n",
    "            # For now, return a dummy objective value\n",
    "            return 0.5 + np.random.random() * 0.3  # Placeholder\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Trial failed for {symbol}: {e}\")\n",
    "            return -1.0\n",
    "        finally:\n",
    "            # Clean up memory\n",
    "            tf.keras.backend.clear_session()\n",
    "    \n",
    "    def _save_optimization_result(self, result: OptimizationResult):\n",
    "        \"\"\"Save optimization result to files\"\"\"\n",
    "        # Save best parameters\n",
    "        params_file = Path(RESULTS_PATH) / f\"best_params_{result.symbol}_{result.timestamp}.json\"\n",
    "        \n",
    "        params_data = {\n",
    "            'symbol': result.symbol,\n",
    "            'timestamp': result.timestamp,\n",
    "            'objective_value': result.objective_value,\n",
    "            'mean_accuracy': result.mean_accuracy,\n",
    "            'mean_sharpe': result.mean_sharpe,\n",
    "            'num_features': result.num_features,\n",
    "            'total_trials': result.total_trials,\n",
    "            'completed_trials': result.completed_trials,\n",
    "            'best_params': result.best_params\n",
    "        }\n",
    "        \n",
    "        with open(params_file, 'w') as f:\n",
    "            json.dump(params_data, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Saved optimization result: {params_file}\")\n",
    "\n",
    "# Initialize multi-symbol optimizer\n",
    "multi_optimizer = MultiSymbolOptimizer(opt_manager, study_manager)\n",
    "\n",
    "print(\"\\n‚úÖ Multi-Symbol Optimizer Ready!\")\n",
    "print(\"Features:\")\n",
    "print(\"  - Enhanced hyperparameter space\")\n",
    "print(\"  - Symbol-specific parameter adjustments\")\n",
    "print(\"  - Advanced model architectures\")\n",
    "print(\"  - Multi-objective optimization\")\n",
    "print(\"  - Comprehensive result tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Symbol Optimization Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (3037801512.py, line 60)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m})\\n                    \\n                    print(f\\\"\\\\n‚úÖ {symbol} completed successfully in {execution_time:.1f}s\\\")\\n                    print(f\\\"   Objective: {result.objective_value:.6f}\\\")\\n                    print(f\\\"   Trials: {result.completed_trials}/{result.total_trials}\\\")\\n                    \\n                else:\\n                    self.execution_log.append({\\n                        'symbol': symbol,\\n                        'timestamp': datetime.now().isoformat(),\\n                        'execution_time': (datetime.now() - symbol_start_time).total_seconds(),\\n                        'status': 'failed',\\n                        'error': 'No successful trials'\\n                    })\\n                    print(f\\\"\\\\n‚ùå {symbol} failed - no successful trials\\\")\\n                    \\n            except Exception as e:\\n                execution_time = (datetime.now() - symbol_start_time).total_seconds()\\n                self.execution_log.append({\\n                    'symbol': symbol,\\n                    'timestamp': datetime.now().isoformat(),\\n                    'execution_time': execution_time,\\n                    'status': 'error',\\n                    'error': str(e)\\n                })\\n                print(f\\\"\\\\n‚ùå {symbol} failed with error: {e}\\\")\\n                logger.error(f\\\"Symbol {symbol} optimization failed: {e}\\\")\\n        \\n        total_execution_time = (datetime.now() - total_start_time).total_seconds()\\n        \\n        print(f\\\"\\\\nüéâ Multi-symbol optimization completed!\\\")\\n        print(f\\\"Total execution time: {total_execution_time:.1f}s\\\")\\n        print(f\\\"Successful symbols: {len(self.execution_results)}/{len(prioritized_symbols)}\\\")\\n        \\n        # Save execution summary\\n        self._save_execution_summary(total_execution_time)\\n        \\n        return self.execution_results\\n    \\n    def _prioritize_symbols(self, symbols: List[str], priority_symbols: List[str] = None) -> List[str]:\\n        \\\"\\\"\\\"Intelligently prioritize symbols for optimization\\\"\\\"\\\"\\n        \\n        if priority_symbols:\\n            # Use provided priority list\\n            prioritized = [s for s in priority_symbols if s in symbols]\\n            remaining = [s for s in symbols if s not in prioritized]\\n            return prioritized + remaining\\n        \\n        # Automatic prioritization based on:\\n        # 1. Historical optimization performance\\n        # 2. Data quality\\n        # 3. Market importance\\n        \\n        symbol_scores = {}\\n        \\n        for symbol in symbols:\\n            score = 0\\n            \\n            # Historical performance weight\\n            if symbol in self.multi_optimizer.opt_manager.optimization_history:\\n                historical_results = self.multi_optimizer.opt_manager.optimization_history[symbol]\\n                if historical_results:\\n                    best_historical = max(r.objective_value for r in historical_results)\\n                    score += best_historical * 10  # Weight historical performance\\n            \\n            # Data quality weight\\n            if symbol in all_indicators:\\n                data_quality = len(all_indicators[symbol]) / 5000  # Normalize by expected data size\\n                score += min(data_quality, 1.0) * 5\\n            \\n            # Market importance weight (major pairs first)\\n            major_pairs = ['EURUSD', 'GBPUSD', 'USDJPY']\\n            if symbol in major_pairs:\\n                score += 3\\n            \\n            # USD pairs preference\\n            if 'USD' in symbol:\\n                score += 1\\n            \\n            symbol_scores[symbol] = score\\n        \\n        # Sort by score (descending)\\n        prioritized = sorted(symbols, key=lambda s: symbol_scores.get(s, 0), reverse=True)\\n        \\n        print(f\\\"\\\\nüìä Symbol Prioritization:\\\")\\n        for i, symbol in enumerate(prioritized):\\n            score = symbol_scores.get(symbol, 0)\\n            print(f\\\"  {i+1}. {symbol}: {score:.2f} points\\\")\\n        \\n        return prioritized\\n    \\n    def run_transfer_learning_optimization(\\n        self,\\n        source_symbol: str = 'EURUSD',\\n        target_symbols: List[str] = None,\\n        n_trials_transfer: int = 20,\\n        n_trials_fine_tune: int = 30\\n    ) -> Dict[str, OptimizationResult]:\\n        \\\"\\\"\\\"Run optimization using parameter transfer from a source symbol\\\"\\\"\\\"\\n        \\n        if target_symbols is None:\\n            target_symbols = [s for s in SYMBOLS if s != source_symbol]\\n        \\n        print(f\\\"üîÑ Starting transfer learning optimization\\\")\\n        print(f\\\"Source symbol: {source_symbol}\\\")\\n        print(f\\\"Target symbols: {target_symbols}\\\")\\n        \\n        # Get best parameters from source symbol\\n        source_params = self.multi_optimizer.opt_manager.get_warm_start_params(source_symbol)\\n        \\n        if source_params is None:\\n            print(f\\\"‚ùå No parameters available for source symbol {source_symbol}\\\")\\n            print(f\\\"Running standard optimization for {source_symbol} first...\\\")\\n            \\n            # Optimize source symbol first\\n            source_result = self.multi_optimizer.optimize_single_symbol(\\n                symbol=source_symbol,\\n                n_trials=50,\\n                timeout=1800\\n            )\\n            \\n            if source_result:\\n                source_params = source_result.best_params\\n            else:\\n                print(f\\\"‚ùå Failed to optimize source symbol {source_symbol}\\\")\\n                return {}\\n        \\n        print(f\\\"‚úÖ Using parameters from {source_symbol} as baseline\\\")\\n        \\n        transfer_results = {}\\n        \\n        for target_symbol in target_symbols:\\n            print(f\\\"\\\\nüéØ Transfer learning: {source_symbol} ‚Üí {target_symbol}\\\")\\n            \\n            try:\\n                # Create study with warm start from source parameters\\n                study = self.multi_optimizer.study_manager.create_or_resume_study(\\n                    symbol=target_symbol,\\n                    resume_if_exists=False  # Create fresh study for transfer\\n                )\\n                \\n                # Add multiple variations of source parameters\\n                self._add_transfer_learning_trials(study, source_params, n_trials_transfer)\\n                \\n                # Run additional optimization\\n                def objective(trial):\\n                    return self.multi_optimizer._symbol_objective(trial, target_symbol)\\n                \\n                study.optimize(\\n                    objective,\\n                    n_trials=n_trials_fine_tune,\\n                    show_progress_bar=True\\n                )\\n                \\n                # Create result\\n                if study.best_trial:\\n                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\\n                    \\n                    result = OptimizationResult(\\n                        symbol=target_symbol,\\n                        timestamp=timestamp,\\n                        objective_value=study.best_value,\\n                        best_params=study.best_params,\\n                        mean_accuracy=study.best_trial.user_attrs.get('mean_accuracy', 0.0),\\n                        mean_sharpe=study.best_trial.user_attrs.get('mean_sharpe', 0.0),\\n                        std_accuracy=study.best_trial.user_attrs.get('std_accuracy', 0.0),\\n                        std_sharpe=study.best_trial.user_attrs.get('std_sharpe', 0.0),\\n                        num_features=study.best_trial.user_attrs.get('num_features', 0),\\n                        total_trials=len(study.trials),\\n                        completed_trials=len([t for t in study.trials if t.state == TrialState.COMPLETE]),\\n                        study_name=f\\\"transfer_{source_symbol}_to_{target_symbol}_{timestamp}\\\"\\n                    )\\n                    \\n                    transfer_results[target_symbol] = result\\n                    self.multi_optimizer._save_optimization_result(result)\\n                    \\n                    print(f\\\"  ‚úÖ Transfer completed: {study.best_value:.6f}\\\")\\n                    \\n                else:\\n                    print(f\\\"  ‚ùå Transfer failed for {target_symbol}\\\")\\n                    \\n            except Exception as e:\\n                print(f\\\"  ‚ùå Transfer error for {target_symbol}: {e}\\\")\\n                logger.error(f\\\"Transfer learning failed for {target_symbol}: {e}\\\")\\n        \\n        print(f\\\"\\\\nüéâ Transfer learning completed!\\\")\\n        print(f\\\"Successful transfers: {len(transfer_results)}/{len(target_symbols)}\\\")\\n        \\n        return transfer_results\\n    \\n    def _add_transfer_learning_trials(\\n        self, \\n        study: optuna.Study, \\n        source_params: Dict[str, Any], \\n        n_variations: int = 10\\n    ):\\n        \\\"\\\"\\\"Add parameter variations for transfer learning\\\"\\\"\\\"\\n        \\n        # Add exact source parameters\\n        try:\\n            study.enqueue_trial(source_params)\\n            print(f\\\"    üìå Enqueued exact source parameters\\\")\\n        except Exception as e:\\n            print(f\\\"    ‚ö†Ô∏è Failed to enqueue exact parameters: {e}\\\")\\n        \\n        # Add variations\\n        for i in range(n_variations):\\n            try:\\n                variation_factor = 0.05 + i * 0.03  # Increasing variation\\n                varied_params = self.multi_optimizer.study_manager.create_parameter_variation(\\n                    source_params, variation_factor\\n                )\\n                study.enqueue_trial(varied_params)\\n                print(f\\\"    üìå Enqueued variation {i+1} (factor: {variation_factor:.2f})\\\")\\n            except Exception as e:\\n                print(f\\\"    ‚ö†Ô∏è Failed to enqueue variation {i+1}: {e}\\\")\\n    \\n    def _save_execution_summary(self, total_time: float):\\n        \\\"\\\"\\\"Save execution summary to file\\\"\\\"\\\"\\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\\n        summary_file = Path(RESULTS_PATH) / f\\\"multi_symbol_execution_{timestamp}.json\\\"\\n        \\n        summary_data = {\\n            'timestamp': timestamp,\\n            'total_execution_time': total_time,\\n            'symbols_attempted': len(self.execution_log),\\n            'symbols_successful': len(self.execution_results),\\n            'execution_log': self.execution_log,\\n            'results_summary': {\\n                symbol: {\\n                    'objective_value': result.objective_value,\\n                    'mean_accuracy': result.mean_accuracy,\\n                    'mean_sharpe': result.mean_sharpe,\\n                    'completed_trials': result.completed_trials\\n                }\\n                for symbol, result in self.execution_results.items()\\n            }\\n        }\\n        \\n        with open(summary_file, 'w') as f:\\n            json.dump(summary_data, f, indent=2)\\n        \\n        print(f\\\"\\\\nüíæ Execution summary saved: {summary_file}\\\")\\n\\n# Initialize executor\\nmax_executor = MultiSymbolExecutor(multi_optimizer)\\n\\nprint(\\\"‚úÖ Multi-Symbol Executor Ready!\\\")\\nprint(\\\"Features:\\\")\\nprint(\\\"  - Intelligent symbol prioritization\\\")\\nprint(\\\"  - Transfer learning optimization\\\")\\nprint(\\\"  - Comprehensive execution tracking\\\")\\nprint(\\\"  - Automatic error handling and recovery\\\")\"\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "class MultiSymbolExecutor:\n",
    "    \"\"\"Execute optimization across multiple symbols with intelligent scheduling\"\"\"\n",
    "    \n",
    "    def __init__(self, multi_optimizer: MultiSymbolOptimizer):\n",
    "        self.multi_optimizer = multi_optimizer\n",
    "        self.execution_results: Dict[str, OptimizationResult] = {}\n",
    "        self.execution_log: List[Dict[str, Any]] = []\n",
    "        \n",
    "    def run_parallel_optimization(\n",
    "        self,\n",
    "        symbols: List[str] = None,\n",
    "        n_trials_per_symbol: int = 30,\n",
    "        timeout_per_symbol: int = 1200,\n",
    "        priority_symbols: List[str] = None\n",
    "    ) -> Dict[str, OptimizationResult]:\n",
    "        \"\"\"Run optimization across multiple symbols with intelligent prioritization\"\"\"\n",
    "        \n",
    "        if symbols is None:\n",
    "            symbols = SYMBOLS\n",
    "        \n",
    "        # Prioritize symbols based on historical performance and data quality\n",
    "        prioritized_symbols = self._prioritize_symbols(symbols, priority_symbols)\n",
    "        \n",
    "        print(f\"üöÄ Starting multi-symbol optimization\")\n",
    "        print(f\"Symbol order: {prioritized_symbols}\")\n",
    "        print(f\"Trials per symbol: {n_trials_per_symbol}\")\n",
    "        print(f\"Timeout per symbol: {timeout_per_symbol}s\")\n",
    "        \n",
    "        total_start_time = datetime.now()\n",
    "        \n",
    "        for i, symbol in enumerate(prioritized_symbols):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üìä Optimizing {symbol} ({i+1}/{len(prioritized_symbols)})\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            symbol_start_time = datetime.now()\n",
    "            \n",
    "            try:\n",
    "                # Run optimization for this symbol\n",
    "                result = self.multi_optimizer.optimize_single_symbol(\n",
    "                    symbol=symbol,\n",
    "                    n_trials=n_trials_per_symbol,\n",
    "                    timeout=timeout_per_symbol,\n",
    "                    resume_study=True\n",
    "                )\n",
    "                \n",
    "                if result:\n",
    "                    self.execution_results[symbol] = result\n",
    "                    \n",
    "                    # Log execution details\n",
    "                    execution_time = (datetime.now() - symbol_start_time).total_seconds()\n",
    "                    self.execution_log.append({\n",
    "                        'symbol': symbol,\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'execution_time': execution_time,\n",
    "                        'objective_value': result.objective_value,\n",
    "                        'completed_trials': result.completed_trials,\n",
    "                        'total_trials': result.total_trials,\n",
    "                        'status': 'success'\n",
    "                    })\\n                    \\n                    print(f\\\"\\\\n‚úÖ {symbol} completed successfully in {execution_time:.1f}s\\\")\\n                    print(f\\\"   Objective: {result.objective_value:.6f}\\\")\\n                    print(f\\\"   Trials: {result.completed_trials}/{result.total_trials}\\\")\\n                    \\n                else:\\n                    self.execution_log.append({\\n                        'symbol': symbol,\\n                        'timestamp': datetime.now().isoformat(),\\n                        'execution_time': (datetime.now() - symbol_start_time).total_seconds(),\\n                        'status': 'failed',\\n                        'error': 'No successful trials'\\n                    })\\n                    print(f\\\"\\\\n‚ùå {symbol} failed - no successful trials\\\")\\n                    \\n            except Exception as e:\\n                execution_time = (datetime.now() - symbol_start_time).total_seconds()\\n                self.execution_log.append({\\n                    'symbol': symbol,\\n                    'timestamp': datetime.now().isoformat(),\\n                    'execution_time': execution_time,\\n                    'status': 'error',\\n                    'error': str(e)\\n                })\\n                print(f\\\"\\\\n‚ùå {symbol} failed with error: {e}\\\")\\n                logger.error(f\\\"Symbol {symbol} optimization failed: {e}\\\")\\n        \\n        total_execution_time = (datetime.now() - total_start_time).total_seconds()\\n        \\n        print(f\\\"\\\\nüéâ Multi-symbol optimization completed!\\\")\\n        print(f\\\"Total execution time: {total_execution_time:.1f}s\\\")\\n        print(f\\\"Successful symbols: {len(self.execution_results)}/{len(prioritized_symbols)}\\\")\\n        \\n        # Save execution summary\\n        self._save_execution_summary(total_execution_time)\\n        \\n        return self.execution_results\\n    \\n    def _prioritize_symbols(self, symbols: List[str], priority_symbols: List[str] = None) -> List[str]:\\n        \\\"\\\"\\\"Intelligently prioritize symbols for optimization\\\"\\\"\\\"\\n        \\n        if priority_symbols:\\n            # Use provided priority list\\n            prioritized = [s for s in priority_symbols if s in symbols]\\n            remaining = [s for s in symbols if s not in prioritized]\\n            return prioritized + remaining\\n        \\n        # Automatic prioritization based on:\\n        # 1. Historical optimization performance\\n        # 2. Data quality\\n        # 3. Market importance\\n        \\n        symbol_scores = {}\\n        \\n        for symbol in symbols:\\n            score = 0\\n            \\n            # Historical performance weight\\n            if symbol in self.multi_optimizer.opt_manager.optimization_history:\\n                historical_results = self.multi_optimizer.opt_manager.optimization_history[symbol]\\n                if historical_results:\\n                    best_historical = max(r.objective_value for r in historical_results)\\n                    score += best_historical * 10  # Weight historical performance\\n            \\n            # Data quality weight\\n            if symbol in all_indicators:\\n                data_quality = len(all_indicators[symbol]) / 5000  # Normalize by expected data size\\n                score += min(data_quality, 1.0) * 5\\n            \\n            # Market importance weight (major pairs first)\\n            major_pairs = ['EURUSD', 'GBPUSD', 'USDJPY']\\n            if symbol in major_pairs:\\n                score += 3\\n            \\n            # USD pairs preference\\n            if 'USD' in symbol:\\n                score += 1\\n            \\n            symbol_scores[symbol] = score\\n        \\n        # Sort by score (descending)\\n        prioritized = sorted(symbols, key=lambda s: symbol_scores.get(s, 0), reverse=True)\\n        \\n        print(f\\\"\\\\nüìä Symbol Prioritization:\\\")\\n        for i, symbol in enumerate(prioritized):\\n            score = symbol_scores.get(symbol, 0)\\n            print(f\\\"  {i+1}. {symbol}: {score:.2f} points\\\")\\n        \\n        return prioritized\\n    \\n    def run_transfer_learning_optimization(\\n        self,\\n        source_symbol: str = 'EURUSD',\\n        target_symbols: List[str] = None,\\n        n_trials_transfer: int = 20,\\n        n_trials_fine_tune: int = 30\\n    ) -> Dict[str, OptimizationResult]:\\n        \\\"\\\"\\\"Run optimization using parameter transfer from a source symbol\\\"\\\"\\\"\\n        \\n        if target_symbols is None:\\n            target_symbols = [s for s in SYMBOLS if s != source_symbol]\\n        \\n        print(f\\\"üîÑ Starting transfer learning optimization\\\")\\n        print(f\\\"Source symbol: {source_symbol}\\\")\\n        print(f\\\"Target symbols: {target_symbols}\\\")\\n        \\n        # Get best parameters from source symbol\\n        source_params = self.multi_optimizer.opt_manager.get_warm_start_params(source_symbol)\\n        \\n        if source_params is None:\\n            print(f\\\"‚ùå No parameters available for source symbol {source_symbol}\\\")\\n            print(f\\\"Running standard optimization for {source_symbol} first...\\\")\\n            \\n            # Optimize source symbol first\\n            source_result = self.multi_optimizer.optimize_single_symbol(\\n                symbol=source_symbol,\\n                n_trials=50,\\n                timeout=1800\\n            )\\n            \\n            if source_result:\\n                source_params = source_result.best_params\\n            else:\\n                print(f\\\"‚ùå Failed to optimize source symbol {source_symbol}\\\")\\n                return {}\\n        \\n        print(f\\\"‚úÖ Using parameters from {source_symbol} as baseline\\\")\\n        \\n        transfer_results = {}\\n        \\n        for target_symbol in target_symbols:\\n            print(f\\\"\\\\nüéØ Transfer learning: {source_symbol} ‚Üí {target_symbol}\\\")\\n            \\n            try:\\n                # Create study with warm start from source parameters\\n                study = self.multi_optimizer.study_manager.create_or_resume_study(\\n                    symbol=target_symbol,\\n                    resume_if_exists=False  # Create fresh study for transfer\\n                )\\n                \\n                # Add multiple variations of source parameters\\n                self._add_transfer_learning_trials(study, source_params, n_trials_transfer)\\n                \\n                # Run additional optimization\\n                def objective(trial):\\n                    return self.multi_optimizer._symbol_objective(trial, target_symbol)\\n                \\n                study.optimize(\\n                    objective,\\n                    n_trials=n_trials_fine_tune,\\n                    show_progress_bar=True\\n                )\\n                \\n                # Create result\\n                if study.best_trial:\\n                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\\n                    \\n                    result = OptimizationResult(\\n                        symbol=target_symbol,\\n                        timestamp=timestamp,\\n                        objective_value=study.best_value,\\n                        best_params=study.best_params,\\n                        mean_accuracy=study.best_trial.user_attrs.get('mean_accuracy', 0.0),\\n                        mean_sharpe=study.best_trial.user_attrs.get('mean_sharpe', 0.0),\\n                        std_accuracy=study.best_trial.user_attrs.get('std_accuracy', 0.0),\\n                        std_sharpe=study.best_trial.user_attrs.get('std_sharpe', 0.0),\\n                        num_features=study.best_trial.user_attrs.get('num_features', 0),\\n                        total_trials=len(study.trials),\\n                        completed_trials=len([t for t in study.trials if t.state == TrialState.COMPLETE]),\\n                        study_name=f\\\"transfer_{source_symbol}_to_{target_symbol}_{timestamp}\\\"\\n                    )\\n                    \\n                    transfer_results[target_symbol] = result\\n                    self.multi_optimizer._save_optimization_result(result)\\n                    \\n                    print(f\\\"  ‚úÖ Transfer completed: {study.best_value:.6f}\\\")\\n                    \\n                else:\\n                    print(f\\\"  ‚ùå Transfer failed for {target_symbol}\\\")\\n                    \\n            except Exception as e:\\n                print(f\\\"  ‚ùå Transfer error for {target_symbol}: {e}\\\")\\n                logger.error(f\\\"Transfer learning failed for {target_symbol}: {e}\\\")\\n        \\n        print(f\\\"\\\\nüéâ Transfer learning completed!\\\")\\n        print(f\\\"Successful transfers: {len(transfer_results)}/{len(target_symbols)}\\\")\\n        \\n        return transfer_results\\n    \\n    def _add_transfer_learning_trials(\\n        self, \\n        study: optuna.Study, \\n        source_params: Dict[str, Any], \\n        n_variations: int = 10\\n    ):\\n        \\\"\\\"\\\"Add parameter variations for transfer learning\\\"\\\"\\\"\\n        \\n        # Add exact source parameters\\n        try:\\n            study.enqueue_trial(source_params)\\n            print(f\\\"    üìå Enqueued exact source parameters\\\")\\n        except Exception as e:\\n            print(f\\\"    ‚ö†Ô∏è Failed to enqueue exact parameters: {e}\\\")\\n        \\n        # Add variations\\n        for i in range(n_variations):\\n            try:\\n                variation_factor = 0.05 + i * 0.03  # Increasing variation\\n                varied_params = self.multi_optimizer.study_manager.create_parameter_variation(\\n                    source_params, variation_factor\\n                )\\n                study.enqueue_trial(varied_params)\\n                print(f\\\"    üìå Enqueued variation {i+1} (factor: {variation_factor:.2f})\\\")\\n            except Exception as e:\\n                print(f\\\"    ‚ö†Ô∏è Failed to enqueue variation {i+1}: {e}\\\")\\n    \\n    def _save_execution_summary(self, total_time: float):\\n        \\\"\\\"\\\"Save execution summary to file\\\"\\\"\\\"\\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\\n        summary_file = Path(RESULTS_PATH) / f\\\"multi_symbol_execution_{timestamp}.json\\\"\\n        \\n        summary_data = {\\n            'timestamp': timestamp,\\n            'total_execution_time': total_time,\\n            'symbols_attempted': len(self.execution_log),\\n            'symbols_successful': len(self.execution_results),\\n            'execution_log': self.execution_log,\\n            'results_summary': {\\n                symbol: {\\n                    'objective_value': result.objective_value,\\n                    'mean_accuracy': result.mean_accuracy,\\n                    'mean_sharpe': result.mean_sharpe,\\n                    'completed_trials': result.completed_trials\\n                }\\n                for symbol, result in self.execution_results.items()\\n            }\\n        }\\n        \\n        with open(summary_file, 'w') as f:\\n            json.dump(summary_data, f, indent=2)\\n        \\n        print(f\\\"\\\\nüíæ Execution summary saved: {summary_file}\\\")\\n\\n# Initialize executor\\nmax_executor = MultiSymbolExecutor(multi_optimizer)\\n\\nprint(\\\"‚úÖ Multi-Symbol Executor Ready!\\\")\\nprint(\\\"Features:\\\")\\nprint(\\\"  - Intelligent symbol prioritization\\\")\\nprint(\\\"  - Transfer learning optimization\\\")\\nprint(\\\"  - Comprehensive execution tracking\\\")\\nprint(\\\"  - Automatic error handling and recovery\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Benchmarking Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkingDashboard:\n",
    "    \\\"\\\"\\\"Comprehensive benchmarking and analysis dashboard\\\"\\\"\\\"\\n    \\n    def __init__(self, opt_manager: AdvancedOptimizationManager):\\n        self.opt_manager = opt_manager\\n        self.dashboard_data: Dict[str, Any] = {}\\n        \\n    def generate_comprehensive_report(self, output_dir: str = \\\"reports\\\") -> str:\\n        \\\"\\\"\\\"Generate a comprehensive benchmarking report\\\"\\\"\\\"\\n        \\n        print(\\\"üìä Generating comprehensive benchmarking report...\\\")\\n        \\n        # Create output directory\\n        report_dir = Path(output_dir)\\n        report_dir.mkdir(exist_ok=True)\\n        \\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\\n        report_file = report_dir / f\\\"optimization_benchmark_report_{timestamp}.html\\\"\\n        \\n        # Collect all data\\n        self._collect_dashboard_data()\\n        \\n        # Generate HTML report\\n        html_content = self._generate_html_report()\\n        \\n        with open(report_file, 'w') as f:\\n            f.write(html_content)\\n        \\n        print(f\\\"‚úÖ Comprehensive report generated: {report_file}\\\")\\n        return str(report_file)\\n    \\n    def _collect_dashboard_data(self):\\n        \\\"\\\"\\\"Collect all data for the dashboard\\\"\\\"\\\"\\n        \\n        print(\\\"üìà Collecting optimization data...\\\")\\n        \\n        # Historical performance summary\\n        self.dashboard_data['performance_summary'] = self._get_performance_summary()\\n        \\n        # Parameter analysis\\n        self.dashboard_data['parameter_analysis'] = self._analyze_parameters()\\n        \\n        # Benchmark comparisons\\n        self.dashboard_data['benchmark_comparisons'] = self._get_benchmark_comparisons()\\n        \\n        # Trend analysis\\n        self.dashboard_data['trend_analysis'] = self._analyze_trends()\\n        \\n        # Success rate analysis\\n        self.dashboard_data['success_analysis'] = self._analyze_success_rates()\\n        \\n    def _get_performance_summary(self) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Get performance summary across all symbols\\\"\\\"\\\"\\n        \\n        summary = {\\n            'total_symbols': len(SYMBOLS),\\n            'optimized_symbols': len(self.opt_manager.optimization_history),\\n            'symbol_details': {}\\n        }\\n        \\n        for symbol in SYMBOLS:\\n            if symbol in self.opt_manager.optimization_history:\\n                results = self.opt_manager.optimization_history[symbol]\\n                \\n                if results:\\n                    best_result = max(results, key=lambda r: r.objective_value)\\n                    latest_result = max(results, key=lambda r: r.timestamp)\\n                    \\n                    summary['symbol_details'][symbol] = {\\n                        'total_runs': len(results),\\n                        'best_objective': best_result.objective_value,\\n                        'best_accuracy': best_result.mean_accuracy,\\n                        'best_sharpe': best_result.mean_sharpe,\\n                        'latest_objective': latest_result.objective_value,\\n                        'latest_timestamp': latest_result.timestamp,\\n                        'improvement_trend': self._calculate_improvement_trend(results)\\n                    }\\n                else:\\n                    summary['symbol_details'][symbol] = {\\n                        'total_runs': 0,\\n                        'status': 'no_data'\\n                    }\\n            else:\\n                summary['symbol_details'][symbol] = {\\n                    'total_runs': 0,\\n                    'status': 'not_optimized'\\n                }\\n        \\n        return summary\\n    \\n    def _analyze_parameters(self) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Analyze parameter patterns across successful optimizations\\\"\\\"\\\"\\n        \\n        parameter_analysis = {\\n            'successful_patterns': {},\\n            'parameter_distributions': {},\\n            'correlation_analysis': {}\\n        }\\n        \\n        # Collect all successful parameters\\n        all_params = []\\n        all_objectives = []\\n        \\n        for symbol, results in self.opt_manager.optimization_history.items():\\n            for result in results:\\n                if result.objective_value > 0:  # Successful optimization\\n                    all_params.append(result.best_params)\\n                    all_objectives.append(result.objective_value)\\n        \\n        if all_params:\\n            # Find common parameter patterns\\n            param_keys = set()\\n            for params in all_params:\\n                param_keys.update(params.keys())\\n            \\n            for param_key in param_keys:\\n                values = [params.get(param_key) for params in all_params if param_key in params]\\n                if values and all(isinstance(v, (int, float)) for v in values):\\n                    parameter_analysis['parameter_distributions'][param_key] = {\\n                        'mean': np.mean(values),\\n                        'std': np.std(values),\\n                        'min': np.min(values),\\n                        'max': np.max(values),\\n                        'median': np.median(values)\\n                    }\\n            \\n            # Correlation with objective values\\n            for param_key in param_keys:\\n                values = [params.get(param_key) for params in all_params if param_key in params]\\n                if len(values) == len(all_objectives) and all(isinstance(v, (int, float)) for v in values):\\n                    correlation = np.corrcoef(values, all_objectives[:len(values)])[0, 1]\\n                    if not np.isnan(correlation):\\n                        parameter_analysis['correlation_analysis'][param_key] = correlation\\n        \\n        return parameter_analysis\\n    \\n    def _get_benchmark_comparisons(self) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Compare current results with benchmarks\\\"\\\"\\\"\\n        \\n        comparisons = {\\n            'symbol_rankings': [],\\n            'improvement_metrics': {},\\n            'competitive_analysis': {}\\n        }\\n        \\n        # Rank symbols by best performance\\n        symbol_scores = []\\n        for symbol in SYMBOLS:\\n            if symbol in self.opt_manager.optimization_history:\\n                results = self.opt_manager.optimization_history[symbol]\\n                if results:\\n                    best_score = max(r.objective_value for r in results)\\n                    symbol_scores.append((symbol, best_score))\\n        \\n        # Sort by score (descending)\\n        symbol_scores.sort(key=lambda x: x[1], reverse=True)\\n        comparisons['symbol_rankings'] = symbol_scores\\n        \\n        # Calculate improvement metrics\\n        for symbol, results in self.opt_manager.optimization_history.items():\\n            if len(results) >= 2:\\n                sorted_results = sorted(results, key=lambda r: r.timestamp)\\n                first_score = sorted_results[0].objective_value\\n                latest_score = sorted_results[-1].objective_value\\n                improvement = latest_score - first_score\\n                improvement_pct = (improvement / abs(first_score)) * 100 if first_score != 0 else 0\\n                \\n                comparisons['improvement_metrics'][symbol] = {\\n                    'absolute_improvement': improvement,\\n                    'percentage_improvement': improvement_pct,\\n                    'total_runs': len(results)\\n                }\\n        \\n        return comparisons\\n    \\n    def _analyze_trends(self) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Analyze optimization trends over time\\\"\\\"\\\"\\n        \\n        trends = {\\n            'temporal_patterns': {},\\n            'convergence_analysis': {},\\n            'seasonal_effects': {}\\n        }\\n        \\n        for symbol, results in self.opt_manager.optimization_history.items():\\n            if len(results) >= 3:\\n                # Sort by timestamp\\n                sorted_results = sorted(results, key=lambda r: r.timestamp)\\n                \\n                # Calculate moving averages\\n                scores = [r.objective_value for r in sorted_results]\\n                timestamps = [r.timestamp for r in sorted_results]\\n                \\n                trends['temporal_patterns'][symbol] = {\\n                    'scores': scores,\\n                    'timestamps': timestamps,\\n                    'trend_direction': 'improving' if scores[-1] > scores[0] else 'declining',\\n                    'volatility': np.std(scores) if len(scores) > 1 else 0\\n                }\\n        \\n        return trends\\n    \\n    def _analyze_success_rates(self) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Analyze success rates and failure patterns\\\"\\\"\\\"\\n        \\n        success_analysis = {\\n            'overall_stats': {},\\n            'failure_patterns': {},\\n            'success_factors': {}\\n        }\\n        \\n        total_optimizations = sum(len(results) for results in self.opt_manager.optimization_history.values())\\n        successful_optimizations = sum(\\n            len([r for r in results if r.objective_value > 0]) \\n            for results in self.opt_manager.optimization_history.values()\\n        )\\n        \\n        success_analysis['overall_stats'] = {\\n            'total_optimizations': total_optimizations,\\n            'successful_optimizations': successful_optimizations,\\n            'success_rate': successful_optimizations / total_optimizations if total_optimizations > 0 else 0,\\n            'symbols_with_success': len([\\n                symbol for symbol, results in self.opt_manager.optimization_history.items()\\n                if any(r.objective_value > 0 for r in results)\\n            ])\\n        }\\n        \\n        return success_analysis\\n    \\n    def _calculate_improvement_trend(self, results: List[OptimizationResult]) -> str:\\n        \\\"\\\"\\\"Calculate improvement trend for a symbol\\\"\\\"\\\"\\n        \\n        if len(results) < 2:\\n            return 'insufficient_data'\\n        \\n        sorted_results = sorted(results, key=lambda r: r.timestamp)\\n        recent_scores = [r.objective_value for r in sorted_results[-3:]]  # Last 3 results\\n        \\n        if len(recent_scores) >= 2:\\n            if recent_scores[-1] > recent_scores[0]:\\n                return 'improving'\\n            elif recent_scores[-1] < recent_scores[0]:\\n                return 'declining'\\n            else:\\n                return 'stable'\\n        \\n        return 'unknown'\\n    \\n    def _generate_html_report(self) -> str:\\n        \\\"\\\"\\\"Generate HTML report content\\\"\\\"\\\"\\n        \\n        html_template = \\\"\\\"\\\"\\n<!DOCTYPE html>\\n<html>\\n<head>\\n    <title>Advanced Hyperparameter Optimization Benchmark Report</title>\\n    <style>\\n        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\\n        .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}\\n        .header {{ text-align: center; color: #2c3e50; margin-bottom: 30px; }}\\n        .section {{ margin-bottom: 30px; padding: 20px; border-left: 4px solid #3498db; background-color: #f8f9fa; }}\\n        .metric {{ display: inline-block; margin: 10px 20px; padding: 15px; background-color: white; border-radius: 5px; border: 1px solid #ddd; }}\\n        .metric-label {{ font-weight: bold; color: #2c3e50; }}\\n        .metric-value {{ font-size: 1.2em; color: #27ae60; }}\\n        .symbol-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; }}\\n        .symbol-card {{ padding: 15px; border: 1px solid #ddd; border-radius: 5px; background-color: white; }}\\n        .symbol-name {{ font-weight: bold; font-size: 1.1em; color: #2c3e50; margin-bottom: 10px; }}\\n        .status-success {{ color: #27ae60; }}\\n        .status-warning {{ color: #f39c12; }}\\n        .status-error {{ color: #e74c3c; }}\\n        .trend-improving {{ color: #27ae60; }}\\n        .trend-declining {{ color: #e74c3c; }}\\n        .trend-stable {{ color: #f39c12; }}\\n        table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}\\n        th, td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}\\n        th {{ background-color: #3498db; color: white; }}\\n        .parameter-analysis {{ background-color: #ecf0f1; padding: 15px; border-radius: 5px; }}\\n        .footer {{ text-align: center; margin-top: 30px; color: #7f8c8d; font-size: 0.9em; }}\\n    </style>\\n</head>\\n<body>\\n    <div class=\\\"container\\\">\\n        <div class=\\\"header\\\">\\n            <h1>üöÄ Advanced Hyperparameter Optimization Report</h1>\\n            <p>Generated on {timestamp}</p>\\n        </div>\\n        \\n        <div class=\\\"section\\\">\\n            <h2>üìä Overall Performance Summary</h2>\\n            <div class=\\\"metric\\\">\\n                <div class=\\\"metric-label\\\">Total Symbols</div>\\n                <div class=\\\"metric-value\\\">{total_symbols}</div>\\n            </div>\\n            <div class=\\\"metric\\\">\\n                <div class=\\\"metric-label\\\">Optimized Symbols</div>\\n                <div class=\\\"metric-value\\\">{optimized_symbols}</div>\\n            </div>\\n            <div class=\\\"metric\\\">\\n                <div class=\\\"metric-label\\\">Success Rate</div>\\n                <div class=\\\"metric-value\\\">{success_rate:.1f}%</div>\\n            </div>\\n            <div class=\\\"metric\\\">\\n                <div class=\\\"metric-label\\\">Total Optimizations</div>\\n                <div class=\\\"metric-value\\\">{total_optimizations}</div>\\n            </div>\\n        </div>\\n        \\n        <div class=\\\"section\\\">\\n            <h2>üéØ Symbol Performance Details</h2>\\n            <div class=\\\"symbol-grid\\\">\\n                {symbol_cards}\\n            </div>\\n        </div>\\n        \\n        <div class=\\\"section\\\">\\n            <h2>üìà Top Performing Symbols</h2>\\n            <table>\\n                <tr>\\n                    <th>Rank</th>\\n                    <th>Symbol</th>\\n                    <th>Best Objective</th>\\n                    <th>Best Accuracy</th>\\n                    <th>Best Sharpe</th>\\n                    <th>Status</th>\\n                </tr>\\n                {ranking_rows}\\n            </table>\\n        </div>\\n        \\n        <div class=\\\"section\\\">\\n            <h2>üîß Parameter Analysis</h2>\\n            <div class=\\\"parameter-analysis\\\">\\n                {parameter_insights}\\n            </div>\\n        </div>\\n        \\n        <div class=\\\"section\\\">\\n            <h2>üìä Improvement Metrics</h2>\\n            <table>\\n                <tr>\\n                    <th>Symbol</th>\\n                    <th>Absolute Improvement</th>\\n                    <th>Percentage Improvement</th>\\n                    <th>Total Runs</th>\\n                </tr>\\n                {improvement_rows}\\n            </table>\\n        </div>\\n        \\n        <div class=\\\"footer\\\">\\n            <p>Generated by Advanced Hyperparameter Optimization System</p>\\n            <p>ü§ñ Powered by Optuna and TensorFlow</p>\\n        </div>\\n    </div>\\n</body>\\n</html>\\n        \\\"\\\"\\\"\\n        \\n        # Prepare data for template\\n        perf_summary = self.dashboard_data['performance_summary']\\n        success_analysis = self.dashboard_data['success_analysis']\\n        benchmark_comparisons = self.dashboard_data['benchmark_comparisons']\\n        parameter_analysis = self.dashboard_data['parameter_analysis']\\n        \\n        # Generate symbol cards\\n        symbol_cards = \\\"\\\"\\n        for symbol, details in perf_summary['symbol_details'].items():\\n            if 'best_objective' in details:\\n                trend_class = f\\\"trend-{details.get('improvement_trend', 'unknown')}\\\"\\n                symbol_cards += f\\\"\\\"\\\"\\n                <div class=\\\"symbol-card\\\">\\n                    <div class=\\\"symbol-name\\\">{symbol}</div>\\n                    <div>Best Objective: <span class=\\\"status-success\\\">{details['best_objective']:.6f}</span></div>\\n                    <div>Accuracy: {details['best_accuracy']:.4f}</div>\\n                    <div>Sharpe: {details['best_sharpe']:.4f}</div>\\n                    <div>Runs: {details['total_runs']}</div>\\n                    <div>Trend: <span class=\\\"{trend_class}\\\">{details.get('improvement_trend', 'unknown')}</span></div>\\n                </div>\\n                \\\"\\\"\\\"\\n            else:\\n                status_class = \\\"status-error\\\" if details.get('status') == 'not_optimized' else \\\"status-warning\\\"\\n                symbol_cards += f\\\"\\\"\\\"\\n                <div class=\\\"symbol-card\\\">\\n                    <div class=\\\"symbol-name\\\">{symbol}</div>\\n                    <div class=\\\"{status_class}\\\">Status: {details.get('status', 'unknown')}</div>\\n                </div>\\n                \\\"\\\"\\\"\\n        \\n        # Generate ranking rows\\n        ranking_rows = \\\"\\\"\\n        for i, (symbol, score) in enumerate(benchmark_comparisons['symbol_rankings'][:10]):\\n            details = perf_summary['symbol_details'][symbol]\\n            ranking_rows += f\\\"\\\"\\\"\\n            <tr>\\n                <td>{i+1}</td>\\n                <td>{symbol}</td>\\n                <td>{score:.6f}</td>\\n                <td>{details.get('best_accuracy', 0):.4f}</td>\\n                <td>{details.get('best_sharpe', 0):.4f}</td>\\n                <td class=\\\"status-success\\\">Optimized</td>\\n            </tr>\\n            \\\"\\\"\\\"\\n        \\n        # Generate parameter insights\\n        param_insights = \\\"<h3>Parameter Distribution Analysis</h3>\\\"\\n        for param, stats in parameter_analysis.get('parameter_distributions', {}).items():\\n            param_insights += f\\\"\\\"\\\"\\n            <p><strong>{param}:</strong> Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}, Range: [{stats['min']:.4f}, {stats['max']:.4f}]</p>\\n            \\\"\\\"\\\"\\n        \\n        param_insights += \\\"<h3>Parameter-Objective Correlations</h3>\\\"\\n        for param, correlation in parameter_analysis.get('correlation_analysis', {}).items():\\n            correlation_strength = \\\"Strong\\\" if abs(correlation) > 0.5 else \\\"Moderate\\\" if abs(correlation) > 0.3 else \\\"Weak\\\"\\n            param_insights += f\\\"\\\"\\\"\\n            <p><strong>{param}:</strong> {correlation:.4f} ({correlation_strength})</p>\\n            \\\"\\\"\\\"\\n        \\n        # Generate improvement rows\\n        improvement_rows = \\\"\\\"\\n        for symbol, metrics in benchmark_comparisons.get('improvement_metrics', {}).items():\\n            improvement_rows += f\\\"\\\"\\\"\\n            <tr>\\n                <td>{symbol}</td>\\n                <td>{metrics['absolute_improvement']:.6f}</td>\\n                <td>{metrics['percentage_improvement']:.2f}%</td>\\n                <td>{metrics['total_runs']}</td>\\n            </tr>\\n            \\\"\\\"\\\"\\n        \\n        # Fill template\\n        return html_template.format(\\n            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\\n            total_symbols=perf_summary['total_symbols'],\\n            optimized_symbols=perf_summary['optimized_symbols'],\\n            success_rate=success_analysis['overall_stats'].get('success_rate', 0) * 100,\\n            total_optimizations=success_analysis['overall_stats'].get('total_optimizations', 0),\\n            symbol_cards=symbol_cards,\\n            ranking_rows=ranking_rows,\\n            parameter_insights=param_insights,\\n            improvement_rows=improvement_rows\\n        )\\n    \\n    def create_interactive_plots(self, output_dir: str = \\\"plots\\\"):\\n        \\\"\\\"\\\"Create interactive plots for optimization analysis\\\"\\\"\\\"\\n        \\n        print(\\\"üìà Creating interactive optimization plots...\\\")\\n        \\n        plot_dir = Path(output_dir)\\n        plot_dir.mkdir(exist_ok=True)\\n        \\n        # Performance comparison plot\\n        self._create_performance_comparison_plot(plot_dir)\\n        \\n        # Parameter correlation heatmap\\n        self._create_parameter_correlation_plot(plot_dir)\\n        \\n        # Optimization timeline\\n        self._create_optimization_timeline_plot(plot_dir)\\n        \\n        print(f\\\"‚úÖ Interactive plots saved to: {plot_dir}\\\")\\n    \\n    def _create_performance_comparison_plot(self, output_dir: Path):\\n        \\\"\\\"\\\"Create performance comparison plot\\\"\\\"\\\"\\n        \\n        plt.figure(figsize=(12, 8))\\n        \\n        symbols = []\\n        best_scores = []\\n        colors = []\\n        \\n        for symbol in SYMBOLS:\\n            if symbol in self.opt_manager.optimization_history:\\n                results = self.opt_manager.optimization_history[symbol]\\n                if results:\\n                    best_score = max(r.objective_value for r in results)\\n                    symbols.append(symbol)\\n                    best_scores.append(best_score)\\n                    colors.append('#27ae60' if best_score > 0.5 else '#f39c12' if best_score > 0 else '#e74c3c')\\n        \\n        plt.bar(symbols, best_scores, color=colors)\\n        plt.title('Best Optimization Scores by Symbol', fontsize=16, fontweight='bold')\\n        plt.xlabel('Currency Pairs', fontsize=12)\\n        plt.ylabel('Best Objective Value', fontsize=12)\\n        plt.xticks(rotation=45)\\n        plt.grid(True, alpha=0.3)\\n        \\n        # Add value labels on bars\\n        for i, v in enumerate(best_scores):\\n            plt.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\\n        \\n        plt.tight_layout()\\n        plt.savefig(output_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')\\n        plt.close()\\n    \\n    def _create_parameter_correlation_plot(self, output_dir: Path):\\n        \\\"\\\"\\\"Create parameter correlation heatmap\\\"\\\"\\\"\\n        \\n        # Collect parameter data\\n        param_data = []\\n        for results in self.opt_manager.optimization_history.values():\\n            for result in results:\\n                if result.objective_value > 0:\\n                    param_data.append(result.best_params)\\n        \\n        if param_data:\\n            # Convert to DataFrame\\n            df = pd.DataFrame(param_data)\\n            \\n            # Select numerical columns\\n            numerical_cols = df.select_dtypes(include=[np.number]).columns\\n            if len(numerical_cols) > 1:\\n                correlation_matrix = df[numerical_cols].corr()\\n                \\n                plt.figure(figsize=(12, 10))\\n                sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\\n                           square=True, linewidths=0.5, cbar_kws={\\\"shrink\\\": .5})\\n                plt.title('Parameter Correlation Matrix', fontsize=16, fontweight='bold')\\n                plt.tight_layout()\\n                plt.savefig(output_dir / 'parameter_correlation.png', dpi=300, bbox_inches='tight')\\n                plt.close()\\n    \\n    def _create_optimization_timeline_plot(self, output_dir: Path):\\n        \\\"\\\"\\\"Create optimization timeline plot\\\"\\\"\\\"\\n        \\n        plt.figure(figsize=(14, 8))\\n        \\n        for symbol, results in self.opt_manager.optimization_history.items():\\n            if results:\\n                timestamps = [datetime.strptime(r.timestamp, '%Y%m%d_%H%M%S') for r in results]\\n                objectives = [r.objective_value for r in results]\\n                \\n                plt.plot(timestamps, objectives, marker='o', label=symbol, linewidth=2, markersize=6)\\n        \\n        plt.title('Optimization Progress Timeline', fontsize=16, fontweight='bold')\\n        plt.xlabel('Time', fontsize=12)\\n        plt.ylabel('Objective Value', fontsize=12)\\n        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\n        plt.grid(True, alpha=0.3)\\n        plt.xticks(rotation=45)\\n        \\n        plt.tight_layout()\\n        plt.savefig(output_dir / 'optimization_timeline.png', dpi=300, bbox_inches='tight')\\n        plt.close()\\n\\n# Initialize benchmarking dashboard\\nbenchmark_dashboard = BenchmarkingDashboard(opt_manager)\\n\\nprint(\\\"‚úÖ Benchmarking Dashboard Ready!\\\")\\nprint(\\\"Features:\\\")\\nprint(\\\"  - Comprehensive HTML reports\\\")\\nprint(\\\"  - Interactive performance plots\\\")\\nprint(\\\"  - Parameter correlation analysis\\\")\\nprint(\\\"  - Trend and improvement tracking\\\")\\nprint(\\\"  - Success rate analysis\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Execution Examples and Usage Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\\n# USAGE EXAMPLES - Choose your optimization approach\\n# ========================================\\n\\nprint(\\\"üöÄ Advanced Hyperparameter Optimization System Ready!\\\")\\nprint(\\\"\\\\nChoose your optimization approach:\\\")\\nprint(\\\"\\\\n1Ô∏è‚É£  QUICK TEST (Single Symbol - 10 trials)\\\")\\nprint(\\\"2Ô∏è‚É£  STANDARD OPTIMIZATION (All symbols - 30 trials each)\\\")\\nprint(\\\"3Ô∏è‚É£  INTENSIVE OPTIMIZATION (All symbols - 50+ trials each)\\\")\\nprint(\\\"4Ô∏è‚É£  TRANSFER LEARNING (EURUSD ‚Üí other symbols)\\\")\\nprint(\\\"5Ô∏è‚É£  BENCHMARK REPORT ONLY (Generate analysis of existing results)\\\")\\n\\n# Uncomment the section you want to run:\\n\\n# ========================================\\n# 1Ô∏è‚É£ QUICK TEST - Single Symbol\\n# ========================================\\n\\\"\\\"\\\"\\nprint(\\\"\\\\nüéØ Running QUICK TEST on EURUSD...\\\")\\n\\n# Single symbol optimization\\nresult = multi_optimizer.optimize_single_symbol(\\n    symbol='EURUSD',\\n    n_trials=10,\\n    timeout=600,  # 10 minutes\\n    resume_study=True\\n)\\n\\nif result:\\n    print(f\\\"\\\\n‚úÖ Quick test completed!\\\")\\n    print(f\\\"Best objective: {result.objective_value:.6f}\\\")\\n    print(f\\\"Best accuracy: {result.mean_accuracy:.4f}\\\")\\n    print(f\\\"Best Sharpe: {result.mean_sharpe:.4f}\\\")\\nelse:\\n    print(\\\"‚ùå Quick test failed\\\")\\n\\\"\\\"\\\"\\n\\n# ========================================\\n# 2Ô∏è‚É£ STANDARD OPTIMIZATION - All Symbols\\n# ========================================\\n\\\"\\\"\\\"\\nprint(\\\"\\\\nüéØ Running STANDARD OPTIMIZATION on all symbols...\\\")\\n\\n# Multi-symbol optimization with moderate settings\\nresults = max_executor.run_parallel_optimization(\\n    symbols=SYMBOLS,\\n    n_trials_per_symbol=30,\\n    timeout_per_symbol=1200,  # 20 minutes per symbol\\n    priority_symbols=['EURUSD', 'GBPUSD', 'USDJPY']  # Major pairs first\\n)\\n\\nprint(f\\\"\\\\n‚úÖ Standard optimization completed!\\\")\\nprint(f\\\"Successful symbols: {len(results)}/{len(SYMBOLS)}\\\")\\n\\nfor symbol, result in results.items():\\n    print(f\\\"  {symbol}: {result.objective_value:.6f} (accuracy: {result.mean_accuracy:.4f})\\\")\\n\\\"\\\"\\\"\\n\\n# ========================================\\n# 3Ô∏è‚É£ INTENSIVE OPTIMIZATION - Maximum Performance\\n# ========================================\\n\\\"\\\"\\\"\\nprint(\\\"\\\\nüéØ Running INTENSIVE OPTIMIZATION...\\\")\\nprint(\\\"‚ö†Ô∏è  This will take several hours but provides the best results\\\")\\n\\n# Intensive optimization with maximum trials\\nresults = max_executor.run_parallel_optimization(\\n    symbols=SYMBOLS,\\n    n_trials_per_symbol=75,  # More trials for better optimization\\n    timeout_per_symbol=2400,  # 40 minutes per symbol\\n    priority_symbols=['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD']  # Focus on major pairs\\n)\\n\\nprint(f\\\"\\\\n‚úÖ Intensive optimization completed!\\\")\\nprint(f\\\"Total execution time: {sum(log['execution_time'] for log in max_executor.execution_log):.1f}s\\\")\\n\\\"\\\"\\\"\\n\\n# ========================================\\n# 4Ô∏è‚É£ TRANSFER LEARNING - Parameter Transfer\\n# ========================================\\n\\\"\\\"\\\"\\nprint(\\\"\\\\nüéØ Running TRANSFER LEARNING optimization...\\\")\\n\\n# First ensure EURUSD is optimized\\nif 'EURUSD' not in opt_manager.best_parameters:\\n    print(\\\"üìä Optimizing EURUSD first as source for transfer learning...\\\")\\n    eurusd_result = multi_optimizer.optimize_single_symbol(\\n        symbol='EURUSD',\\n        n_trials=50,\\n        timeout=1800\\n    )\\n    print(f\\\"EURUSD baseline: {eurusd_result.objective_value:.6f}\\\")\\n\\n# Transfer learning to other symbols\\ntransfer_results = max_executor.run_transfer_learning_optimization(\\n    source_symbol='EURUSD',\\n    target_symbols=['GBPUSD', 'USDJPY', 'AUDUSD', 'USDCAD'],\\n    n_trials_transfer=15,  # Initial transfer trials\\n    n_trials_fine_tune=35  # Fine-tuning trials\\n)\\n\\nprint(f\\\"\\\\n‚úÖ Transfer learning completed!\\\")\\nfor symbol, result in transfer_results.items():\\n    print(f\\\"  {symbol}: {result.objective_value:.6f} (transferred from EURUSD)\\\")\\n\\\"\\\"\\\"\\n\\n# ========================================\\n# 5Ô∏è‚É£ BENCHMARK REPORT - Analysis Only\\n# ========================================\\n\\nprint(\\\"\\\\nüìä Generating comprehensive benchmark report...\\\")\\n\\n# Generate HTML report\\nreport_file = benchmark_dashboard.generate_comprehensive_report(output_dir=\\\"reports\\\")\\nprint(f\\\"‚úÖ Report generated: {report_file}\\\")\\n\\n# Create interactive plots\\nbenchmark_dashboard.create_interactive_plots(output_dir=\\\"plots\\\")\\nprint(f\\\"‚úÖ Interactive plots created in: plots/\\\")\\n\\n# Display summary statistics\\nprint(\\\"\\\\nüìà Quick Summary:\\\")\\nif opt_manager.optimization_history:\\n    total_runs = sum(len(results) for results in opt_manager.optimization_history.values())\\n    successful_runs = sum(\\n        len([r for r in results if r.objective_value > 0]) \\n        for results in opt_manager.optimization_history.values()\\n    )\\n    \\n    print(f\\\"  Total optimization runs: {total_runs}\\\")\\n    print(f\\\"  Successful runs: {successful_runs}\\\")\\n    print(f\\\"  Success rate: {successful_runs/total_runs*100:.1f}%\\\" if total_runs > 0 else \\\"  Success rate: N/A\\\")\\n    \\n    # Best performing symbols\\n    best_performers = []\\n    for symbol, results in opt_manager.optimization_history.items():\\n        if results:\\n            best_score = max(r.objective_value for r in results)\\n            best_performers.append((symbol, best_score))\\n    \\n    best_performers.sort(key=lambda x: x[1], reverse=True)\\n    \\n    print(f\\\"\\\\nüèÜ Top 3 Performing Symbols:\\\")\\n    for i, (symbol, score) in enumerate(best_performers[:3]):\\n        print(f\\\"  {i+1}. {symbol}: {score:.6f}\\\")\\nelse:\\n    print(\\\"  No optimization history found.\\\")\\n    print(\\\"  Run one of the optimization examples above first.\\\")\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*60)\\nprint(\\\"üéâ ADVANCED HYPERPARAMETER OPTIMIZATION SYSTEM\\\")\\nprint(\\\"=\\\"*60)\\nprint(\\\"\\\\n‚ú® Key Features Implemented:\\\")\\nprint(\\\"  ‚úÖ Study resumption and warm start capabilities\\\")\\nprint(\\\"  ‚úÖ Multi-symbol optimization with intelligent prioritization\\\")\\nprint(\\\"  ‚úÖ Parameter transfer learning across currency pairs\\\")\\nprint(\\\"  ‚úÖ Comprehensive benchmarking and analysis dashboard\\\")\\nprint(\\\"  ‚úÖ Enhanced model architectures and hyperparameter spaces\\\")\\nprint(\\\"  ‚úÖ Advanced feature selection and data preparation\\\")\\nprint(\\\"  ‚úÖ Real-time progress tracking and error handling\\\")\\nprint(\\\"  ‚úÖ Interactive visualization and reporting\\\")\\n\\nprint(\\\"\\\\nüöÄ Next Steps:\\\")\\nprint(\\\"  1. Uncomment and run your preferred optimization approach above\\\")\\nprint(\\\"  2. Monitor results in the optimization_results/ directory\\\")\\nprint(\\\"  3. Review generated reports and plots for insights\\\")\\nprint(\\\"  4. Use best parameters for production trading models\\\")\\nprint(\\\"  5. Set up automated reoptimization schedules\\\")\\n\\nprint(\\\"\\\\nüí° Pro Tips:\\\")\\nprint(\\\"  - Start with QUICK TEST to verify system works\\\")\\nprint(\\\"  - Use TRANSFER LEARNING for faster optimization of similar pairs\\\")\\nprint(\\\"  - Run INTENSIVE OPTIMIZATION overnight for best results\\\")\\nprint(\\\"  - Generate BENCHMARK REPORTS regularly to track progress\\\")\\nprint(\\\"  - Check the 'reports/' and 'plots/' directories for detailed analysis\\\")\\n\\nprint(f\\\"\\\\nüìÅ Output Directories:\\\")\\nprint(f\\\"  - Optimization results: {RESULTS_PATH}/\\\")\\nprint(f\\\"  - Model exports: {MODELS_PATH}/\\\")\\nprint(f\\\"  - Reports: reports/\\\")\\nprint(f\\\"  - Plots: plots/\\\")\\nprint(f\\\"  - Logs: advanced_optimization.log\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDataLoader:\n",
    "    \"\"\"Enhanced data loader with multi-symbol support and caching\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.cached_data: Dict[str, pd.DataFrame] = {}\n",
    "        self.cached_indicators: Dict[str, pd.DataFrame] = {}\n",
    "        self.cached_rcs: Optional[pd.DataFrame] = None\n",
    "        \n",
    "    def load_all_data(self, symbols: List[str] = None) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Load all available forex data\"\"\"\n",
    "        if symbols is None:\n",
    "            symbols = SYMBOLS\n",
    "        \n",
    "        print(f\"üìä Loading data for {len(symbols)} symbols...\")\n",
    "        \n",
    "        data = {}\n",
    "        for symbol in symbols:\n",
    "            symbol_data = self.load_symbol_data(symbol)\n",
    "            if symbol_data is not None:\n",
    "                data[symbol] = symbol_data\n",
    "                print(f\"  ‚úÖ {symbol}: {len(symbol_data)} rows\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå {symbol}: Failed to load\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def load_symbol_data(self, symbol: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data for a specific symbol with caching\"\"\"\n",
    "        if symbol in self.cached_data:\n",
    "            return self.cached_data[symbol]\n",
    "        \n",
    "        # Try different file formats\n",
    "        file_patterns = [\n",
    "            f\"metatrader_{symbol}.parquet\",\n",
    "            f\"metatrader_{symbol}.h5\",\n",
    "            f\"{symbol}.parquet\",\n",
    "            f\"{symbol}.h5\",\n",
    "            f\"{symbol}.csv\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in file_patterns:\n",
    "            file_path = self.data_path / pattern\n",
    "            if file_path.exists():\n",
    "                try:\n",
    "                    if pattern.endswith('.parquet'):\n",
    "                        df = pd.read_parquet(file_path)\n",
    "                    elif pattern.endswith('.h5'):\n",
    "                        df = pd.read_hdf(file_path, key='data')\n",
    "                    elif pattern.endswith('.csv'):\n",
    "                        df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Standardize index\n",
    "                    if 'time' in df.columns:\n",
    "                        df = df.set_index('time')\n",
    "                    elif 'timestamp' in df.columns:\n",
    "                        df = df.set_index('timestamp')\n",
    "                    \n",
    "                    # Ensure datetime index\n",
    "                    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "                        df.index = pd.to_datetime(df.index)\n",
    "                    \n",
    "                    # Cache and return\n",
    "                    self.cached_data[symbol] = df\n",
    "                    return df\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {file_path}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        logger.error(f\"No data file found for {symbol}\")\n",
    "        return None\n",
    "    \n",
    "    def create_multi_index_prices(self, data: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "        \"\"\"Create MultiIndex DataFrame for price data\"\"\"\n",
    "        print(\"üîÑ Creating MultiIndex price structure...\")\n",
    "        \n",
    "        multi_data = {}\n",
    "        \n",
    "        for symbol, df in data.items():\n",
    "            # Ensure we have OHLC columns\n",
    "            required_cols = ['open', 'high', 'low', 'close']\n",
    "            \n",
    "            for col in required_cols:\n",
    "                if col in df.columns:\n",
    "                    multi_data[(symbol, col)] = df[col]\n",
    "                elif col == 'open' and 'close' in df.columns:\n",
    "                    # Use close as fallback for open\n",
    "                    multi_data[(symbol, col)] = df['close']\n",
    "                elif col in ['high', 'low'] and 'close' in df.columns:\n",
    "                    # Use close as fallback\n",
    "                    multi_data[(symbol, col)] = df['close']\n",
    "                else:\n",
    "                    logger.warning(f\"Missing {col} for {symbol}\")\n",
    "            \n",
    "            # Add volume if available\n",
    "            volume_cols = ['tick_volume', 'volume', 'real_volume']\n",
    "            for vol_col in volume_cols:\n",
    "                if vol_col in df.columns:\n",
    "                    multi_data[(symbol, 'tick_volume')] = df[vol_col]\n",
    "                    break\n",
    "            else:\n",
    "                # Create synthetic volume\n",
    "                multi_data[(symbol, 'tick_volume')] = pd.Series(\n",
    "                    np.random.randint(100, 2000, len(df)),\n",
    "                    index=df.index\n",
    "                )\n",
    "        \n",
    "        prices_df = pd.DataFrame(multi_data)\n",
    "        print(f\"‚úÖ MultiIndex prices created: {prices_df.shape}\")\n",
    "        \n",
    "        return prices_df\n",
    "    \n",
    "    def calculate_rcs(self, prices_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate Relative Currency Strength\"\"\"\n",
    "        if self.cached_rcs is not None:\n",
    "            return self.cached_rcs\n",
    "        \n",
    "        print(\"üßÆ Calculating Relative Currency Strength...\")\n",
    "        \n",
    "        # Extract close prices\n",
    "        close_prices = {}\n",
    "        for symbol in SYMBOLS:\n",
    "            if (symbol, 'close') in prices_df.columns:\n",
    "                close_prices[symbol] = prices_df[(symbol, 'close')]\n",
    "        \n",
    "        if not close_prices:\n",
    "            logger.warning(\"No close prices found for RCS calculation\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        close_df = pd.DataFrame(close_prices)\n",
    "        log_returns = np.log(close_df / close_df.shift(1)).dropna()\n",
    "        \n",
    "        # Extract unique currencies\n",
    "        currencies = list(set([s[:3] for s in log_returns.columns] + [s[3:6] for s in log_returns.columns]))\n",
    "        \n",
    "        # Calculate RCS\n",
    "        rcs_data = {c: [] for c in currencies}\n",
    "        \n",
    "        for i in range(len(log_returns)):\n",
    "            row = log_returns.iloc[i]\n",
    "            daily_strength = {c: 0 for c in currencies}\n",
    "            counts = {c: 0 for c in currencies}\n",
    "            \n",
    "            for pair, ret in row.items():\n",
    "                if pd.notna(ret):\n",
    "                    base, quote = pair[:3], pair[3:]\n",
    "                    daily_strength[base] += ret\n",
    "                    daily_strength[quote] -= ret\n",
    "                    counts[base] += 1\n",
    "                    counts[quote] += 1\n",
    "            \n",
    "            for c in currencies:\n",
    "                avg_strength = daily_strength[c] / counts[c] if counts[c] > 0 else 0\n",
    "                rcs_data[c].append(avg_strength)\n",
    "        \n",
    "        rcs_df = pd.DataFrame(rcs_data, index=log_returns.index)\n",
    "        self.cached_rcs = rcs_df\n",
    "        \n",
    "        print(f\"‚úÖ RCS calculated for {len(currencies)} currencies\")\n",
    "        return rcs_df\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = AdvancedDataLoader(DATA_PATH)\n",
    "\n",
    "# Load all available data\n",
    "all_data = data_loader.load_all_data(SYMBOLS)\n",
    "multi_prices = data_loader.create_multi_index_prices(all_data)\n",
    "rcs_data = data_loader.calculate_rcs(multi_prices)\n",
    "\n",
    "print(f\"\\n‚úÖ Data loading completed:\")\n",
    "print(f\"  - Loaded data for {len(all_data)} symbols\")\n",
    "print(f\"  - MultiIndex prices: {multi_prices.shape}\")\n",
    "print(f\"  - RCS data: {rcs_data.shape}\")\n",
    "if not multi_prices.empty:\n",
    "    print(f\"  - Date range: {multi_prices.index.min()} to {multi_prices.index.max()}\")\n",
    "else:\n",
    "    print(\"  - No price data loaded - check data directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Technical Indicators with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the existing indicator calculation function from the original notebook\n",
    "# We'll enhance it with caching and parallel processing\n",
    "\n",
    "def calculate_enhanced_indicators(prices_df: pd.DataFrame, symbols: List[str], use_cache: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Calculate enhanced technical indicators with caching\"\"\"\n",
    "    \n",
    "    # Check cache first\n",
    "    if use_cache and hasattr(data_loader, 'cached_indicators') and data_loader.cached_indicators:\n",
    "        print(\"üìä Using cached indicators...\")\n",
    "        return data_loader.cached_indicators\n",
    "    \n",
    "    print(\"‚öôÔ∏è Calculating enhanced technical indicators...\")\n",
    "    all_indicators = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        if symbol not in all_data:\n",
    "            print(f\"‚ö†Ô∏è No data available for {symbol}, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"üîß Processing {symbol}...\")\n",
    "        \n",
    "        try:\n",
    "            # Extract OHLC data\n",
    "            close = prices_df[(symbol, 'close')]\n",
    "            high = prices_df[(symbol, 'high')]\n",
    "            low = prices_df[(symbol, 'low')]\n",
    "            open_price = prices_df[(symbol, 'open')]\n",
    "            volume = prices_df.get((symbol, 'tick_volume'), pd.Series(index=prices_df.index, data=1000))\n",
    "        except KeyError:\n",
    "            print(f\"‚ö†Ô∏è Missing OHLC data for {symbol}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Initialize indicators DataFrame\n",
    "        indicators = pd.DataFrame(index=close.index)\n",
    "        \n",
    "        # === MOMENTUM INDICATORS ===\n",
    "        try:\n",
    "            # RSI variations\n",
    "            indicators['rsi'] = RSIIndicator(close=close, window=14).rsi()\n",
    "            indicators['rsi_fast'] = RSIIndicator(close=close, window=7).rsi()\n",
    "            indicators['rsi_slow'] = RSIIndicator(close=close, window=21).rsi()\n",
    "            \n",
    "            # ROC variations\n",
    "            indicators['roc'] = ROCIndicator(close=close, window=10).roc()\n",
    "            indicators['roc_fast'] = ROCIndicator(close=close, window=5).roc()\n",
    "            \n",
    "            # Stochastic\n",
    "            stoch = StochasticOscillator(high=high, low=low, close=close)\n",
    "            indicators['stoch_k'] = stoch.stoch()\n",
    "            indicators['stoch_d'] = stoch.stoch_signal()\n",
    "            \n",
    "            # Additional momentum indicators\n",
    "            indicators['momentum'] = close.pct_change(10)  # 10-period momentum\n",
    "            indicators['williams_r'] = ((high.rolling(14).max() - close) / \n",
    "                                      (high.rolling(14).max() - low.rolling(14).min())) * -100\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Some momentum indicators failed for {symbol}: {e}\")\n",
    "        \n",
    "        # === TREND INDICATORS ===\n",
    "        try:\n",
    "            # MACD\n",
    "            macd = MACD(close=close)\n",
    "            indicators['macd'] = macd.macd()\n",
    "            indicators['macd_signal'] = macd.macd_signal()\n",
    "            indicators['macd_histogram'] = macd.macd_diff()\n",
    "            \n",
    "            # ADX\n",
    "            adx_indicator = ADXIndicator(high=high, low=low, close=close)\n",
    "            indicators['adx'] = adx_indicator.adx()\n",
    "            indicators['adx_pos'] = adx_indicator.adx_pos()\n",
    "            indicators['adx_neg'] = adx_indicator.adx_neg()\n",
    "            \n",
    "            # CCI\n",
    "            indicators['cci'] = CCIIndicator(high=high, low=low, close=close).cci()\n",
    "            \n",
    "            # Trend direction\n",
    "            indicators['trend_direction'] = (close > close.rolling(20).mean()).astype(int)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Some trend indicators failed for {symbol}: {e}\")\n",
    "        \n",
    "        # === VOLATILITY INDICATORS ===\n",
    "        try:\n",
    "            # ATR\n",
    "            atr_indicator = AverageTrueRange(high=high, low=low, close=close)\n",
    "            indicators['atr'] = atr_indicator.average_true_range()\n",
    "            indicators['atr_norm'] = indicators['atr'] / close  # Normalized ATR\n",
    "            \n",
    "            # Bollinger Bands\n",
    "            bb = BollingerBands(close=close, window=20, window_dev=2)\n",
    "            indicators['bb_upper'] = bb.bollinger_hband()\n",
    "            indicators['bb_lower'] = bb.bollinger_lband()\n",
    "            indicators['bb_middle'] = bb.bollinger_mavg()\n",
    "            indicators['bb_width'] = bb.bollinger_wband()\n",
    "            indicators['bb_position'] = (close - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())\n",
    "            \n",
    "            # Volatility measures\n",
    "            indicators['volatility_5'] = close.rolling(5).std()\n",
    "            indicators['volatility_20'] = close.rolling(20).std()\n",
    "            indicators['volatility_50'] = close.rolling(50).std()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Some volatility indicators failed for {symbol}: {e}\")\n",
    "        \n",
    "        # === PRICE-BASED FEATURES ===\n",
    "        try:\n",
    "            # Returns\n",
    "            indicators['return_1'] = close.pct_change(1)\n",
    "            indicators['return_5'] = close.pct_change(5)\n",
    "            indicators['return_10'] = close.pct_change(10)\n",
    "            indicators['return_20'] = close.pct_change(20)\n",
    "            \n",
    "            # Moving averages\n",
    "            indicators['sma_5'] = close.rolling(5).mean()\n",
    "            indicators['sma_10'] = close.rolling(10).mean()\n",
    "            indicators['sma_20'] = close.rolling(20).mean()\n",
    "            indicators['sma_50'] = close.rolling(50).mean()\n",
    "            \n",
    "            indicators['ema_5'] = close.ewm(span=5).mean()\n",
    "            indicators['ema_10'] = close.ewm(span=10).mean()\n",
    "            indicators['ema_20'] = close.ewm(span=20).mean()\n",
    "            \n",
    "            # Price position\n",
    "            indicators['price_position_20'] = (close - close.rolling(20).min()) / (close.rolling(20).max() - close.rolling(20).min())\n",
    "            indicators['price_position_50'] = (close - close.rolling(50).min()) / (close.rolling(50).max() - close.rolling(50).min())\n",
    "            \n",
    "            # Price momentum\n",
    "            indicators['momentum_slope'] = close.diff(1)\n",
    "            indicators['price_acceleration'] = indicators['momentum_slope'].diff(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Some price features failed for {symbol}: {e}\")\n",
    "        \n",
    "        # === TIME-BASED FEATURES ===\n",
    "        try:\n",
    "            indicators['hour'] = indicators.index.hour\n",
    "            indicators['day_of_week'] = indicators.index.dayofweek\n",
    "            indicators['month'] = indicators.index.month\n",
    "            indicators['quarter'] = indicators.index.quarter\n",
    "            indicators['is_weekend'] = (indicators.index.dayofweek >= 5).astype(int)\n",
    "            \n",
    "            # Market session\n",
    "            def get_market_session(hour):\n",
    "                if 0 <= hour < 8:\n",
    "                    return 1  # Asian\n",
    "                elif 8 <= hour < 16:\n",
    "                    return 2  # European\n",
    "                else:\n",
    "                    return 3  # US\n",
    "            \n",
    "            indicators['market_session'] = indicators['hour'].apply(get_market_session)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Time features failed for {symbol}: {e}\")\n",
    "        \n",
    "        # === DATA CLEANING ===\n",
    "        # Forward fill, backward fill, then fill with appropriate values\n",
    "        for col in indicators.columns:\n",
    "            if indicators[col].dtype in ['float64', 'int64']:\n",
    "                indicators[col] = indicators[col].ffill().bfill()\n",
    "                \n",
    "                # Fill remaining NaN with neutral values\n",
    "                if 'return' in col or 'momentum' in col or 'acceleration' in col:\n",
    "                    indicators[col] = indicators[col].fillna(0)\n",
    "                elif any(x in col for x in ['rsi', 'williams_r']):\n",
    "                    indicators[col] = indicators[col].fillna(50)\n",
    "                elif 'position' in col:\n",
    "                    indicators[col] = indicators[col].fillna(0.5)\n",
    "                else:\n",
    "                    indicators[col] = indicators[col].fillna(0)\n",
    "        \n",
    "        # Replace infinite values\n",
    "        indicators = indicators.replace([np.inf, -np.inf], np.nan)\n",
    "        indicators = indicators.ffill().bfill().fillna(0)\n",
    "        \n",
    "        all_indicators[symbol] = indicators\n",
    "        print(f\"  ‚úÖ {symbol}: {indicators.shape[0]} rows, {indicators.shape[1]} features\")\n",
    "    \n",
    "    # Cache the results\n",
    "    if use_cache:\n",
    "        data_loader.cached_indicators = all_indicators\n",
    "    \n",
    "    print(f\"\\nüéâ Enhanced indicators calculated for {len(all_indicators)} symbols\")\n",
    "    return all_indicators\n",
    "\n",
    "# Calculate indicators for all symbols\n",
    "all_indicators = calculate_enhanced_indicators(multi_prices, SYMBOLS)\n",
    "\n",
    "print(f\"\\nüìä Technical Indicators Summary:\")\n",
    "for symbol in SYMBOLS:\n",
    "    if symbol in all_indicators:\n",
    "        indicator_df = all_indicators[symbol]\n",
    "        print(f\"  {symbol}: {indicator_df.shape[0]} rows √ó {indicator_df.shape[1]} features\")\n",
    "    else:\n",
    "        print(f\"  {symbol}: ‚ùå No indicators calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Symbol Optimization Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Symbol Optimization Framework\n",
    "\n",
    "The complete MultiSymbolOptimizer implementation is in cell 5 above. This framework includes:\n",
    "\n",
    "- Enhanced hyperparameter space with symbol-specific adjustments\n",
    "- Advanced model architectures with flexible configurations  \n",
    "- Multi-objective optimization combining accuracy and Sharpe ratio\n",
    "- Comprehensive result tracking and benchmarking\n",
    "- Cross-validation with time series splits\n",
    "- Feature selection and data preparation utilities\n",
    "\n",
    "The optimizer supports:\n",
    "- Single symbol optimization with study resumption\n",
    "- Parameter transfer learning between symbols\n",
    "- Intelligent symbol prioritization\n",
    "- Advanced model architectures (CNN-LSTM)\n",
    "- Real-time progress tracking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
