{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Force environment activation and GPU configuration\nimport os\nimport sys\nimport subprocess\n\n# Method 1: Try to activate conda environment programmatically\ndef activate_trading_env():\n    \"\"\"Attempt to activate trading-env within Python.\"\"\"\n    try:\n        # Get conda info\n        result = subprocess.run(['conda', 'info', '--base'], capture_output=True, text=True)\n        conda_base = result.stdout.strip()\n        \n        # Add conda env to path\n        trading_env_path = f\"{conda_base}/envs/trading-env\"\n        if os.path.exists(trading_env_path):\n            # Prepend to PATH\n            env_bin = f\"{trading_env_path}/bin\"\n            if env_bin not in os.environ.get('PATH', ''):\n                os.environ['PATH'] = f\"{env_bin}:{os.environ.get('PATH', '')}\"\n            \n            # Set conda environment variables\n            os.environ['CONDA_DEFAULT_ENV'] = 'trading-env'\n            os.environ['CONDA_PREFIX'] = trading_env_path\n            \n            print(f\"‚úÖ Activated trading-env: {trading_env_path}\")\n            return True\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not activate trading-env: {e}\")\n    \n    return False\n\n# Method 2: Import TensorFlow from correct environment\ndef import_tensorflow_gpu():\n    \"\"\"Import TensorFlow with GPU support.\"\"\"\n    try:\n        # Try importing TensorFlow\n        import tensorflow as tf\n        \n        print(f\"TensorFlow version: {tf.__version__}\")\n        print(f\"TensorFlow location: {tf.__file__}\")\n        \n        # Check if it's from the correct environment\n        if 'trading-env' in tf.__file__:\n            print(\"‚úÖ Using TensorFlow from trading-env\")\n        else:\n            print(\"‚ö†Ô∏è TensorFlow not from trading-env\")\n            \n        return tf\n        \n    except ImportError as e:\n        print(f\"‚ùå TensorFlow import failed: {e}\")\n        print(\"Installing TensorFlow...\")\n        subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow'])\n        import tensorflow as tf\n        return tf\n\n# Activate environment and import TensorFlow\nactivate_trading_env()\ntf = import_tensorflow_gpu()\n\n# Enhanced GPU configuration with fallback detection\ndef configure_gpu_with_fallback():\n    \"\"\"Configure GPU with enhanced detection and fallback options.\"\"\"\n    print(\"üîß Configuring GPU with enhanced detection...\")\n    \n    # Method 1: Standard TensorFlow GPU detection\n    gpus = tf.config.list_physical_devices('GPU')\n    \n    # Method 2: Force CUDA device detection if standard method fails\n    if not gpus:\n        print(\"Standard GPU detection failed, trying CUDA detection...\")\n        try:\n            # Force CUDA initialization\n            with tf.device('/GPU:0'):\n                test_tensor = tf.constant([1.0])\n                print(\"CUDA device accessible\")\n                gpus = ['/GPU:0']  # Manual GPU specification\n        except:\n            print(\"CUDA detection also failed\")\n    \n    if gpus:\n        print(f\"üéÆ Found {len(gpus)} GPU(s):\")\n        \n        try:\n            # Configure each GPU\n            for i, gpu in enumerate(gpus):\n                print(f\"  GPU {i}: {gpu}\")\n                \n                if hasattr(tf.config.experimental, 'set_memory_growth'):\n                    if isinstance(gpu, str):\n                        # Manual GPU reference\n                        print(f\"    Manual GPU configuration for {gpu}\")\n                    else:\n                        # Standard PhysicalDevice\n                        tf.config.experimental.set_memory_growth(gpu, True)\n                        print(f\"    ‚úÖ Memory growth enabled\")\n                        \n                        # Get GPU details\n                        try:\n                            details = tf.config.experimental.get_device_details(gpu)\n                            print(f\"    Details: {details}\")\n                        except:\n                            pass\n            \n            # Set mixed precision\n            try:\n                policy = tf.keras.mixed_precision.Policy('mixed_float16')\n                tf.keras.mixed_precision.set_global_policy(policy)\n                print(\"  ‚úÖ Mixed precision enabled (float16)\")\n            except Exception as e:\n                print(f\"  ‚ö†Ô∏è Mixed precision failed: {e}\")\n            \n            # Test GPU computation\n            try:\n                with tf.device('/GPU:0'):\n                    a = tf.random.normal([1000, 1000])\n                    b = tf.random.normal([1000, 1000])\n                    c = tf.matmul(a, b)\n                    print(f\"  ‚úÖ GPU computation test: {c.device}\")\n            except Exception as e:\n                print(f\"  ‚ö†Ô∏è GPU computation test failed: {e}\")\n            \n            return True\n            \n        except Exception as e:\n            print(f\"‚ùå GPU configuration failed: {e}\")\n            \n    else:\n        print(\"‚ùå No GPUs detected\")\n        print(\"Diagnostic information:\")\n        print(f\"  Python executable: {sys.executable}\")\n        print(f\"  TensorFlow built with CUDA: {tf.test.is_built_with_cuda()}\")\n        \n        # Try nvidia-smi check\n        try:\n            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n            if result.returncode == 0:\n                print(\"  ‚úÖ nvidia-smi works (GPU hardware available)\")\n            else:\n                print(\"  ‚ùå nvidia-smi failed\")\n        except:\n            print(\"  ‚ùå nvidia-smi not available\")\n    \n    return False\n\n# Run GPU configuration\ngpu_available = configure_gpu_with_fallback()\n\nif gpu_available:\n    print(\"\\n‚ö° GPU Configuration Complete!\")\n    print(\"  - GPU memory growth enabled\")\n    print(\"  - Mixed precision training ready\")\n    print(\"  - Ready for hyperparameter optimization\")\n    \n    # Enable XLA if possible\n    try:\n        tf.config.optimizer.set_jit(True)\n        print(\"  - XLA compilation enabled\")\n    except:\n        print(\"  - XLA compilation not available\")\n        \nelse:\n    print(\"\\nüñ•Ô∏è Using CPU Configuration:\")\n    print(\"  - Multi-threading enabled\")\n    print(\"  - CPU optimization active\")\n    \n    # Optimize for CPU\n    tf.config.threading.set_intra_op_parallelism_threads(0)\n    tf.config.threading.set_inter_op_parallelism_threads(0)\n\nprint(f\"\\nüìä Final Status:\")\nprint(f\"  TensorFlow: {tf.__version__}\")\nprint(f\"  GPU devices: {len(tf.config.list_physical_devices('GPU'))}\")\nprint(f\"  Ready for optimization: {'GPU' if gpu_available else 'CPU'}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Hyperparameter Optimization for CNN-LSTM Trading Strategy\n",
    "\n",
    "This notebook optimizes hyperparameters for the CNN-LSTM forex trading model using Optuna.\n",
    "\n",
    "## Optimization Targets\n",
    "- **Model Architecture**: Conv1D filters, LSTM units, layers\n",
    "- **Training Parameters**: Learning rate, batch size, epochs\n",
    "- **Regularization**: Dropout rates, L1/L2 regularization\n",
    "- **Data Parameters**: Lookback window, feature selection\n",
    "- **Trading Parameters**: Confidence thresholds, signal generation\n",
    "\n",
    "## Objective Functions\n",
    "- Primary: Sharpe Ratio (risk-adjusted returns)\n",
    "- Secondary: Total Return, Maximum Drawdown, Win Rate\n",
    "- Model: Validation Accuracy, Loss\n",
    "\n",
    "## Search Strategy\n",
    "- Multi-objective optimization\n",
    "- Bayesian optimization with TPE sampler\n",
    "- Early pruning of poor trials\n",
    "- Cross-validation for robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Suppress TensorFlow warnings AFTER GPU configuration\ntf.get_logger().setLevel('ERROR')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Configuration\nDATA_PATH = \"RCS_CNN_LSTM_Notebook/RCS_CNN_LSTM_Notebook/data\"\nSYMBOLS = ['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD', 'USDCAD', 'EURJPY', 'GBPJPY']\nTARGET_SYMBOL = 'EURUSD'  # Focus on one symbol for optimization\n\n# Optimization settings\nN_TRIALS = 100  # Number of optimization trials\nN_JOBS = 1      # Parallel jobs (set to 1 for stability)\nTIMEOUT = 3600  # 1 hour timeout\nCV_SPLITS = 3   # Cross-validation splits\n\nprint(f\"Target symbol for optimization: {TARGET_SYMBOL}\")\nprint(f\"Optimization trials: {N_TRIALS}\")\nprint(f\"CV splits: {CV_SPLITS}\")\n\n# Verify GPU is still available after configuration\nprint(f\"\\nüîç GPU Status Check:\")\nprint(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\nprint(f\"GPU available: {tf.test.is_gpu_available()}\")\nif tf.config.list_physical_devices('GPU'):\n    print(f\"GPU name: {tf.test.gpu_device_name()}\")\n    print(\"‚úÖ GPU acceleration ready for hyperparameter optimization\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Install optuna if not available\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    from optuna.pruners import MedianPruner\n",
    "    print(\"‚úÖ Optuna available\")\n",
    "except ImportError:\n",
    "    print(\"Installing Optuna...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna\"])\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    from optuna.pruners import MedianPruner\n",
    "    print(\"‚úÖ Optuna installed and imported\")\n",
    "\n",
    "# Add src path\n",
    "src_path = os.path.join(os.getcwd(), 'RCS_CNN_LSTM_Notebook', 'RCS_CNN_LSTM_Notebook', 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Import ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Technical analysis\n",
    "import ta\n",
    "from ta.volatility import BollingerBands, AverageTrueRange\n",
    "from ta.trend import ADXIndicator, MACD, CCIIndicator\n",
    "from ta.momentum import StochasticOscillator, ROCIndicator, RSIIndicator\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"RCS_CNN_LSTM_Notebook/RCS_CNN_LSTM_Notebook/data\"\n",
    "SYMBOLS = ['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD', 'USDCAD', 'EURJPY', 'GBPJPY']\n",
    "TARGET_SYMBOL = 'EURUSD'  # Focus on one symbol for optimization\n",
    "\n",
    "# Optimization settings\n",
    "N_TRIALS = 100  # Number of optimization trials\n",
    "N_JOBS = 1      # Parallel jobs (set to 1 for stability)\n",
    "TIMEOUT = 3600  # 1 hour timeout\n",
    "CV_SPLITS = 3   # Cross-validation splits\n",
    "\n",
    "print(f\"Target symbol for optimization: {TARGET_SYMBOL}\")\n",
    "print(f\"Optimization trials: {N_TRIALS}\")\n",
    "print(f\"CV splits: {CV_SPLITS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Load all data once for optimization.\n",
    "    \"\"\"\n",
    "    print(\"üì• Loading base data...\")\n",
    "    \n",
    "    # Load parquet files\n",
    "    data = {}\n",
    "    for symbol in SYMBOLS:\n",
    "        parquet_file = os.path.join(DATA_PATH, f\"metatrader_{symbol}.parquet\")\n",
    "        if os.path.exists(parquet_file):\n",
    "            df = pd.read_parquet(parquet_file)\n",
    "            \n",
    "            # Standardize time column\n",
    "            if 'timestamp' in df.columns:\n",
    "                df['time'] = pd.to_datetime(df['timestamp'])\n",
    "            elif 'time' in df.columns:\n",
    "                df['time'] = pd.to_datetime(df['time'])\n",
    "            \n",
    "            df = df.set_index('time')\n",
    "            \n",
    "            # Add volume if missing\n",
    "            if 'tick_volume' not in df.columns:\n",
    "                df['tick_volume'] = df.get('volume', 0)\n",
    "            \n",
    "            # Store in MultiIndex format\n",
    "            for col in ['open', 'high', 'low', 'close', 'tick_volume']:\n",
    "                if col in df.columns:\n",
    "                    data[(symbol, col)] = df[col]\n",
    "    \n",
    "    prices_df = pd.DataFrame(data)\n",
    "    print(f\"‚úÖ Loaded data: {prices_df.shape}\")\n",
    "    return prices_df\n",
    "\n",
    "def calculate_all_indicators(prices_df, symbols):\n",
    "    \"\"\"\n",
    "    Calculate technical indicators for all symbols.\n",
    "    \"\"\"\n",
    "    print(\"üìä Calculating technical indicators...\")\n",
    "    \n",
    "    all_indicators = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        if (symbol, 'close') not in prices_df.columns:\n",
    "            continue\n",
    "            \n",
    "        close = prices_df[(symbol, 'close')].dropna()\n",
    "        high = prices_df[(symbol, 'high')].dropna() if (symbol, 'high') in prices_df.columns else close\n",
    "        low = prices_df[(symbol, 'low')].dropna() if (symbol, 'low') in prices_df.columns else close\n",
    "        \n",
    "        indicators = pd.DataFrame(index=close.index)\n",
    "        \n",
    "        # Comprehensive indicator set\n",
    "        try:\n",
    "            # Momentum\n",
    "            indicators['rsi'] = RSIIndicator(close=close, window=14).rsi()\n",
    "            indicators['rsi_fast'] = RSIIndicator(close=close, window=7).rsi()\n",
    "            indicators['rsi_slow'] = RSIIndicator(close=close, window=21).rsi()\n",
    "            indicators['roc'] = ROCIndicator(close=close, window=10).roc()\n",
    "            indicators['roc_fast'] = ROCIndicator(close=close, window=5).roc()\n",
    "            \n",
    "            # Trend\n",
    "            macd = MACD(close=close)\n",
    "            indicators['macd'] = macd.macd()\n",
    "            indicators['macd_signal'] = macd.macd_signal()\n",
    "            indicators['macd_histogram'] = macd.macd_diff()\n",
    "            \n",
    "            indicators['cci'] = CCIIndicator(high=high, low=low, close=close).cci()\n",
    "            indicators['adx'] = ADXIndicator(high=high, low=low, close=close).adx()\n",
    "            \n",
    "            # Volatility\n",
    "            indicators['atr'] = AverageTrueRange(high=high, low=low, close=close).average_true_range()\n",
    "            indicators['atr_norm'] = indicators['atr'] / close\n",
    "            \n",
    "            bb = BollingerBands(close=close)\n",
    "            indicators['bb_upper'] = bb.bollinger_hband()\n",
    "            indicators['bb_lower'] = bb.bollinger_lband()\n",
    "            indicators['bb_width'] = bb.bollinger_wband()\n",
    "            indicators['bb_position'] = (close - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())\n",
    "            \n",
    "            # Stochastic\n",
    "            stoch = StochasticOscillator(high=high, low=low, close=close)\n",
    "            indicators['stoch_k'] = stoch.stoch()\n",
    "            indicators['stoch_d'] = stoch.stoch_signal()\n",
    "            \n",
    "            # Price features\n",
    "            indicators['return_1'] = close.pct_change(1)\n",
    "            indicators['return_3'] = close.pct_change(3)\n",
    "            indicators['return_5'] = close.pct_change(5)\n",
    "            indicators['return_10'] = close.pct_change(10)\n",
    "            \n",
    "            # Moving averages\n",
    "            indicators['sma_5'] = close.rolling(5).mean()\n",
    "            indicators['sma_10'] = close.rolling(10).mean()\n",
    "            indicators['sma_20'] = close.rolling(20).mean()\n",
    "            indicators['sma_50'] = close.rolling(50).mean()\n",
    "            \n",
    "            indicators['ema_5'] = close.ewm(span=5).mean()\n",
    "            indicators['ema_10'] = close.ewm(span=10).mean()\n",
    "            indicators['ema_20'] = close.ewm(span=20).mean()\n",
    "            \n",
    "            # Volatility measures\n",
    "            indicators['volatility_5'] = close.rolling(5).std()\n",
    "            indicators['volatility_10'] = close.rolling(10).std()\n",
    "            indicators['volatility_20'] = close.rolling(20).std()\n",
    "            \n",
    "            # Price position\n",
    "            indicators['price_position_5'] = (close - close.rolling(5).min()) / (close.rolling(5).max() - close.rolling(5).min())\n",
    "            indicators['price_position_20'] = (close - close.rolling(20).min()) / (close.rolling(20).max() - close.rolling(20).min())\n",
    "            \n",
    "            # Time features\n",
    "            indicators['hour'] = indicators.index.hour\n",
    "            indicators['day_of_week'] = indicators.index.dayofweek\n",
    "            indicators['month'] = indicators.index.month\n",
    "            \n",
    "            # Clean data\n",
    "            indicators = indicators.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "            indicators = indicators.replace([np.inf, -np.inf], 0)\n",
    "            \n",
    "            all_indicators[symbol] = indicators\n",
    "            print(f\"‚úÖ {symbol}: {indicators.shape[1]} indicators\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to calculate indicators for {symbol}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_indicators\n",
    "\n",
    "def calculate_rcs(prices_df, symbols):\n",
    "    \"\"\"\n",
    "    Calculate Relative Currency Strength.\n",
    "    \"\"\"\n",
    "    print(\"üßÆ Calculating RCS...\")\n",
    "    \n",
    "    close_prices = {}\n",
    "    for symbol in symbols:\n",
    "        if (symbol, 'close') in prices_df.columns:\n",
    "            close_prices[symbol] = prices_df[(symbol, 'close')]\n",
    "    \n",
    "    if not close_prices:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    close_df = pd.DataFrame(close_prices)\n",
    "    log_returns = np.log(close_df / close_df.shift(1)).dropna()\n",
    "    \n",
    "    currencies = list(set([s[:3] for s in close_prices.keys()] + [s[3:6] for s in close_prices.keys()]))\n",
    "    \n",
    "    rcs_data = {c: [] for c in currencies}\n",
    "    \n",
    "    for i in range(len(log_returns)):\n",
    "        row = log_returns.iloc[i]\n",
    "        daily_strength = {c: 0 for c in currencies}\n",
    "        counts = {c: 0 for c in currencies}\n",
    "        \n",
    "        for pair, ret in row.items():\n",
    "            if pd.notna(ret):\n",
    "                base, quote = pair[:3], pair[3:]\n",
    "                daily_strength[base] += ret\n",
    "                daily_strength[quote] -= ret\n",
    "                counts[base] += 1\n",
    "                counts[quote] += 1\n",
    "        \n",
    "        for c in currencies:\n",
    "            avg = daily_strength[c] / counts[c] if counts[c] else 0\n",
    "            rcs_data[c].append(avg)\n",
    "    \n",
    "    rcs_df = pd.DataFrame(rcs_data, index=log_returns.index)\n",
    "    print(f\"‚úÖ RCS calculated: {rcs_df.shape}\")\n",
    "    \n",
    "    return rcs_df\n",
    "\n",
    "# Load base data\n",
    "print(\"Loading base dataset...\")\n",
    "prices = load_all_data()\n",
    "all_indicators = calculate_all_indicators(prices, SYMBOLS)\n",
    "rcs = calculate_rcs(prices, SYMBOLS)\n",
    "\n",
    "print(f\"\\n‚úÖ Base data loaded:\")\n",
    "print(f\"  Prices: {prices.shape}\")\n",
    "print(f\"  Indicators: {len(all_indicators)} symbols\")\n",
    "print(f\"  RCS: {rcs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Hyperparameter Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_hyperparameters(trial):\n",
    "    \"\"\"\n",
    "    Define the hyperparameter search space.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        # Data parameters\n",
    "        'lookback_window': trial.suggest_int('lookback_window', 10, 50),\n",
    "        'feature_selection_method': trial.suggest_categorical('feature_selection_method', \n",
    "                                                            ['all', 'top_correlation', 'variance_threshold', 'rfe']),\n",
    "        'max_features': trial.suggest_int('max_features', 10, 30),\n",
    "        \n",
    "        # Model architecture\n",
    "        'conv1d_filters_1': trial.suggest_int('conv1d_filters_1', 32, 128, step=16),\n",
    "        'conv1d_filters_2': trial.suggest_int('conv1d_filters_2', 16, 64, step=8),\n",
    "        'conv1d_kernel_size': trial.suggest_int('conv1d_kernel_size', 2, 5),\n",
    "        'lstm_units': trial.suggest_int('lstm_units', 25, 100, step=5),\n",
    "        'dense_units': trial.suggest_int('dense_units', 10, 50, step=5),\n",
    "        \n",
    "        # Regularization\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "        'l1_reg': trial.suggest_float('l1_reg', 1e-5, 1e-2, log=True),\n",
    "        'l2_reg': trial.suggest_float('l2_reg', 1e-5, 1e-2, log=True),\n",
    "        \n",
    "        # Training parameters\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "        'epochs': trial.suggest_int('epochs', 50, 200),\n",
    "        'patience': trial.suggest_int('patience', 5, 20),\n",
    "        \n",
    "        # Trading parameters\n",
    "        'confidence_threshold_high': trial.suggest_float('confidence_threshold_high', 0.55, 0.8),\n",
    "        'confidence_threshold_low': trial.suggest_float('confidence_threshold_low', 0.2, 0.45),\n",
    "    }\n",
    "    \n",
    "    # Ensure threshold consistency\n",
    "    if params['confidence_threshold_low'] >= params['confidence_threshold_high']:\n",
    "        params['confidence_threshold_low'] = params['confidence_threshold_high'] - 0.1\n",
    "    \n",
    "    return params\n",
    "\n",
    "print(\"‚úÖ Hyperparameter search space defined\")\n",
    "print(\"\\nSearch space summary:\")\n",
    "print(\"- Lookback window: 10-50\")\n",
    "print(\"- Conv1D filters: 32-128, 16-64\")\n",
    "print(\"- LSTM units: 25-100\")\n",
    "print(\"- Learning rate: 1e-4 to 1e-2\")\n",
    "print(\"- Batch size: 16, 32, 64, 128\")\n",
    "print(\"- Dropout: 0.1-0.5\")\n",
    "print(\"- Confidence thresholds: optimized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def select_features(features_df, target, method='all', max_features=20):\n",
    "    \"\"\"\n",
    "    Select features based on specified method.\n",
    "    \"\"\"\n",
    "    if method == 'all':\n",
    "        return features_df.columns.tolist()\n",
    "    \n",
    "    elif method == 'top_correlation':\n",
    "        # Select features with highest correlation to target\n",
    "        correlations = abs(features_df.corrwith(target))\n",
    "        top_features = correlations.nlargest(max_features).index.tolist()\n",
    "        return top_features\n",
    "    \n",
    "    elif method == 'variance_threshold':\n",
    "        # Remove low variance features first\n",
    "        selector = VarianceThreshold(threshold=0.01)\n",
    "        selector.fit(features_df)\n",
    "        selected_features = features_df.columns[selector.get_support()].tolist()\n",
    "        \n",
    "        # If still too many, use correlation\n",
    "        if len(selected_features) > max_features:\n",
    "            correlations = abs(features_df[selected_features].corrwith(target))\n",
    "            selected_features = correlations.nlargest(max_features).index.tolist()\n",
    "        \n",
    "        return selected_features\n",
    "    \n",
    "    elif method == 'rfe':\n",
    "        # Recursive Feature Elimination with RandomForest\n",
    "        rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        selector = RFE(rf, n_features_to_select=min(max_features, len(features_df.columns)))\n",
    "        selector.fit(features_df, target)\n",
    "        selected_features = features_df.columns[selector.support_].tolist()\n",
    "        return selected_features\n",
    "    \n",
    "    else:\n",
    "        return features_df.columns.tolist()\n",
    "\n",
    "print(\"‚úÖ Feature selection functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Building and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def create_optimized_model(input_shape, params):\n    \"\"\"\n    Create model with optimized hyperparameters using GPU.\n    \"\"\"\n    # Force GPU device placement if available\n    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n        model = Sequential()\n        \n        # First Conv1D layer\n        model.add(Conv1D(\n            filters=params['conv1d_filters_1'],\n            kernel_size=params['conv1d_kernel_size'],\n            activation='relu',\n            input_shape=input_shape,\n            kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n        ))\n        model.add(BatchNormalization())\n        model.add(Dropout(params['dropout_rate']))\n        \n        # Second Conv1D layer\n        model.add(Conv1D(\n            filters=params['conv1d_filters_2'],\n            kernel_size=params['conv1d_kernel_size'],\n            activation='relu',\n            kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n        ))\n        model.add(BatchNormalization())\n        model.add(Dropout(params['dropout_rate']))\n        \n        # LSTM layer\n        model.add(LSTM(\n            units=params['lstm_units'],\n            kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg']),\n            recurrent_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n        ))\n        model.add(BatchNormalization())\n        model.add(Dropout(params['dropout_rate']))\n        \n        # Dense layer (optional)\n        if params['dense_units'] > 0:\n            model.add(Dense(\n                units=params['dense_units'],\n                activation='relu',\n                kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n            ))\n            model.add(Dropout(params['dropout_rate']))\n        \n        # Output layer\n        model.add(Dense(1, activation='sigmoid'))\n        \n        # Compile with optimized parameters\n        optimizer = Adam(learning_rate=params['learning_rate'])\n        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n\ndef prepare_training_data(symbol, indicators, rcs_df, prices_df, params):\n    \"\"\"\n    Prepare training data with optimized parameters.\n    \"\"\"\n    # Create target\n    close_prices = prices_df[(symbol, 'close')]\n    target = (close_prices.shift(-1) > close_prices).astype(int).dropna()\n    \n    # Combine features\n    features = indicators.copy()\n    \n    # Add RCS features if available\n    if not rcs_df.empty:\n        for currency in ['USD', 'EUR', 'GBP', 'JPY']:\n            if currency in rcs_df.columns:\n                rcs_aligned = rcs_df[currency].reindex(features.index, method='ffill')\n                features[f'rcs_{currency}'] = rcs_aligned\n    \n    # Align features and target\n    common_index = features.index.intersection(target.index)\n    features = features.loc[common_index]\n    target = target.loc[common_index]\n    \n    # Clean features\n    features = features.fillna(method='ffill').fillna(method='bfill').fillna(0)\n    features = features.replace([np.inf, -np.inf], 0)\n    \n    # Feature selection\n    selected_features = select_features(\n        features, target, \n        method=params['feature_selection_method'],\n        max_features=params['max_features']\n    )\n    \n    features_selected = features[selected_features]\n    \n    # Scale features\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features_selected)\n    \n    # Create sequences\n    lookback = params['lookback_window']\n    X, y = [], []\n    \n    for i in range(lookback, len(features_scaled)):\n        X.append(features_scaled[i-lookback:i])\n        y.append(target.iloc[i])\n    \n    X = np.array(X)\n    y = np.array(y)\n    \n    return X, y, selected_features, scaler\n\nprint(\"‚úÖ Model building functions defined with GPU device placement\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Objective Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def calculate_trading_metrics(y_true, y_pred_proba, returns, params):\n    \"\"\"\n    Calculate trading performance metrics.\n    \"\"\"\n    # Generate trading signals\n    signals = np.where(\n        y_pred_proba > params['confidence_threshold_high'], 1,\n        np.where(y_pred_proba < params['confidence_threshold_low'], -1, 0)\n    )\n    \n    # Calculate strategy returns\n    strategy_returns = signals * returns\n    \n    # Performance metrics\n    total_return = np.sum(strategy_returns)\n    \n    # Sharpe ratio (annualized)\n    if np.std(strategy_returns) > 0:\n        sharpe_ratio = np.sqrt(252 * 24) * np.mean(strategy_returns) / np.std(strategy_returns)\n    else:\n        sharpe_ratio = 0\n    \n    # Maximum drawdown\n    cumulative_returns = np.cumsum(strategy_returns)\n    running_max = np.maximum.accumulate(cumulative_returns)\n    drawdowns = cumulative_returns - running_max\n    max_drawdown = np.min(drawdowns) if len(drawdowns) > 0 else 0\n    \n    # Win rate\n    total_trades = np.sum(signals != 0)\n    winning_trades = np.sum(strategy_returns > 0)\n    win_rate = winning_trades / total_trades if total_trades > 0 else 0\n    \n    # Return metrics\n    return {\n        'total_return': total_return,\n        'sharpe_ratio': sharpe_ratio,\n        'max_drawdown': max_drawdown,\n        'win_rate': win_rate,\n        'total_trades': total_trades\n    }\n\ndef cross_validate_model(X, y, params, n_splits=3):\n    \"\"\"\n    Perform time series cross-validation with GPU acceleration.\n    \"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    \n    cv_scores = []\n    cv_sharpe_ratios = []\n    \n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        # Create and train model with GPU\n        with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n            model = create_optimized_model((X.shape[1], X.shape[2]), params)\n            \n            # Early stopping\n            early_stopping = EarlyStopping(\n                monitor='val_loss',\n                patience=params['patience'],\n                restore_best_weights=True,\n                verbose=0\n            )\n            \n            # Train model\n            history = model.fit(\n                X_train, y_train,\n                validation_data=(X_val, y_val),\n                epochs=params['epochs'],\n                batch_size=params['batch_size'],\n                callbacks=[early_stopping],\n                verbose=0\n            )\n        \n        # Evaluate\n        y_pred_proba = model.predict(X_val, verbose=0).flatten()\n        y_pred = (y_pred_proba > 0.5).astype(int)\n        \n        # Model accuracy\n        accuracy = accuracy_score(y_val, y_pred)\n        cv_scores.append(accuracy)\n        \n        # Trading performance (simplified - assume unit returns)\n        returns = np.random.normal(0, 0.001, len(y_val))  # Placeholder returns\n        trading_metrics = calculate_trading_metrics(y_val, y_pred_proba, returns, params)\n        cv_sharpe_ratios.append(trading_metrics['sharpe_ratio'])\n        \n        # Clear model to save memory\n        del model\n        tf.keras.backend.clear_session()\n    \n    return {\n        'mean_accuracy': np.mean(cv_scores),\n        'std_accuracy': np.std(cv_scores),\n        'mean_sharpe': np.mean(cv_sharpe_ratios),\n        'std_sharpe': np.std(cv_sharpe_ratios)\n    }\n\nprint(\"‚úÖ Evaluation functions defined with GPU acceleration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimization Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna optimization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get hyperparameters\n",
    "        params = suggest_hyperparameters(trial)\n",
    "        \n",
    "        # Prepare data\n",
    "        X, y, selected_features, scaler = prepare_training_data(\n",
    "            TARGET_SYMBOL, \n",
    "            all_indicators[TARGET_SYMBOL], \n",
    "            rcs, \n",
    "            prices, \n",
    "            params\n",
    "        )\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        if len(X) < 100:\n",
    "            return -1.0  # Penalize insufficient data\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_results = cross_validate_model(X, y, params, n_splits=CV_SPLITS)\n",
    "        \n",
    "        # Multi-objective optimization\n",
    "        # Primary: Sharpe ratio (risk-adjusted returns)\n",
    "        # Secondary: Accuracy (model performance)\n",
    "        \n",
    "        sharpe_score = cv_results['mean_sharpe']\n",
    "        accuracy_score = cv_results['mean_accuracy']\n",
    "        \n",
    "        # Combined objective (weighted)\n",
    "        # Prioritize Sharpe ratio but ensure reasonable accuracy\n",
    "        if accuracy_score < 0.5:  # Below random chance\n",
    "            combined_score = -1.0\n",
    "        else:\n",
    "            combined_score = 0.7 * sharpe_score + 0.3 * (accuracy_score - 0.5) * 2\n",
    "        \n",
    "        # Log trial results\n",
    "        trial.set_user_attr('mean_accuracy', cv_results['mean_accuracy'])\n",
    "        trial.set_user_attr('std_accuracy', cv_results['std_accuracy'])\n",
    "        trial.set_user_attr('mean_sharpe', cv_results['mean_sharpe'])\n",
    "        trial.set_user_attr('std_sharpe', cv_results['std_sharpe'])\n",
    "        trial.set_user_attr('num_features', len(selected_features))\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(combined_score, step=0)\n",
    "        \n",
    "        # Check if trial should be pruned\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "        return combined_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trial failed: {str(e)}\")\n",
    "        return -1.0  # Return poor score for failed trials\n",
    "    \n",
    "    finally:\n",
    "        # Clean up memory\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"‚úÖ Objective function defined\")\n",
    "print(\"\\nObjective function details:\")\n",
    "print(\"- Primary metric: Sharpe ratio (70% weight)\")\n",
    "print(\"- Secondary metric: Accuracy above 50% (30% weight)\")\n",
    "print(\"- Minimum accuracy threshold: 50% (random chance)\")\n",
    "print(\"- Cross-validation: Time series split\")\n",
    "print(\"- Pruning: MedianPruner for early stopping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization():\n",
    "    \"\"\"\n",
    "    Run the hyperparameter optimization.\n",
    "    \"\"\"\n",
    "    print(f\"üéØ Starting hyperparameter optimization for {TARGET_SYMBOL}\")\n",
    "    print(f\"Trials: {N_TRIALS}, CV splits: {CV_SPLITS}, Timeout: {TIMEOUT}s\")\n",
    "    \n",
    "    # Create study\n",
    "    study_name = f\"cnn_lstm_{TARGET_SYMBOL}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=TPESampler(seed=42),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    "        study_name=study_name\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=N_TRIALS,\n",
    "        timeout=TIMEOUT,\n",
    "        n_jobs=N_JOBS,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    return study\n",
    "\n",
    "# Run the optimization\n",
    "print(\"üöÄ Starting optimization...\")\n",
    "study = run_optimization()\n",
    "\n",
    "print(f\"\\n‚úÖ Optimization completed!\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Number of pruned trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
    "print(f\"Number of complete trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(study):\n",
    "    \"\"\"\n",
    "    Analyze and visualize optimization results.\n",
    "    \"\"\"\n",
    "    print(\"üìä Analyzing optimization results...\")\n",
    "    \n",
    "    # Best trial\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Trial Results:\")\n",
    "    print(f\"Objective value: {best_trial.value:.6f}\")\n",
    "    print(f\"Mean accuracy: {best_trial.user_attrs.get('mean_accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"Mean Sharpe ratio: {best_trial.user_attrs.get('mean_sharpe', 'N/A'):.4f}\")\n",
    "    print(f\"Number of features: {best_trial.user_attrs.get('num_features', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nüîß Best Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Top 5 trials\n",
    "    print(f\"\\nü•á Top 5 Trials:\")\n",
    "    top_trials = sorted(study.trials, key=lambda t: t.value or -999, reverse=True)[:5]\n",
    "    \n",
    "    for i, trial in enumerate(top_trials, 1):\n",
    "        print(f\"\\n{i}. Trial {trial.number}:\")\n",
    "        print(f\"   Objective: {trial.value:.6f}\")\n",
    "        print(f\"   Accuracy: {trial.user_attrs.get('mean_accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"   Sharpe: {trial.user_attrs.get('mean_sharpe', 'N/A'):.4f}\")\n",
    "        print(f\"   Lookback: {trial.params.get('lookback_window')}\")\n",
    "        print(f\"   Features: {trial.user_attrs.get('num_features', 'N/A')}\")\n",
    "    \n",
    "    # Parameter importance\n",
    "    print(f\"\\nüìà Parameter Importance:\")\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        for param, imp in sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"  {param}: {imp:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not calculate importance: {e}\")\n",
    "    \n",
    "    return best_trial\n",
    "\n",
    "# Analyze results\n",
    "best_trial = analyze_results(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optimization_results(study):\n",
    "    \"\"\"\n",
    "    Create visualization plots for optimization results.\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating optimization visualizations...\")\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'Hyperparameter Optimization Results - {TARGET_SYMBOL}', fontsize=16)\n",
    "    \n",
    "    # 1. Optimization history\n",
    "    ax1 = axes[0, 0]\n",
    "    values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "    ax1.plot(values, 'b-', alpha=0.7)\n",
    "    ax1.plot(np.cummax(values), 'r-', linewidth=2, label='Best value')\n",
    "    ax1.set_xlabel('Trial')\n",
    "    ax1.set_ylabel('Objective Value')\n",
    "    ax1.set_title('Optimization History')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Parameter correlation\n",
    "    ax2 = axes[0, 1]\n",
    "    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    \n",
    "    if len(completed_trials) > 10:\n",
    "        # Lookback vs Objective\n",
    "        lookbacks = [t.params['lookback_window'] for t in completed_trials]\n",
    "        objectives = [t.value for t in completed_trials]\n",
    "        ax2.scatter(lookbacks, objectives, alpha=0.6)\n",
    "        ax2.set_xlabel('Lookback Window')\n",
    "        ax2.set_ylabel('Objective Value')\n",
    "        ax2.set_title('Lookback Window vs Performance')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Accuracy vs Sharpe\n",
    "    ax3 = axes[1, 0]\n",
    "    accuracies = [t.user_attrs.get('mean_accuracy', 0) for t in completed_trials]\n",
    "    sharpes = [t.user_attrs.get('mean_sharpe', 0) for t in completed_trials]\n",
    "    \n",
    "    if accuracies and sharpes:\n",
    "        scatter = ax3.scatter(accuracies, sharpes, c=[t.value for t in completed_trials], \n",
    "                            cmap='viridis', alpha=0.7)\n",
    "        ax3.set_xlabel('Mean Accuracy')\n",
    "        ax3.set_ylabel('Mean Sharpe Ratio')\n",
    "        ax3.set_title('Accuracy vs Sharpe Ratio')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax3, label='Objective Value')\n",
    "    \n",
    "    # 4. Feature count distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    feature_counts = [t.user_attrs.get('num_features', 0) for t in completed_trials]\n",
    "    \n",
    "    if feature_counts:\n",
    "        ax4.hist(feature_counts, bins=20, alpha=0.7, edgecolor='black')\n",
    "        ax4.set_xlabel('Number of Features')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.set_title('Feature Count Distribution')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plot_filename = f\"optimization_results_{TARGET_SYMBOL}_{timestamp}.png\"\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìÅ Plot saved as: {plot_filename}\")\n",
    "\n",
    "# Create visualizations\n",
    "plot_optimization_results(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_optimization_results(study, best_trial):\n",
    "    \"\"\"\n",
    "    Save optimization results to files.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = \"optimization_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params_file = os.path.join(results_dir, f\"best_params_{TARGET_SYMBOL}_{timestamp}.json\")\n",
    "    with open(best_params_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'symbol': TARGET_SYMBOL,\n",
    "            'timestamp': timestamp,\n",
    "            'objective_value': best_trial.value,\n",
    "            'mean_accuracy': best_trial.user_attrs.get('mean_accuracy'),\n",
    "            'mean_sharpe': best_trial.user_attrs.get('mean_sharpe'),\n",
    "            'num_features': best_trial.user_attrs.get('num_features'),\n",
    "            'best_params': best_trial.params\n",
    "        }, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Best parameters saved to: {best_params_file}\")\n",
    "    \n",
    "    # Save all trials data\n",
    "    trials_data = []\n",
    "    for trial in study.trials:\n",
    "        trial_data = {\n",
    "            'trial_number': trial.number,\n",
    "            'value': trial.value,\n",
    "            'state': str(trial.state),\n",
    "            'params': trial.params,\n",
    "            'user_attrs': trial.user_attrs\n",
    "        }\n",
    "        trials_data.append(trial_data)\n",
    "    \n",
    "    trials_file = os.path.join(results_dir, f\"all_trials_{TARGET_SYMBOL}_{timestamp}.json\")\n",
    "    with open(trials_file, 'w') as f:\n",
    "        json.dump(trials_data, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ All trials saved to: {trials_file}\")\n",
    "    \n",
    "    # Save study object\n",
    "    study_file = os.path.join(results_dir, f\"study_{TARGET_SYMBOL}_{timestamp}.pkl\")\n",
    "    with open(study_file, 'wb') as f:\n",
    "        pickle.dump(study, f)\n",
    "    \n",
    "    print(f\"‚úÖ Study object saved to: {study_file}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_file = os.path.join(results_dir, f\"optimization_summary_{TARGET_SYMBOL}_{timestamp}.txt\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"Hyperparameter Optimization Summary\\n\")\n",
    "        f.write(f\"=====================================\\n\\n\")\n",
    "        f.write(f\"Symbol: {TARGET_SYMBOL}\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Total trials: {len(study.trials)}\\n\")\n",
    "        f.write(f\"Completed trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\\n\")\n",
    "        f.write(f\"Pruned trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Best Trial Results:\\n\")\n",
    "        f.write(f\"  Objective value: {best_trial.value:.6f}\\n\")\n",
    "        f.write(f\"  Mean accuracy: {best_trial.user_attrs.get('mean_accuracy', 'N/A'):.4f}\\n\")\n",
    "        f.write(f\"  Mean Sharpe ratio: {best_trial.user_attrs.get('mean_sharpe', 'N/A'):.4f}\\n\")\n",
    "        f.write(f\"  Number of features: {best_trial.user_attrs.get('num_features', 'N/A')}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Best Hyperparameters:\\n\")\n",
    "        for key, value in best_trial.params.items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Summary report saved to: {summary_file}\")\n",
    "    \n",
    "    return {\n",
    "        'best_params_file': best_params_file,\n",
    "        'trials_file': trials_file,\n",
    "        'study_file': study_file,\n",
    "        'summary_file': summary_file\n",
    "    }\n",
    "\n",
    "# Save results\n",
    "print(\"üíæ Saving optimization results...\")\n",
    "saved_files = save_optimization_results(study, best_trial)\n",
    "\n",
    "print(\"\\nüéâ Hyperparameter optimization completed successfully!\")\n",
    "print(\"\\nüìÅ Files saved:\")\n",
    "for file_type, filepath in saved_files.items():\n",
    "    print(f\"  {file_type}: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Best Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def test_best_model(best_params):\n    \"\"\"\n    Train and test model with best parameters using GPU.\n    \"\"\"\n    print(f\"üß™ Testing best model with optimized parameters...\")\n    \n    # Prepare data with best parameters\n    X, y, selected_features, scaler = prepare_training_data(\n        TARGET_SYMBOL, \n        all_indicators[TARGET_SYMBOL], \n        rcs, \n        prices, \n        best_params\n    )\n    \n    print(f\"Data prepared: {X.shape}, Features: {len(selected_features)}\")\n    \n    # Split data\n    split_idx = int(len(X) * 0.8)\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n    \n    # Create and train best model with GPU\n    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n        model = create_optimized_model((X.shape[1], X.shape[2]), best_params)\n        \n        early_stopping = EarlyStopping(\n            monitor='val_loss',\n            patience=best_params['patience'],\n            restore_best_weights=True,\n            verbose=1\n        )\n        \n        print(\"Training best model on GPU...\")\n        print(f\"üéÆ Using device: {'/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'}\")\n        \n        history = model.fit(\n            X_train, y_train,\n            validation_split=0.15,\n            epochs=best_params['epochs'],\n            batch_size=best_params['batch_size'],\n            callbacks=[early_stopping],\n            verbose=1\n        )\n    \n    # Evaluate\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    y_pred_proba = model.predict(X_test, verbose=0).flatten()\n    \n    print(f\"\\n‚úÖ Best Model Results:\")\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Features used: {len(selected_features)}\")\n    \n    # Plot training history\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title(f'{TARGET_SYMBOL} - Best Model Training Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title(f'{TARGET_SYMBOL} - Best Model Training Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Save best model\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_filename = f\"best_model_{TARGET_SYMBOL}_{timestamp}.h5\"\n    model.save(model_filename)\n    print(f\"\\nüíæ Best model saved as: {model_filename}\")\n    \n    return model, selected_features, scaler\n\n# Test the best model\nbest_model, best_features, best_scaler = test_best_model(best_trial.params)\n\nprint(f\"\\nüéØ Hyperparameter optimization completed successfully!\")\nprint(f\"\\nüìä Summary:\")\nprint(f\"  Best objective value: {best_trial.value:.6f}\")\nprint(f\"  Best accuracy: {best_trial.user_attrs.get('mean_accuracy', 'N/A'):.4f}\")\nprint(f\"  Best Sharpe ratio: {best_trial.user_attrs.get('mean_sharpe', 'N/A'):.4f}\")\nprint(f\"  Optimal features: {len(best_features)}\")\nprint(f\"  Optimal lookback: {best_trial.params['lookback_window']}\")\n\nprint(f\"\\nüöÄ Ready for deployment with optimized hyperparameters!\")\n\n# Final GPU status check\nprint(f\"\\nüîç Final GPU Status:\")\nprint(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\nif tf.config.list_physical_devices('GPU'):\n    print(\"‚úÖ GPU was successfully used throughout hyperparameter optimization\")\nelse:\n    print(\"‚ö†Ô∏è CPU was used for hyperparameter optimization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This hyperparameter optimization notebook provides:\n",
    "\n",
    "### ‚úÖ **Comprehensive Optimization**\n",
    "- **Model Architecture**: Conv1D filters, LSTM units, layer configuration\n",
    "- **Training Parameters**: Learning rate, batch size, epochs, patience\n",
    "- **Regularization**: Dropout rates, L1/L2 regularization\n",
    "- **Data Processing**: Lookback window, feature selection methods\n",
    "- **Trading Logic**: Confidence thresholds for signal generation\n",
    "\n",
    "### üéØ **Multi-Objective Optimization**\n",
    "- **Primary**: Sharpe ratio (risk-adjusted returns)\n",
    "- **Secondary**: Model accuracy above random chance\n",
    "- **Constraints**: Minimum data requirements, performance thresholds\n",
    "\n",
    "### üî¨ **Robust Validation**\n",
    "- **Time Series Cross-Validation**: Respects temporal order\n",
    "- **Early Pruning**: Stops poor trials early\n",
    "- **Multiple Metrics**: Accuracy, Sharpe ratio, drawdown, win rate\n",
    "\n",
    "### üìä **Comprehensive Analysis**\n",
    "- **Parameter Importance**: Identifies most influential parameters\n",
    "- **Visualization**: Optimization history, parameter relationships\n",
    "- **Results Export**: JSON, pickle, summary reports\n",
    "\n",
    "### üöÄ **Production Ready**\n",
    "- **Best Model Export**: Saves optimized model for deployment\n",
    "- **Reproducible Results**: Saved parameters and random seeds\n",
    "- **Memory Management**: Proper cleanup between trials\n",
    "\n",
    "### üìà **Next Steps**\n",
    "1. **Apply optimized parameters** to other currency pairs\n",
    "2. **Multi-symbol optimization** for portfolio strategies\n",
    "3. **Online optimization** for adaptive parameters\n",
    "4. **Ensemble optimization** combining multiple models\n",
    "\n",
    "The optimization process systematically finds the best combination of hyperparameters to maximize risk-adjusted returns while maintaining model robustness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}